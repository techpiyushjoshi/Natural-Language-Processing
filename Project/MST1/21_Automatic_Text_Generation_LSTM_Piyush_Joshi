,Category,Title,Article
0,Machine Learning,Introduction to Artificial Neural Network ,"This is an introductory article for the artificial neural network. It is one of the machine learning techniques that is inspired by the biological neural system and used to solve pattern recognition problems. An artificial neural network (ANN) is an information processing element that is similar to the biological neural network. It is a combination of multiple interconnected neurons that execute information in parallel mode. It has the capability to learn by example. ANN is flexible in nature, it has the capability to change the weights of the network. ANN is like a black box trained to solve complex problems. Neural network algorithms are inherently parallel in nature and this parallelization helpful in faster computation. ANN has the capability to solve complex pattern recognition problems such as face recognition, object detection, image classification, named entity recognition, and machine translation. The idea of ANN algorithms is stimulated from the human brain. ANN learn things by processing input information and adjusting weights to forecast the exact output label. We can define a neural network as “It is an interconnected set of neurons, input and output units. Each connection in this interconnected network assigned with weight. These weights are adjusted as per output label in an adaptive manner.” Here, x1, x2 …. xn are input variables, w1,w2….wn are weights for the respective inputs and b is the bias. Y is the output and that is the summation of weighted inputs and bias. After learning and adjusting the weights we apply the activation function to map output to a certain range. The main purpose of the activation function is to introduce non-linearity in the network. An artificial neural network has a set of neurons with input and output units. Multiple neurons were arranged in a layered manner. Each layer is a set of neurons and it implies a stage in the network. Here, each connection is associated with a weight. In the training phase, the ANN learns things by fine-tuning the weights to predict the correct class label of the input tuples.  Feedforward neural network is a network that is not recursive in nature. Here, neurons in one connected to another layer of neurons but it doesn’t create any cycle, signals travel in only one direction towards the output layer.  Feedback neural networks contain cycles. Here, signals travel in both directions via loops in the network. Feedback neural network is also known as recurrent neural networks. In 1943, Warren McCulloch and Walter Pitts created the first mathematical model of an artificial neuron. This simple mathematical model represents a single cell that takes inputs, processes those inputs, and returns an output. Hebb network introduced by Donald Hebb in 1949. Hebb network learns by performing the change in the synaptical weight. Here, weights increased in proportion to the product of the input and learning signals. Perceptron was developed by Frank Rosenblatt in 1950. Perceptron is the simplest supervised neural network that has the capability to learn complex things. Perceptron network consists of three units: Sensory Unit (Input Unit), Associator Unit (Hidden Unit), and Response Unit (Output Unit). Adaline stands for Adaptive Linear Line. It uses a linear activation function and captures the linear relationship between input and output. For learning, it uses Delta Rule or Least Mean Square(LMS) or Widrow-Hoff rule. Madaline(Multiple Adaline) consists of many Adalines in parallel with a single output unit. It uses bipolar linear activation function. Adaline and Madaline models can be used in adaptive equalizer and adaptive noise cancellation systems. Multi-Layer Perceptron(MLP) is the simplest type of artificial neural network. It is a combination of multiple perceptron models. Perceptrons are inspired by the human brain and try to simulate its functionality to solve problems. In MLP, these perceptrons are highly interconnected and parallel in nature. BPN was discovered by Rumelhart, Williams & Honton in 1986. The core concept of BPN is to backpropagate or spread the error from units of output layer to internal hidden layers in order to tune the weights ensures lower error rates.  RBFN is introduced by M.J.D. Powell in 1985 but Broomhead and Lowe (1988) were the first to exploit the use of radial-basis functions(Gaussian kernel function) in the design of artificial neural networks as a curve-fitting problem. It has a single hidden layer that is non-linear in nature. In RBFN, the function of the hidden layer is different from that of the output layer. ANN can deal with non-linear and complex problems. It can learn from a variety of problems(such as clustering, classification) and data(such as text, audio, image, videos). ANN has powerful prediction power. It can easily handle the quality issues in the data. It has a high tolerance for noisy dataand has the ability to classify patterns on unseen data. ANN is a kind of black-box model, lacking in interpretations. It needs a large dataset for training. ANNs always require numerical inputs and non-missing values in the datasets. Tuning neural networks requires a large number of variables. Artificial neural networks are complex systems that simulate human brain functionality. It has the capability to solve a variety of data mining problems such as pattern recognition. It offers a black box solution with higher accuracy and less interpretability. For more machine learning articles, please visit the following link."
1,Machine Learning,Activation Functions ,"The activation function defines the output of a neuron in terms of the induced local field. Activation functions are a single line of code that gives the neural networks non-linearity and expressiveness. There are many activation functions such as Identity function, Step function, Sigmoid function, Tanh, ReLU, Leaky ReLU, Parametric ReLU, and Softmax function. We can see some of them in the following table: In this tutorial, we are going to cover the following topics: The identity function is a function that maps input to the same output value. It is a linear operator in vector space. Also, a known straight-line function where activation is proportional to the input. The simplest example of a linear activation function is a linear equation.  f(x) = a * x,where a ∈ R The major problem with such kind of linear function it cannot handle complex scenarios.  In Binary Step Function, if the value of Y is above a certain value known as the threshold, the output is True(or activated) and if it’s less than the threshold then the output is false (or not activated). It is very useful in the classifier. The main problem with the binary step function is zero gradients or it is not differentiable at zero. It cannot update the gradient in backpropagation. It only works with binary class problems because it maps to only two categories 0 and 1. In the Bipolar Step Function, if the value of Y is above a certain value known as the threshold, the output is +1and if it’s less than the threshold then the output is -1. It has bipolar outputs (+1 to -1). It can be utilized in single-layer networks.  It is also called S-shaped functions. Logistic and hyperbolic tangent functions are commonly used in sigmoid functions. There are two types of sigmoid functions. Binary Sigmoid Function or Sigmoid function is a logistic function where the output values are either binary or vary from 0 to 1. It is differentiable, non-linear, and produces non-binary activations But the problem with Sigmoid is the vanishing gradients. Also, sigmoid activation is not a zero-centric function. Hyperbolic Tangent Function or Tanh is a logistic function where the output value varies from -1 to 1. Also known as Bipolar Sigmoid Function. The output of Tanh centers around 0 and sigmoid’s around 0.5. Tanh Convergence is usually faster if the average of each input variable over the training set is close to zero. When you struggle to quickly find the local or global minimum, in such case Tanh can be helpful in faster convergence. The derivatives of Tanh are larger than Sigmoid that causes faster optimization of the cost function. Tanh suffered from vanishing gradient problems. ReLu stands for the rectified linear unit (ReLU). It is the most used activation function in the world. It output 0 for negative values of x. This is also known as a ramp function. The name of the ramp function is derived from the appearance of its graph. ReLu(Rectified Linear Unit) is like a linearity switch. If you don’t need it, you “switch” it off. If you need it, you “switch” it on. ReLu avoids the problem of vanishing gradient. ReLu also provides the benefit of sparsity and sigmoids result in dense representations. Sparse representations are more useful than dense representations.  The main problem with ReLU is, it is not differentiable at 0 and may result in exploding gradients.  The main problem of ReLU is, it is not differentiable at 0 and may result in exploding gradients. To resolve this problem Leaky ReLu was introduced that is differentiable at 0. It provides small negative values when input is less than 0. The main problem with Leaky ReLu is not offering consistent predictions in terms of negative data. PReLU (Parametric ReLU) overcome the dying ReLU problem and Leaky ReLU inconsistent predictions for negative input values. The core idea behind the Parametric ReLU is to make the coefficient of leakage into a parameter that gets learned.   The softmax function is typically used on the output layer for multi-class classification problems. It provides the probability distribution of possible outcomes of the network. In conclusion, we can say in deep learning problems, ReLu is used on hidden layers and sigmoid/softmax on the output layer. Sigmoid is used for binary classification, and Softmax is used for multi-class classification problems. In this tutorial, we have discussed various activation functions, types of activation functions such as Identity function, Step function, Sigmoid function, Tanh, ReLU, Leaky ReLU, Parametric ReLU, and Softmax function. We have discussed the pros and cons of various activation functions. Also, you have understood, how to decide which activation funciton to use? "
2,Machine Learning,Multi-Layer Perceptron Neural Network using Python ,"In this tutorial, we will focus on the multi-layer perceptron, it’s working, and hands-on in python. Multi-Layer Perceptron(MLP) is the simplest type of artificial neural network. It is a combination of multiple perceptron models. Perceptrons are inspired by the human brain and try to simulate its functionality to solve problems. In MLP, these perceptrons are highly interconnected and parallel in nature. This parallelization helpful in faster computation. Perceptron was introduced by Frank Rosenblatt in 1950. It has the capability to learn complex things just like the human brain. Perceptron network consists of three units: Sensory Unit (Input Unit), Associator Unit (Hidden Unit), and Response Unit (Output Unit).  The Perceptron consists of an input layer and an output layer which are fully connected. MLPs have the same input and output layers but may have multiple hidden layers in between as mentioned in the previous section figure. Multi-Layer Perceptron trains model in an iterative manner. In each iteration, partial derivatives of the loss function used to update the parameters. We can also use regularization of the loss function to prevent overfitting in the model. In this section, we will perform employee churn prediction using Multi-Layer Perceptron. Employee churn prediction helps us in designing better employee retention plans and improving employee satisfaction. For its exploratory data analysis you can refer to the following article on Predicting Employee Churn in Python: Let’s start the model building hands-on in python. Let’s first load the required HR dataset using pandas’ read CSV function. You can download data from the following link:  Output: Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column’s value can be represented as low:0, medium:1, and high:2. This process is known as label encoding. In sklearn, we can do this using LabelEncoder. Here, we imported the preprocessing module and created the Label Encoder object. Using this LabelEncoder object you fit and transform the “salary” and “Departments “ column into the numeric column. In order to assess the model performance, we need to divide the dataset into a training set and a test set. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size.  Let’s build an employee churn prediction model. Here, our objective is to predict churn using MLPClassifier. First, import the MLPClassifier module and create MLP Classifier object using MLPClassifier() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Parameters: In this section, we will make predictions on the test dataset and assess model accuracy based on available actual labels of the test dataset.  Output:0.9386666666666666 Well, you got a classification rate of 93.8%, considered as good accuracy. Congratulations, you have made it to the end of this tutorial! In this tutorial, we have discussed perception, multilayer perception, it’s working, and MLP Classifier hands-on with python. we have built the classifier model for employee churn using Multi-Layer Perceptron Classification with the scikit-learn package."
3,Machine Learning,Backpropagation Neural Network using Python ,"Backpropagation neural network is used to improve the accuracy of neural network and make them capable of self-learning. Backpropagation means “backward propagation of errors”. Here error is spread into the reverse direction in order to achieve better performance.  Backpropagation is an algorithm for supervised learning of artificial neural networks that uses the gradient descent method to minimize the cost function. It searches for optimal weights that optimize the mean-squared distance between the predicted and actual labels. In this tutorial, we are going to cover the following topics: BPN was discovered by Rumelhart, Williams & Honton in 1986. The core concept of BPN is to backpropagate or spread the error from units of output layer to internal hidden layers in order to tune the weights to ensure lower error rates. It is considered a practice of fine-tuning the weights of neural networks in each iteration. Proper tuning of the weights will make a sure minimum loss and this will make a more robust, and generalizable trained neural network.  BPN learns in an iterative manner. In each iteration, it compares training examples with the actual target label. target label can be a class label or continuous value. The backpropagation algorithm works in the following steps: Lets import the required modules and libraries such as numpy, pandas, scikit-learn, and matplotlib. Let’s first load the Iris dataset using load_iris() function of scikit-learn library and seprate them in features and target labels. This data set has three classes Iris-setosa, Iris-versicolor, and Iris-virginica.  Create dummy variables for class labels using get_dummies() function Output: To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state in order to get the same kind of train and test set. Lets initialize the hyperparameters such as learning rate, iterations, input size, number of hidden layers, and number of output layers.  Lets initialize the weights for hidden and output layers with random values. Lets create helper functions such as sigmoid, mean_square_error, and accuracy.  In this phase, we will create backpropagation neural network in three steps feedforward propagation, error calculation and backpropagation phase. Here , we will create a for loop for given number of iterations that execute the three steps(feedforward propagation, error calculation and backpropagation phase) and update the weights in each iteration.  Lets plot mean squared error in each iteration using pandas plot() funciton. Output:  Lets plot accuracy in each iteration using pandas plot() funciton. Output: Lets make prediction for the test data and assess the performance of Backpropagation neural network.  Output:  you can see in the above output, we are getting 80% accuracy on test dataset. Backpropagation Neural Network is a simple and faster model compared to its earlier models. It is also a flexible and standard method. It does not need any prior knowledge for training.  BPN performance depends upon the kind of input data is used. It is quite sensitive to noisy data. We need to use a matrix-based approach instead of a mini-batch.  Congratulations, you have made it to the end of this tutorial! Backpropagation neural network is a method to optimize neural networks by propagating the error or loss into a backward direction. It finds loss for each node and updates its weights accordingly in order to minimize the loss using gradient descent.  In this tutorial, you have learned What is Backpropagation Neural Network, Backpropagation algorithm working, and Implementation from scratch in python. We have also discussed the pros and cons of the Backpropagation Neural Network. "
4,Machine Learning,Understanding Logistic Regression and Building Model in Python ,"Learn about Logistic Regression, its basic properties, it’s working, and build a machine learning model on the real-world applications in Python. Classification techniques are an important part of machine learning and data mining applications. Approx 70% of problems in Data Science are classification problems. There are lots of classification problems are available but logistics regression is a very common and useful regression method for solving a binary classification problem. Another category of classification is Multinomial classification, which handles the problems where multiple classes are present in the target variable. For example, the IRIS dataset has various famous examples of multiclass classification. Other examples are classifying article/blog/document categories. Logistic Regression can be used for various classification problems such as spam detection, Diabetes prediction, if a given customer will purchase a particular product or will churn to another competitor, the user will click on a given advertisement link or not and many more examples are in the bucket. Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also very helpful in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables. In this tutorial, you will learn the following things in Logistic Regression: Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature, dichotomous means there are only two possible classes. for example, it can be used for cancer detection problems. It computes the probability of an event occurrence. It is a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event using a logit function. Linear Regression Equation: Where, y is the dependent variable and x1, x2 … and Xn are explanatory variables. Sigmoid Function: Apply Sigmoid function on linear regression: Properties of Logistic Regression: Linear regression gives you a continuous output but logistic regression gives a discrete output. An example of a continuous output is house price and stock price. An example of the discrete output is predicting whether a patient has cancer or not, predicting whether the customer will churn. Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using the Maximum Likelihood Estimation (MLE) approach. The MLE is a “likelihood” maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution. Ordinary Least squares estimates are computed by fitting a regression line on given data points that has the minimum sum of the squared deviations (least square error). Both are used to estimate the parameters of a linear regression model. MLE assumes a joint probability mass function, while OLS doesn’t require any stochastic assumptions for minimizing distance. The sigmoid function also called the logistic function gives an ‘S-shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1 and If the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES and if it is less than 0.5, we can classify it as 0 or NO. The outputcannotFor example: If the output is 0.75, we can say in terms of probability as: There is a 75 percent chance that the patient will suffer from cancer. Types of Logistic Regression: Let’s build a diabetes prediction model. Here, you are going to predict diabetes using a Logistic Regression Classifier. Let’s first load the required Pima Indian Diabetes dataset using pandas’ read CSV function. You can download data from the following link: Here, you need to divide given columns into two types of variables dependent(or target variable) and independent variable(or feature variables). To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Here, Dataset is broken into two parts in the ratio of 75:25. It means 75% of data will be used for model training and 25% for model testing. First, import the Logistic Regression module and create a Logistic Regression classifier object using the LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise. Here, you can see the confusion matrix in the form of an array object. The dimension of this matrix is 2*2 because this model is a binary classification. you have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions and 26 and 11 are inaccurate predictions. Let’s visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn. Here, you will visualize the confusion matrix using Heatmap. Let’s evaluate the model using model evaluation metrics such as accuracy, precision, and recall. Well, you got a classification rate of 80%, considered as good accuracy. Precision: Precision is about being precise i.e. How precise your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients actually have 76% time. Recall: If there are patients who actually have diabetes in the test set and your Logistic Regression model is able to identify it 58% of the time. The Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false-positive rate. It shows the tradeoff between sensitivity and specificity. The AUC score for the case is 0.86. AUC score 1 represents a perfect classifier and 0.5 represents a worthless classifier. Because of its simple and efficient nature, doesn’t require high computation power, easy to implement, is easily interpretable, used widely by data analysts and scientists. Also doesn’t require scaling of features. Logistic regression provides a probability score for observations. Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can’t solve the non-linear problem with the logistic regression which is why it requires a transformation of non-linear features. logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other. In this tutorial, you covered a lot of details about Logistic Regression. you have learned what is logistic regression, how to build respective models, how to visualize results, and some of the theoretical background information. Also, you covered some basic concepts such as the sigmoid function, maximum likelihood, confusion matrix, ROC curve. Hopefully, you can now utilize the Logistic Regression technique to analyze your own datasets. Thanks for reading this tutorial! For more such tutorials, projects, and courses visit DataCamp Originally published at https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
5,Machine Learning,Naive Bayes Classification using Scikit-learn ,"Learn how to build and evaluate a Naive Bayes Classifier using Python’s Scikit-learn package. Suppose you are a product manager, you want to classify customer reviews in positive and negative classes. Or As a loan manager, you want to identify which loan applicants are safe or risky? As a healthcare analyst, you want to predict which patients can suffer from diabetes disease. All the examples have the same kind of problem to classify reviews, loan applicants, and patients. Naive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of data. Naive Bayes classifier is successfully used in various applications such as spam filtering, text classification, sentiment analysis, and recommender systems. It uses Bayes theorem of probability for prediction of unknown class. In this tutorial, you are going to learn about all of the following: For more such tutorials, projects, and courses visit DataCamp: Whenever you perform classification, the first step is to understand the problem and identify potential features and labels. Features are those characteristics or attributes which affect the results of the label. For example, in the case of a loan distribution, the bank manager’s identify customer’s occupation, income, age, location, previous loan history, transaction history, and credit score. These characteristics are known as features that help the model classify customers. The classification has two phases, a learning phase, and the evaluation phase. In the learning phase, the classifier trains its model on a given dataset, and in the evaluation phase, it tests the classifier performance. Performance is evaluated on the basis of various parameters such as accuracy, error, precision, and recall. Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is a fast, accurate, and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets. Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that’s why it is considered as naive. This assumption is called class conditional independence. Let’s understand the working of Naive Bayes through an example. Given an example of weather conditions and playing sports. You need to calculate the probability of playing sports. Now, you need to classify whether players will play or not, based on the weather condition. Naive Bayes classifier calculates the probability of an event in the following steps: For simplifying prior and posterior probability calculation you can use the two tables frequency and likelihood tables. Both of these tables will help you to calculate the prior and posterior probability. The Frequency table contains the occurrence of labels for all features. There are two likelihood tables. Likelihood Table 1 is showing prior probabilities of labels and Likelihood Table 2 is showing the posterior probability. Now suppose you want to calculate the probability of playing when the weather is overcast. Probability of playing: P(Yes | Overcast) = P(Overcast | Yes) P(Yes) / P (Overcast) …………………(1) Similarly, you can calculate the probability of not playing: Probability of not playing: P(No | Overcast) = P(Overcast | No) P(No) / P (Overcast) …………………(2) The probability of a ‘Yes’ class is higher. So you can determine here if the weather is overcast than players will play the sport. Now suppose you want to calculate the probability of playing when the weather is overcast, and the temperature is mild. Probability of playing: P(Play= Yes | Weather=Overcast, Temp=Mild) = P(Weather=Overcast, Temp=Mild | Play= Yes)P(Play=Yes) ……….(1) P(Weather=Overcast, Temp=Mild | Play= Yes)= P(Overcast |Yes) P(Mild |Yes) ………..(2) Similarly, you can calculate the probability of not playing: Probability of not playing: P(Play= No | Weather=Overcast, Temp=Mild) = P(Weather=Overcast, Temp=Mild | Play= No)P(Play=No) ……….(3) P(Weather=Overcast, Temp=Mild | Play= No)= P(Weather=Overcast |Play=No) P(Temp=Mild | Play=No) ………..(4) The probability of a ‘Yes’ class is higher. So you can say here that if the weather is overcast than players will play the sport. In this example, you can use the dummy dataset with three columns: weather, temperature, and play. The first two are features(weather, temperature) and the other is the label. First, you need to convert these string labels into numbers. for example: ‘Overcast’, ‘Rainy’, ‘Sunny’ as 0, 1, 2. This is known as label encoding. Scikit-learn provides the LabelEncoder library for encoding labels with a value between 0 and one less than the number of discrete classes. Similarly, you can also encode temp and play columns. Now combine both the features (weather and temp) in a single variable (list of tuples). Generate a model using Naive Bayes classifier in the following steps: Here, 1 indicates that players can ‘play’. Till now you have learned Naive Bayes classification with binary labels. Now you will learn about multiple class classification in Naive Bayes. Which is known as multinomial Naive Bayes classification? For example, if you want to classify a news article about technology, entertainment, politics, or sports. In the model-building part, you can use the wine dataset which is a very famous multi-class classification problem. “This dataset is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.” (UC Irvine) Dataset comprises of 13 features (alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavonoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline) and type of wine cultivar. This data has three types of wine Class_0, Class_1, and Class_3. Here you can build a model to classify the type of wine. The dataset is available in the scikit-learn library. Let’s first load the required wine dataset from scikit-learn datasets. You can print the target and feature names, to make sure you have the right dataset, as such: It’s a good idea to always explore your data a bit, so you know what you’re working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset. First, you separate the columns into dependent and independent variables(or features and labels). Then you split those variables into train and test set. After splitting, you will generate a random forest model on the training set and perform prediction on test set features. After model generation, check the accuracy using actual and predicted values. Suppose there is no tuple for a risky loan in the dataset, in this scenario, the posterior probability will be zero, and the model is unable to make a prediction. This problem is known as Zero Probability because the occurrence of the particular class is zero. The solution for such an issue is the Laplacian correction or Laplace Transformation. Laplacian correction is one of the smoothing techniques. Here, you can assume that the dataset is large enough that adding one row of each class will not make a difference in the estimated probability. This will overcome the issue of probability values to zero. For Example: Suppose that for the class loan risky, there are 1000 training tuples in the database. In this database, the income column has 0 tuples for low income, 990 tuples for medium-income, and 10 tuples for high income. The probabilities of these events, without the Laplacian correction, are 0, 0.990 (from 990/1000), and 0.010 (from 10/1000) Now, apply Laplacian correction on the given dataset. Let’s add 1 more tuple for each income-value pair. The probabilities of these events: Congratulations, you have made it to the end of this tutorial! In this tutorial, you learned about the Naïve Bayes algorithm, it’s working, Naive Bayes assumption, issues, implementation, advantages, and disadvantages. Along the road, you have also learned model building and evaluation in scikit-learn for binary and multinomial classes. Naive Bayes is the most straightforward and most potent algorithm. In spite of the significant advances in Machine Learning in the last couple of years, it has proved its worth. It has been successfully deployed in many applications from text analytics to recommendation engines. I look forward to hearing any feedback or questions. You can ask your questions by leaving a comment, and I will try my best to answer them. Originally published at https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn Do you want to learn data science, check out on DataCamp. Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
6,Machine Learning,Support Vector Machine Classification in Scikit-learn ,"In this tutorial, you’ll learn about support vector machines, one of the most popular and widely used supervised machine learning algorithms. Support Vector Machines are one of the most popular and widely used supervised machine learning algorithms. SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. SVM is known for its kernel trick to handle nonlinear input space. It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles, and web pages, classification of genes, and handwriting recognition. SVM is an exciting algorithm and the concepts are relatively simple. SVM classifier separates data points using a hyperplane with the largest amount of margin. That’s why an SVM classifier is also known as a discriminative classifier. SVM finds an optimal hyperplane which helps in classifying new data points. For more such tutorials and courses visit DataCamp: In this tutorial, you are going to cover the following topics: Generally, Support Vector Machines considered to be a classification approach but, it can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes. Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier. A hyperplane is a decision plane that separates between a set of objects having different class memberships. A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes than it is considered as good margin otherwise it is a bad margin. Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-python The main objective is to segregate the given dataset in the best possible way. The distance between the nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps: 2. Select the right hyperplane with the maximum segregation from either nearest data points as shown in the right-hand side figure. Some problems can’t be solved using linear hyperplane, as shown in the figure below (left-hand side). In such a situation, SVM uses a kernel trick to transform the input space to a higher dimensional space as shown on the right. The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: z=x²=y²). Now you can easily segregate these points using linear separation. The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher-dimensional space. In other words, you can say that it converts nonseparable problems to separable problems by adding more dimension to it. It is most useful in a non-linear separation problem. Kernel trick helps us to build a more accurate classifier. Where d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm. Here gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm. Till now, you have learned about the theoretical background of SVM. Now you will learn about its implementation in Python using scikit-learn. In the model the building part, you can use the cancer dataset, which is a very famous multi-class classification problem. This dataset is computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe the characteristics of the cell nuclei present in the image. The dataset comprises 30 features (mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, and worst fractal dimension) and a target (a type of cancer). This data has two types of cancer classes: malignant (harmful) and benign (not harmful). Here, you can build a model to classify the type of cancer. The dataset is available in the scikit-learn library or you can also download it from the UCI Machine Learning Library. Let’s first load the required dataset you will use. After you have loaded the dataset, you might want to know a little bit more about it. You can check features and target names. Let’s explore it for a bit more. you can also check the shape of the dataset using shape. Let’s check the top 5 records of the feature set. Let’s check the records of the target set. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Let’s build a support vector machine model. First, import the SVM module and create a support vector classifier object by passing the argument kernel as the linear kernel in SVC() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Let’s estimate, how accurately the classifier or model can predict the breast cancer of patients. Accuracy can be computed by comparing actual test set values and predicted values. Well, you got a classification rate of 96.49%, considered as very good accuracy. For further evaluation, you can also check the precision and recall of the model. Well, you got precision 98% and recall 96%, considered as very good precision and recall value. SVM Classifiers offer good accuracy and perform faster prediction compared to the Naïve Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space. SVM is not suitable for large datasets because of its high training time and also it takes more time in training compared to Naïve Bayes. It works poorly with overlapping classes and also sensitive to the type of kernel used. Congratulations, you have made it to the end of this tutorial! In this tutorial, you covered a lot of ground about the Support vector machine algorithm, its working, kernels, hyperparameter tuning, model building, and evaluation on breast cancer dataset using python Scikit-learn package. You have also covered its advantages and disadvantages. I hope you have learned something valuable! I look forward to hearing any feedback or questions. You can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-python Do you want to learn data science, check out on DataCamp. Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
7,Machine Learning,KNN Classification using Scikit-learn ,"Learn K-Nearest Neighbor(KNN) Classification and build a KNN classifier using Python Scikit-learn package. K Nearest Neighbor(KNN) is a very simple, easy-to-understand, versatile, and one of the topmost machine learning algorithms. KNN used in a variety of applications such as finance, healthcare, political science, handwriting detection, image recognition, and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won’t vote. KNN algorithm used for both classification and regression problems. KNN algorithm based on the feature similarity approach. In this tutorial, you are going to cover the following topics: KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real-world datasets do not follow mathematical theoretical assumptions. The lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and the testing phase slower and costlier. The costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points, and scanning all data points will require more memory for storing training data. For more such tutorials, projects, and courses visit DataCamp: In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the closest point to P1 and then the label of the nearest point assigned to P1. Suppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance, and Minkowski distance. KNN has the following basic steps: Eager learners mean when given training points will construct a generalized model before performing prediction on given new points to classify. You can think of such learners as being ready, active, and eager to classify unobserved data points. Lazy Learning means there is no need for learning or training of the model and all of the data points used at the time of prediction. Lazy learners wait until the last minute before classifying any data point. Lazy learners stores merely the training dataset and waits until classification needs to perform. Only when it sees the test tuple does it perform generalization to classify the tuple based on its similarity to the stored training tuples. Unlike eager learning methods, lazy learners do less work in the training phase and more work in the testing phase to make a classification. Lazy learners are also known as instance-based learners because lazy learners store the training points or instances, and all learning is based on instances. KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. An increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality. To deal with the problem of the curse of dimensionality, you need to perform principal component analysis before applying any machine learning algorithm, or you can also use the feature selection approach. Research has shown that in large dimensions Euclidean distance is not useful anymore. Therefore, you can prefer other measures such as cosine similarity, which get decidedly less affected by a high dimension. Now, you understand the KNN algorithm working mechanism. At this point, the question arises that How to choose the optimal number of neighbors? And what are its effects on the classifier? The number of neighbors(K) in KNN is a hyperparameter that you need to choose at the time of model building. You can think of K as a controlling variable for the prediction model. Research has shown that no optimal number of neighbors suits all kinds of data sets. Each dataset has its own requirements. In the case of a small number of neighbors, the noise will have a higher influence on the result, and a large number of neighbors make it computationally expensive. Research has also shown that a small number of neighbors are the most flexible fit which will have low bias but the high variance and a large number of neighbors will have a smoother decision boundary which means lower variance but higher bias. Generally, Data scientists choose an odd number if the number of classes is even. You can also check by generating the model on different values of k and check their performance. You can also try the Elbow method here. Let’s first create your own dataset. Here you need two kinds of attributes or columns in your data: Feature and label. The reason for the two types of columns is the “supervised nature of KNN algorithm”. In this dataset, you have two features (weather and temperature) and one label(play). Various machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Overcast:0, Rainy:1, and Sunny:2. This process is known as label encoding, and sklearn conveniently will do this for you using Label Encoder. Output: [2 2 0 1 1 1 0 2 2 1 2 0 0 1] Here, you imported the preprocessing module and created the Label Encoder object. Using this LabelEncoder object, you can fit and transform the “weather” column into the numeric column. Similarly, you can encode temperature and label it into numeric columns. Here, you will combine multiple columns or features into a single set of data using “zip” function Let’s build the KNN classifier model. First, import the KNeighborsClassifier module and create a KNN classifier object by passing the argument number of neighbors in KNeighborsClassifier() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Output: [1] In the above example, you have given input [0,2], where 0 means Overcast weather and 2 means Mild temperature. The model predicts [1], which means play. Till now, you have learned How to create a KNN classifier for two in python using scikit-learn. Now you will learn about KNN with multiple classes. In the model the building part, you can use the wine dataset, which is a very famous multi-class classification problem. This data is the result of a chemical analysis of wines grown in the same region in Italy using three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The dataset comprises 13 features (‘alcohol’, ‘malic_acid’, ‘ash’, ‘alcalinity_of_ash’, ‘magnesium’, ‘total_phenols’, ‘flavanoids’, ‘nonflavanoid_phenols’, ‘proanthocyanins’, ‘color_intensity’, ‘hue’, ‘od280/od315_of_diluted_wines’, ‘proline’) and a target (type of cultivars). This data has three types of cultivar classes: ‘class_0’, ‘class_1’, and ‘class_2’. Here, you can build a model to classify the type of cultivar. The dataset is available in the scikit-learn library, or you can also download it from the UCI Machine Learning Library. Let’s first load the required wine dataset from scikit-learn datasets. After you have loaded the dataset, you might want to know a little bit more about it. You can check features and target names. Let’s check top 5 records of the feature set. Let’s check the records of the target set. Let’s explore it for a bit more. You can also check the shape of the dataset using shape. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Let’s build the KNN classifier model for k=5. Let’s estimate, how accurately the classifier or model can predict the type of cultivars. Accuracy can be computed by comparing actual test set values and predicted values. Well, you got a classification rate of 68.51%, considered as good accuracy. For further evaluation, you can also create a model for a different number of neighbors. Let’s build the KNN classifier model for k=7. Let’s again estimate, how accurately the classifier or model can predict the type of cultivars for k=7. Well, you got a classification rate of 77.77%, considered as good accuracy. Here, you have increased the number of neighbors in the model, and accuracy got increased. But, this is not necessary for each case that an increase in many neighbors increases the accuracy. For a more detailed understanding of it, you can refer to the section “How to decide the number of neighbors?” of this tutorial. The training phase of K-nearest neighbor classification is much faster compared to other classification algorithms. There is no need to train a model for generalization, That is why KNN is known as the simple and instance-based learning algorithm. KNN can be useful in case of nonlinear data. It can be used with the regression problem. Output value for the object is computed by the average of k closest neighbors value. The testing phase of K-nearest neighbor classification is slower and costlier in terms of time and memory. It requires large memory for storing the entire training dataset for prediction. KNN requires the scaling of data because KNN uses the Euclidean distance between two data points to find the nearest neighbors. Euclidean distance is sensitive to magnitudes. The features with high magnitudes will weigh more than features with low magnitudes. KNN also not suitable for large dimensional data. For better results, normalizing data on the same scale is highly recommended. Generally, the normalization range considered between 0 and 1. KNN is not suitable for large dimensional data. In such cases, the dimension needs to reduce to improve performance. Also, handling missing values will help us in improving results. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned the K-Nearest Neighbor algorithm; it’s working, eager and lazy learner, the curse of dimensionality, model building, and evaluation on wine dataset using Python Scikit-learn package. Also, discussed its advantages, disadvantages, and performance improvement suggestions. I look forward to hearing any feedback or questions. You can ask questions by leaving a comment, and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn Do you want to learn data science, check out on DataCamp. For more such article, you can visit my blog Machine Learning Geek Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
8,Machine Learning,Decision Tree Classification in Python ,"Learn Decision Tree Classification, Attribute Selection Measures, Build and Optimize Decision Tree Classifier using the Python Scikit-learn package. As a marketing manager, you want a set of customers who are most likely to purchase your product. This is how you can save your marketing budget by finding your audience. As a loan manager, you need to identify risky loan applications to achieve a lower loan default rate. This process of classifying customers into a group of potential and non-potential customers or safe or risky loan applications is known as a classification problem. Classification is a two-step process, the learning step, and the prediction step. In the learning step, the model is developed based on given training data. In the prediction step, the model is used to predict the response for given data. Decision Tree is one of the easiest and popular classification algorithms to understand and interpret. It can be used for both classification and regression type of problem. In this tutorial, you are going to cover the following topics: For more such tutorials, projects, and courses visit DataCamp A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision making. It’s visualization like a flowchart diagram which easily mimics the human-level thinking. That is why decision trees are easy to understand and interpret. Decision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and the number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy. The basic idea behind any decision tree algorithm is as follows: Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. The best score attribute will be selected as a splitting attribute. in the case of the continuous-valued attribute, split points for branches also need to define. The most popular selection measures are Information Gain, Gain Ratio, and Gini Index. Shannon invented the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy referred to like the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information gain is a decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain. Where Pi is the probability that an arbitrary tuple in D belongs to class Ci. Where, Attribute A with the highest information gain, Gain(A), is chosen as the splitting attribute at node N. Information gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning. C4.5, an improvement of ID3, uses an extension to information gain known as the gain ratio. The gain ratio handles the issue of bias by normalizing the information gain using Split Info. Java implementation of the C4.5 algorithm is known as J48, which is available in the WEKA data mining tool. Where, The gain ratio can be defined as The attribute with the highest gain ratio is chosen as the splitting attribute. Another decision tree algorithm CART (Classification and Regression Tree) uses the Gini method to create split points. Where pi is the probability that a tuple in D belongs to class Ci. The Gini Index considers a binary split for each attribute. you can compute a weighted sum of the impurity of each partition. If a binary split on an attribute A partitions data D into D1 and D2, the Gini index of D is: In the case of a discrete-valued attribute, the subset that gives the minimum Gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with a smaller Gini index chosen as the splitting point. The attribute with the minimum Gini index is chosen as the splitting attribute. Let’s first load the required libraries. Let’s first load the required Pima Indian Diabetes dataset using pandas’ read CSV function. You can download data from the following link: Here, you need to divide given columns into two types of variables dependent(or target variable) and independent variable(or feature variables). To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). You need to pass basically 3 parameters features, target, and test_set size. Let’s create a Decision Tree Model using Scikit-learn. Let’s estimate, how accurately the classifier or model can predict the type of cultivars. Accuracy can be computed by comparing actual test set values and predicted values. Well, you got a classification rate of 67.53%, considered as good accuracy. You can improve this accuracy by tuning the parameters in the Decision Tree Algorithm. You can use Scikit-learn’s export_graphviz function to display the tree within a Jupyter notebook. For plotting trees, you also need to install graphviz and pydotplus. pip install graphviz pip install pydotplus export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter. In the decision tree chart, each internal node has the decision rule that splits the data. Gini referred to the Gini ratio, which measures the impurity of the node. you can say a node is pure when all of its records belong to the same class, such nodes known as the leaf node. Here, the resultant tree is unpruned. This unpruned tree is unexplainable and not easy to understand. In the next section, let’s optimize it by pruning. In Scikit-learn, optimization of decision tree classifier performed by only pre-pruning. maximum depth of the tree can be used as a control variable for pre-pruning. In the following example, you can plot a decision tree on the same data with max_depth=3. Other than pre-pruning parameters, You can also try other attribute selection measure such as entropy. Well, the classification rate increased to 77.05%, which is better accuracy than the previous model. This pruned model is less complex, explainable, and easy to understand than the previous decision tree model plot. Congratulations, you have made it to the end of this tutorial! In this tutorial, you covered a lot of details about Decision Tree; It’s working, attribute selection measures such as Information Gain, Gain Ratio, and Gini Index, decision tree model building, visualization, and evaluation on diabetes dataset using the Python Scikit-learn package. Also, discussed its pros, cons, and optimizing Decision Tree performance using parameter tuning. Hopefully, you can now utilize the Decision tree algorithm to analyze your own datasets. Thanks for reading this tutorial! Originally published at https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn Do you want to learn data science, check out on DataCamp. Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
9,Machine Learning,Spotify Song Recommender System in Python ,"Build a Song Recommender System using Content-Based Filtering in Python.  With the rapid growth in online and mobile platforms, lots of music platforms are coming into the picture. These platforms are offering songs lists from across the globe. Every individual has a unique taste for music. Most people are using Online music streaming platforms such as Spotify, Apple Music, Google Play, or Pandora.  Online Music listeners have lots of choices for the song. These customers sometimes get very difficult in selecting the songs or browsing the long list. The service providers need an efficient and accurate recommender system for suggesting relevant songs. As data scientists, we need to understand the patterns in music listening habits and predict the accurate and most relevant recommendations.  In this tutorial, we are going to cover the following topics: The content-based filtering method is based on the analysis of item features. It determines which features are most important for suggesting the songs. For example, if the user has liked a song in the past and the feature of that song is the theme and that theme is party songs then Recommender System will recommend the songs based on the same theme. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using the Spotify dataset to discover similar songs for recommendation using cosine similarity and sigmoid kernel.  In this tutorial, you will build a book recommender system. You can download this dataset from here.   Let’s load the data into pandas dataframe:  Output: Let’s understand the dataset. In this dataset, we have 15 columns: acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence, target, song_title, artist. Before building the model, first we normalize or scale the dataset. For scaling it we are using MinMaxScaler of Scikit-learn library. In this section, we are building a content-based recommender system using similarity measures such as Cosine and Sigmoid Kernel. Here, we will find the similarities among items or songs feature set and pick the top 10 most similar songs and recommend them.  Cosine similarity measures the cosine angle between two feature vectors. Its value implies that how two records are related to each other. Cosine similarity can be computed for the non-equal size of text documents. In the above code, we have computed the similarity using Cosine similarity and returned the Top-10 recommended songs. Let’s make a forecast using computed cosine similarity on the Spotify song dataset. In the above code, we have generated the Top-10 song list based on cosine similarity.  Let’s make a forecast using computed Sigmoid kernel on Spotify song dataset.  In the above code, we have generated the Top-10 song list based on Sigmoid Kernel.  Congratulations, you have made it to the end of this tutorial! In this tutorial, we have built the song recommender system using cosine similarity and Sigmoid kernel. This developed recommender system is a content-based recommender system. In another article, we have developed the recommender system using collaborative filtering. You can check that article here Book Recommender System using KNN. You can also check another article on the NLP-based recommender system."
10,Machine Learning,Building Movie Recommender System using Text Similarity ," In this tutorial, we will focus on the movie recommender system using the NLP technique. With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload. In this large amount of information, how to find the right information which meets customer needs. In this context, Recommender System can help us to deal with such huge information. Also, with the increase in user options and rapid change in user preferences, we need some online systems that quickly adapt and recommend the relevant items.  A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings. For example, Netflix is a streaming platform that recommends movies and series and keeps the consumer engaged on their platform. This engagement motivates customers to renew their subscriptions.  Content-based recommender system uses descriptive details products in order to make recommendations. For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using movie description or overview text to discover similar movies for recommendation using text similarity. In this tutorial, we are going to cover the following topics:   In this tutorial, we will build a movie recommender system using text similarity measures. You can download this dataset from here. Let’s load the data into pandas dataframe: Output: In the above code snippet, we have loaded The Movie Database (TMDb) data in Pandas DataFrame. In this section, we can explore the text overview of given movies. for doing exploratory analysis, the best way is to use Wordcloud and understand the most frequent words in the overview of the movie.  In the above code block, we have imported the wordcloud, stopwords, and matplotlib library. First, we created the combined text of all the movie overview descriptions and created the wordcloud on white background.  In the Text Similarity Problems, If we are applying cosine similarity then we have to convert texts into the respective vectors because we directly can’t use text for finding similarity. Let’s create vectors for given movie reviews using the TF-IDF approach.  TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. TF-IDF normalizes the document weights. The higher value of TF-IDF for a word represents a higher occurrence in that document. Output:  In the above code block, Scikit-learn TfidfVectorizer is available for generating the TF-IDF Matrix. Cosine similarity measures the cosine angle between two text vectors. Its value implies that how two documents are related to each other. Cosine similarity can be computed for the non-equal size of text documents. In the above code, we have computed the cosine similarity using the cosine_similarity() method of sklearn.metrics module. Let’s make a forecast using computed cosine similarity on movie description data. In the above code, we have generated the Top-10 movies based on similar movie overview descriptions. Congratulations, you have made it to the end of this tutorial! In the last decade, the use of recommendation systems is increasing rapidly in lots of business ventures such as online retail business, learning, tourism, fashion, and library portals. The recommendation system assists in choosing the right thing from a large number of items by focusing on item features and user profiles. In this tutorial, we have built the movie recommender system using text similarity. In upcoming articles, we will write more articles on different recommender systems using Python. "
11,Machine Learning,Book Recommender System using KNN ,"In this world of the internet, information is generating and growing rapidly. This huge amount of information will create information overload. People need to make decisions and make choices from available information or options. For example, People want to make a decision which Smartphone should they buy, which course they enroll in, which movie they watch, and which book they buy.  Recommender System is an automatic suggestion system that provides suggestions on things just like your friends, relatives, or neighbors give suggestions. A recommender system will help customers to find interesting things, reduce the number of options, and discover new things. At the same time, it also helps businesses to increase sales, increase user satisfaction and understand user needs. You have also seen youtube where they recommend videos based on your previously watched videos. Netflix recommends related movies based on previously watched movies. In this tutorial, we are going to cover the following topics: The main functions of the recommender system are: In this article, we will build a Book Recommenders System using KNN.  Collaborative filtering focuses on the ratings of the items given by users. It is based on “Wisdom of the crowd”. It predicts the suggested items based on the taste information from other users i.e. recommendations are from collaborative user ratings. collaborative filtering is used by large organizations such as Amazon and Netflix. It will suffer from cold start problems, sparsity problems, popularity bias, and first starter. Content-based filtering recommends items to users based on the description of the items and user profile. For example, recommending products based on the textual description, recommending movies based on their textual overview, and recommending books based on associated keywords. It will suffer where content is not well represented by keywords and the problem of indistinguishable items(same set feature items).  In this tutorial, you will build a book recommender system. You can download this dataset from here.   Let’s load the data into pandas dataframe:  Output:  Output: In the above two code snippet, we have loaded the Ratings and Books data in Pandas DataFrame.  In this section, we will merge the ratings and books’ dataframes based on the ISBN column using merge() function. Output: (1031136, 10) In this section, we will create the pivot table with book-title on the index, user id on the column, and fill values book rating. But before this first, we take a sample(1%) of the whole dataset because this dataset has 1 million records. If we don;t do this it will take a very long time or may cause of memory error on 8 GB Laptop.  Output: (10311, 10) Let’s create a Pivot table: Output: It’s time to create a NearestNeighbours model for recommendations using the Scikit-lean library.  Let’s make a forecast using a trained model of NearestNeighbours and generate a list of recommended books.  Output:  In the above output, we can see the list of recommended books.  Issues with NN-Based Collaborative Filtering Congratulations, you have made it to the end of this tutorial! In this tutorial, we have built the recommender system using the K-Nearest Neighbors algorithm.  In upcoming articles, we will write more articles on different recommender systems using Python.   "
12,Machine Learning,Recommendation System for Streaming Platforms ,"In this Python tutorial, explore movie data of popular streaming platforms and build a recommendation system. Due to the new culture of Binge-watching TV Shows and Movies, users are consuming content at a fast pace with available services like Netflix, Prime Video, Hulu, and Disney+. Some of these new platforms, such as Hulu and YouTube TV, also offer live streaming of events like Sports, live concerts/tours, and news channels. Live streaming is still not adopted by some of the streaming platforms, such as Netflix. Streaming platforms provide more flexibility to users to watch their favorite TV shows and movies, at any time, on any device. These services can attract more young and modern consumers because of its wide variety of TV and movie content. It allows them to watch any missed program as their availability. In this tutorial, you will analyze movie data of streaming platforms Netflix, Prime Video, Hulu, and Disney+ and try to understand their viewers. Let’s see the highlights of the tutorial: This data consisted of only movies available on streaming platforms such as Netflix, Prime Video, Hulu, and Disney+. You can download it from Kaggle here. Let’s describe data attributes in detail: Let’s import the necessary modules and load the dataset: In this section, You will work with missing values using the isnull() function. Let’s see an example below: You can see that the variables Age and Rotten tomatoes have more than 50 % missing values, which is alarming. Now, we will handle the missing values in the following steps: You can check the distribution of the Year column using the distplot() the function of seaborn. Let’s plot the Movie Year distribution plot. The chart is showing the distribution of movies origin year. You can interpret that most of the movies were made between the year 2000 to 2020. Let’s plot the IMDB rating distribution plot. The above distribution plot is slightly skewed. You can interpret that the mean IMDB of most movies is 6.5. Let’s plot the Movie Runtime distribution plot. From the above chart, you can interpret that the movie’s average runtime lies between 80 to 120 mins. In this section, you will see Streaming Platform wise movie distribution. First, you need to create a m_cnt() function that counts movies for a given streaming platform. After that, you can plot using Pie charts and understand the shares of streaming platforms. From the above plot, you can say that Prime Videos is hosting the maximum number of titles with 71% share and Netflix hosting 20% of titles. Disney+ and Hulu are hosting the lowest titles, 5.4%, and 3.4%, respectively. In this section, you will see genre-wise movie distribution. First, you need to prepare your data. You need to handle multiple genres given in a single cell of dataframe. For that, you can use split(), apply(), and stack() functions. split() function splits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each genre and stack() function stack them into a single column. After these three operations, a new Genres column will join with the existing dataframe, and you are ready to plot. You can show the top 10 genres with their movie count using value_counts() function and use plot() function of the pandas library. From the above plot, you can say that most of the movies have a common genre as Drama and Comedy. In this section, you will see the country-wise movie distribution. First, you need to prepare your data. You need to handle multiple countries given in a single cell of dataframe. For that, you can use split(), apply(), and stack() functions. The split() function splits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each country and stack() function stack them into a single column. After these three operations, a new Country column will join with the existing dataframe and you are ready to plot. You can show the top 10 countries with their movie count using value_counts() function and use the pandas library’s plot() function. The above graph shows that the majority of the movies were made in the United States. In this section, you will see language-wise movie distribution. First, you need to prepare your data. You need to handle multiple languages given in a single cell of the dataframe. For that, you can use split(), apply(), and stack() functions. split() function spits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each language and stack() function stack them into a single column. After these three operations, a new Language column will join with the existing dataframe, and you are ready to plot. You can show the top 10 languages with their movie count using value_counts() function and use the plot() the function of the pandas library. From the above plot, you can conclude that the majority of movies were in the English language. In this section, you will plot platform-wise IMDB rating distribution. For getting these results, you need to apply the melt() function and plot FacetGrid plot. melt() function converts a wide dataframe to a long dataframe. Let’s see the example below: The above plot shows the average IMDB rating distribution on each platform. In the “Working With Missing Values Section”, I have dropped the Age column. For getting results of Runtime Per Platform Along with Age Group. I need to load the data again and apply the melt() function. After loading the dataset again and performing melting, its time to generate a plot for runtime vs. streaming platform for different age groups. The above plot shows that the total runtime on Prime Videos by 18+ age group users is way higher than compared to any other platform. You can interpret that the Prime Videos most of the content is focused on the 18+ Age group. In the past few years, with the leap of YouTube, Walmart, Netflix, and many other such web-based services, recommender systems have created tremendous impact in the industry. From suggesting products/services to increasing companies value by online Ads-based monetization and matching the user’s relevance and preference to make them buy. Recommender systems are irreplaceable in our daily web quests. Generally, these are Math based frameworks focusing on suggesting products/services to end-users that are relevant to their needs and wants. For example, movies to watch, articles to read, products to buy, music to listen to, or anything depending on the domain. There are majorly three methods to build a Recommender Systems: Let’s first preprocess the dataset for the recommender system. First, you check the missing values: You will build two recommender system based on cosine similarity. Now, you will compute the similarity score using cosine similarity. Here, recommended movies are not up to the mark. The reason behind this poor result is that you are using only movie ratings, movie runtimes, and platform variables. You can improve this by using other information such as genre, directors, and country. Since our last recommender system worked well but the recommendations were not up to the mark, so you will try a new, better approach to improve our results. You will use textual columns into a single column then use tokenizer and TF-IDF Vectorizer to create a sparse matrix of all the words TF-IDF score. Then you will select and scale the numerical variables and add them into the sparse matrix. You need to perform the following steps for preprocessing: This time the recommender system works way better than the older system, which shows that by adding more relevant data like description text, a content-based recommender system can be improved significantly. Congratulations, you have made it to the end of this tutorial! In this tutorial, you performed an exploratory analysis of the streaming platform movie dataset. You have explored missing values, individual distribution plots, and distribution of movies on each streaming platform. You have also discovered insights on genre, country, language, IMDB ratings, and movie runtime. Finally, you also have seen how to build a recommender system in python. Originally published on: Do you want to learn data science, check out on DataCamp. For more such article, you can visit my blog Machine Learning Geek"
13,Machine Learning,Introduction to Ensemble Techniques: Bagging and Boosting ,"Ensemble Techniques are Machine Learning techniques that combine predictions from several models to give an optimal model. Several models are trained using the same algorithm. This increases accuracy and improves robustness over a single model. By combining several models, ensemble techniques also help reduce the differences caused by bias and variance factors of the model, i.e., reducing underfitting and overfitting. In this article, we will look at two Ensemble techniques – Bagging and Boosting. Before moving on them let’s review Bootstrapping, which is useful to understand Bagging and Boosting. Bootstrapping consists of a random sampling of small subsets of data from the dataset with replacement. The sample sizes are small and the samples are selected from the dataset with equal probability. This gives a better understanding of the mean, variance, and standard deviation from the dataset. The estimated mean of the dataset can be achieved by calculating the mean of all such samples created. Bagging is the short form for Bootstrap Aggregation. This is generally used in case of high-variance data (generally decision trees). It is used to reduce the variance of the model. This is done by choosing random training samples from the dataset with replacement and then creating several subsets of data from that, which are then used to train their models. So, for a model with some observations and features, first Bootstrapping is done. A model is created with a sample of those observations and a subset of features. Those features are selected which give the best split. The same process is used to create several models. These models are parallely trained and average (aggregation) of predictions from all the models is taken. Boosting is used to minimize the bias error in the model. This ensemble technique converts weak learners into strong learners. This is achieved by utilizing weighted averages and sequentially improving on the previous classification. In each iteration for this technique, the weights of the data points that are wrongly classified or predicted are increased. Hence, the next learner in the sequence tries to get those points correct. Thus, the Boosting technique creates random datasets sequentially from the previous data received. There are several types of Boosting algorithms. Some of them are: The decision to choose Bagging or Boosting totally depends on the data. For example if the original data model is facing the problem of overfitting, then it is best to go with Bagging. If we want our model to have better bias, then we can go for the Boosting technique. Both these algorithms lead to better stability and robustness as compared to the original model. In this article, we looked at the ensemble techniques – Bagging and Boosting. In the next article, we will focus on Outlier Detection using Isolation Forests."
14,Machine Learning,Understanding Random Forest Classification and Building a Model in Python ,"Learn how the random forest algorithm works for the classification task. Random forest is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. The random forest creates decision trees on randomly selected data samples, gets a prediction from each tree, and selects the best solution by means of voting. It also provides a pretty good indicator of the feature’s importance. The random forest has a variety of applications such as recommendation engines, image classification, and feature selection. It can be used to classify loyal loan applicants, identify fraudulent activity, and predict diseases. It lies at the base of the Boruta algorithm, which selects important features in a dataset. For more such tutorials and courses visit DataCamp: In this tutorial, you are going to learn about all of the following: Let’s understand the random forest in layman’s words. Suppose you want to go on a trip and you would like to go to a place which you will like. So what you do to identify a better place which you like? You can search online read lots of people’s opinions on travel blogs, Quora, travel portals, or you can also ask your friends. Let’s suppose you have decided to ask your friends and talked with them about their past travel experience in various places. You will get some recommendations from every friend. Now you have to make a list of those recommended places. Then, you ask them to vote(or select one best place for the trip) from a given list of recommended places. The place with the highest number of votes will be your final choice for the trip. In the above decision process, there are two parts. First, asking friends about their individual travel experience and getting one recommendation out of multiple places they have visited. This part is using the decision tree algorithm. Here each friend makes a selection of the places he or she has visited so far. Second, after collecting all the recommendations and you performed the voting procedure for selecting the best place. Voting means choosing the best place for given recommendations on the basis of friends’ experience. This whole process (first and second part both) of recommendation from friends and voting for finding the best place is known as the Random forest algorithm. Technically, the random forest is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on the randomly split dataset. This collection of decision tree classifiers is known as the forest. Every individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index of each attribute. Each tree depends upon the independent random sample. In a classification problem, each tree votes, and the most popular class is chosen as the final result. In the case of regression, the average of all the tree outputs is considered as the final result. It is more simple and powerful compared to the other non-linear classification algorithms. Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-python It works in four steps: The random forest also offers a good feature selection indicator. Scikit-learn provides an extra variable with the random forest model, which shows the relative importance or contribution of each feature in the prediction. It automatically computes the relevance score of each feature in the training phase. Then it scales the relevance down so that the sum of all scores is 1. This score will help to choose the most important features and drop the least important ones for model building. Random forest uses gini importance or mean decrease in impurity (MDI) to calculate the importance of each feature. Gini importance is known as a total decrease in node impurity. This is how much the model fit or accuracy decreases when you drop a variable. The larger the decrease, the more significant the variable is. Here the mean decrease is a significant parameter for variable selection. The gini index can describe the overall explanatory power of the variables. You will be building a model on the iris flower dataset, which is a very famous classification set. It comprises sepal length, sepal width, petal length, petal width, and type of flower. There are three species or classes: Setosa, Versicolor, and Virginica. You will build a model to classify the type of flower. The dataset is available in the scikit-learn library or you can download it from UCI Machine Learning Repository. Start by importing the datasets library from scikit-learn, and load the iris dataset with load_iris(). You can print the target and feature names, to make sure you have the right dataset, as such: It’s a good idea to always explore your data a bit, so you know what you’re working with. Here, you can see the first five rows of the dataset are printed, as well as the target variable for the whole dataset. Output: [[ 5.1 3.5 1.4 0.2][ 4.9 3. 1.4 0.2][ 4.7 3.2 1.3 0.2][ 4.6 3.1 1.5 0.2][ 5. 3.6 1.4 0.2]][0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] Here, you can create a dataframe of iris data set in the following way. Output: First, you separate the columns into dependent and independent variables(or features and labels). Then you split those variables into train and test set. After splitting, you will generate a random forest model on the training set and perform prediction on test set features. After model generation, check the accuracy using actual and predicted values. you can also make a prediction for a single individual. For example, sepal length=3 sepal width=5 petal length=4 petal width=2 Now you can predict the “Which type of flower is?” Output: array([2]) Here, 2 indicates the flower type: ‘Virginica’ Here, you are finding important features or selecting features in a given IRIS dataset. In scikit-learn, you can perform this task in the following steps: Output: petal width (cm) 0.458607petal length (cm) 0.413859sepal length (cm) 0.103600sepal width (cm) 0.023933dtype: float64 You can also visualize the feature importance. Visualization is easy to understand and interpretable. Also, visualization has the highest bandwidth channel to the human brain. For visualizing you can use a combination of matplotlib and seaborn because seaborn built on top of matplotlib, offers a number of customized themes, and provides additional plot types. Matplotlib is a superset of seaborn and both are equally required for good visualization. Here, you can remove the feature “sepal width” and select the remaining 3 features because it has very low importance. After splitting, you will generate a random forest model on selected training set features, perform prediction on selected test set features and compare actual and predicted values. Here you can see, after removing less important features(sepal length) accuracy got increased because it reduces misleading data and noise, which increases the accuracy. Also, less number of important feature reduces training time. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned about what random forest is, how it works, finding important features, comparison between random forest and decision tree, advantages, and disadvantages. You have also learned model building, evaluation, and finding important features in scikit-learn. Don’t stop here! I recommend you try random forest on different datasets and read more on the confusion matrix. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-python Do you want to learn data science, check out on DataCamp. Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
15,Machine Learning,AdaBoost Classifier in Python ,"Understand the ensemble approach, working of the AdaBoost algorithm, and learn the AdaBoost model building in Python. In recent years, boosting algorithms got huge popularity in data science or machine learning competitions. Most of the winners of these competitions use boosting algorithms to achieve high accuracy. These Data science competitions provide the global platform for learning, exploring, and providing solutions for various business and government problems. Boosting algorithms combine multiple low accuracy(or weak) models to create a high accuracy(or strong) models. It can be utilized in various domains such as credit, insurance, marketing, and sales. Boosting algorithms such as AdaBoost, Gradient Boosting, and XGBoost are widely used machine learning algorithm to win the data science competitions. In this tutorial, you are going to learn the AdaBoost ensemble boosting algorithm and the following topics will be covered: For more such tutorials, projects, and courses visit DataCamp An ensemble is a composite model, combines a series of low performing classifiers with the aim of creating an improved classifier. Here, individual classifier vote and final prediction label returned that performs majority voting. Ensembles offer more accuracy than individual or base classifiers. Ensemble methods can parallelize by allocating each base learner to different-different machines. Finally, you can say, Ensemble learning methods are meta-algorithms that combine several machine learning methods into a single predictive model to increase performance. Ensemble methods can decrease variance using the bagging approach, bias using a boosting approach, or improve predictions using the stacking approach. 3. Stacking(or stacked generalization) is an ensemble learning technique that combines multiple base classification models predictions into a new data set. This new data are treated as the input data for another classifier. This classifier employed to solve this problem. Stacking is often referred to as blending. On the basis of the arrangement of base learners, ensemble methods can be divided into two groups: In parallel ensemble methods, base learners are generated in parallel for example. Random Forest. In sequential ensemble methods, base learners are generated sequentially for example AdaBoost. On the basis of the type of base learners, ensemble methods can be divided into two groups: the homogenous ensemble method uses the same type of base learner in each iteration. heterogeneous ensemble method uses the different types of base learners in each iteration. Ada-boost or Adaptive Boosting is one of the ensemble boosting classifiers proposed by Yoav Freund and Robert Schapire in 1996. It combines multiple classifiers to increase the accuracy of classifiers. AdaBoost is an iterative ensemble method. AdaBoost classifier builds a strong classifier by combining multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training data samples in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as a base classifier if it accepts weights on the training set. Adaboost should meet two conditions: It works in the following steps: Let’s first load the required libraries. In the model the building part, you can use the IRIS dataset, which is a very famous multi-class classification problem. This dataset comprises 4 features (sepal length, sepal width, petal length, petal width) and a target (the type of flower). This data has three types of flower classes: Setosa, Versicolour, and Virginia. The dataset is available in the scikit-learn library or you can also download it from the UCI Machine Learning Library. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Let’s create the AdaBoost Model using Scikit-learn. AdaBoost uses the Decision Tree Classifier as a default Classifier. The most important parameters are base_estimator, n_estimators, and learning_rate. Let’s estimate, how accurately the classifier or model can predict the type of cultivars. Accuracy can be computed by comparing actual test set values and predicted values. Well, you got an accuracy of 88.88%, considered as good accuracy. For further evaluation, you can also create a model using different Base Estimators. I have used SVC as a base estimator, you can use any ML learner as a base estimator if it accepts sample weight such as Decision Tree, Support Vector Classifier. Well, you got a classification rate of 95.55%, considered as good accuracy. In this case, SVC Base Estimator is getting better accuracy than the Decision tree Base Estimator. AdaBoost is easy to implement. It iteratively corrects the mistakes of the weak classifier and improves accuracy by combining weak learners. You can use many base classifiers with AdaBoost. AdaBoost is not prone to overfitting. This can be found out via experiment results but there is no concrete reason available. AdaBoost is sensitive to noise data. It is highly affected by outliers because it tries to fit each point perfectly. AdaBoost is slower compared to XGBoost. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned the Ensemble Machine Learning Approaches, AdaBoost algorithm, it’s working, model building, and evaluation using the Python Scikit-learn package. Also, discussed its pros and cons. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn Do you want to learn data science, check out on DataCamp. For more such article, you can visit my blog Machine Learning Geek Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
16,Machine Learning,XGBoost Algorithm using Python ,"XGBoost is one of the most popular boosting algorithms. It is well known to arrive at better solutions as compared to other Machine Learning Algorithms, for both classification and regression tasks. XGBoost or Extreme Gradient Boosting is an open-source library. Its original codebase is in C++, but the library is combined with Python interface. It helps us achieve a comparatively high-performance implementation of gradient boosted decision trees, can do parallel computations and is easy to implement. The Boosting technique trains models serial-wise, with every new learning model trained to correct the errors made by its predecessors. In Gradient Boosting, we try to optimize the loss function of the previous learning model by adding a new adaptive model that combines weak learning models. This reduces the loss function. The technique, thus, controls the learning rate and reduces variance by introducing randomness. This performance can be improved even further by using XGBoost. The objective function of the XGBoost algorithm is a sum of a specific loss function evaluated over all the predictions and the sum of regularization term for all predictors, as: Let’s get started with Python’s XGBoost library. The first step is to install it. This can be easily done with pip command: Now, we need a dataset to work with XGBoost. For this article, we will use the wine dataset from sklearn.datasets.load_wine. This is done as: This is a simple multi-class classification dataset for wine recognition. It has three classes and the number of instances is equal to 178. It has 13 attributes: Now, let’s split the data into training data testing data. This is done as: Next, we need to create the Xgboost specific DMatrix data format. This is done as: Now to get the XGBoost working, we need to set the parameters for xgboost algorithm. Let’s set some parameters for our model, as: objective is the loss function used, while num_class is the number of classes in the dataset. In our case, it is equal to 3. max_depth is the maximum depth of the decision tree. eta is the learning rate, which is the step size shrinkage. It is used in the update to prevent overfitting. gamma is the minimum loss reduction that is required to make a further partition on a leaf node of the decision tree. Now, let’s train our model using XGBoost: 20 is the number of training iterations. The model can be tested as: Let’s look at the accuracy of our predictions: Output: This is a quite good value of accuracy for this dataset. In this article, we looked at XGBoost Algorithm using Python. In the next article, we will focus on Feature Scaling: MinMax, Standard, and Robust Scaler."
17,Machine Learning,Dimensionality Reduction using tSNE ,"tSNE stands for t-distributed Stochastic Neighbor Embedding. It is a dimensionality reduction technique and is extremely useful for visualizing datasets with high dimensions. Dimensionality reduction is the way to reduce the number of features in a model along with preserving the important information that the data carries. Data Visualization is extremely difficult in data with high dimensions, as the human eye can visualize up to three dimensions. tSNE reduces the higher dimensional data to 2D or 3D, which makes visualization easier. tSNE is a non-linear, probabilistic technique. Dimensionality reduction algorithms like PCA (linear algorithm), fail to retain non-linear variance for the data (as it retains only the total variance for the data). tSNE, on the other hand, retains the local structure of the data. Since tSNE is a probabilistic technique, hence the results on the same data would slightly vary from each other. The probability distribution is modeled around the neighbors (closest points) of each data point. It is modeled as a Gaussian distribution in the high-dimensional space while the modeling in the 2D, i.e., lower-dimensional space is t-distribution. The algorithm tries to find a mapping onto the 2D space that minimizes the differences between these two distributions (the Kullback-Leibler divergence) for all points. For the tSNE algorithm, two hyperparameters have considerable influence on the dimensionality reduction. The first one is n_iter, which is the number of iterations required for optimization. This should be at least 250. The other hyperparameter is perplexity, which is almost equal to the number of nearest neighbors that the algorithm must consider. Dimensionality reduction using tSNE can be performed using Python’s sklearn library’s function sklearn.manifold.TSNE(). Let’s consider the following data: This is the MNIST data set that contains the digit data (i.e., the digits from 0 to 9). We can look at the dimensionality of this data, using: This gives the output as (1797, 64). There are a total of 1797 data points. The dimensionality of this dataset is 64. Each data point is an 8×8 image of a digit, so there are 10 classes, i.e., the digits 0 to 9. We will reduce the dimensions of this data from 64 to a lower value. First, let’s store the data and target variables in x and y, as: The variable x is: Output: And the variable y (which stores the actual digits) is: Output: Both x and y are of length 1797. Now let’s perform dimensionality reduction with tSNE on this digits data, by reducing the data to 2-dimensions. This is done as: The data has been reduced to 2-dimensions. The parameter n_components specifies the number of components in the reduced data. We can view the two-dimensional data as: Output: Now, let’s visualize the dimensionality reduction in a 2D plot. Let’s store this data in a DataFrame and then plot the points: Let’s visualize the scatter plot: Output: tSNE has classified the digits into their own clusters. The clusters for specific digits can be viewed more clearly as: Output: We can also change the hyperparameters to arrive at an optimal model. In this article, we looked at Dimensionality Reduction using tSNE. In the next article, we will focus on ensemble techniques such as Bagging and Boosting."
18,Machine Learning,Dimensionality Reduction using PCA ,"Dimensionality refers to the number of input variables (or features) of the dataset. Data with a large number of features might have reduced accuracy. The training time for such data is also slower. There might be several redundant features or those which do not contribute to our goals for analysis. Too many features can also lead to overfitting. Getting rid of such features increases the performance of the model, and also leads to better accuracy. Dimensionality reduction is the way to reduce the number of features in a model along with preserving the important information that the data carries. PCA or Principal Component Analysis is the most popular technique used for dimensionality reduction. In this article, we will look at how PCA (a technique from Linear Algebra) is used for Dimensionality reduction. PCA is an unsupervised transformation method used for linear dimensionality reduction. This technique identifies patterns present in the data based on the correlation between features. The PCA algorithm tries to find the directions of maximum variance (or, spread) in the data with high dimensions and then map it onto data with lesser dimensions. The variance (or, the spread of data) data should be maximum in the lower dimensional space. The above figure shows the direction of the maximum variance. PCA uses the SVD (Singular Value Decomposition) of the data to map it to a lower-dimensional space. SVD decomposes a matrix A (m*n) into three other matrices. Decomposition could also be Eigenvalue based, where a matrix is broken into the set of eigenvector-eigenvalues (Ax = 𝞴x). These are what lead to dimensionality transformation. The principal components are the perpendicular (orthogonal) axis of the new subspace with fewer dimensions in the directions of maximum variance. These are the new feature axes which are orthogonal to each other, essentially the linear combinations of the original input variables. Here, X1 & X2 are the original features, and PC1 & PC2 are the principal components. In simple words, since variance along PC1 is maximum, we can take that as the projected feature (single dimension) instead of having two features X1 & X2. Dimensionality reduction using PCA can be performed using Python’s sklearn library’s function sklearn.decomposition.PCA(). Let’s consider the following data: This is a two-dimensional dataset consisting of 500 data points. The relation between these two features of the data is almost-linear. It can be visualized as: We can use PCA to quantify the relationship between the two features. This is done by finding the principal axes in the data. That can then be used to describe the dataset. Using sklearn, for the above data, it is done as: We can then view the PCA components_, i.e., the principal axes in the feature space, which represent the directions of maximum variance in the dataset. These components are sorted by explained_variance_. Output: explained_variance_ is the amount of variance explained by those selected components above. This is equal to n_components largest eigenvalues of the covariance matrix of x. To look at the explained_variance_ value: Output: Now, let’s perform dimensionality reduction using PCA. Let’s reduce the dimensions from 2D to 1D. Look at the code below: It has been transformed into 1D shape. We can view the components_ and explained_variance_: Output: To visualize this dimensionality reduction, an inverse transform of this reduced data can be performed and visualized along with the data points of original x. This is done as: Output: We can see that it is linearly transformed into a single dimension along the principal axis In this article, we looked at Dimensionality Reduction using PCA. In the next article, we will focus on Dimensionality Reduction using tSNE."
19,Machine Learning,Introduction to Factor Analysis in Python ,"In this tutorial, you’ll learn the basics of factor analysis and how to implement it in Python. Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score. Factor analysis is widely utilised in market research, advertising, psychology, finance, and operation research. Market researchers use factor analysis to identify price-sensitive customers, identify brand features that influence consumer choice, and helps in understanding channel selection criteria for the distribution channel. In this tutorial, you are going to cover the following topics: For more such tutorials, projects, and courses visit DataCamp: Factor analysis is a linear statistical model. It is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms (Source). Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables. Factor analysis is a method for investigating whether a number of variables of interest X1, X2,……., Xl, are linearly related to a smaller number of unobservable factors F1, F2,..……, Fk. Source: This image is recreated from an image that I found in factor analysis notes. The image gives a full view of factor analysis. Assumptions: The primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables. These unobserved variables help the market researcher to conclude the survey. This conversion of the observed variables to unobserved variables can be achieved in two steps: What is a factor? A factor is a latent variable that describes the association among the number of observed variables. The maximum number of factors is equal to a number of observed variables. Every factor explains a certain variance in observed variables. The factors with the lowest amount of variance were dropped. Factors are also known as latent variables or hidden variables or unobserved variables or Hypothetical variables. What are the factor loadings? The factor loading is a matrix that shows the relationship of each variable to the underlying factor. It shows the correlation coefficient for observed variables and factors. It shows the variance explained by the observed variables. What is Eigenvalues? Eigenvalues represent variance explained each factor from the total variance. It is also known as characteristic roots. What are Communalities? Commonalities are the sum of the squared loadings for each variable. It represents the common variance. It ranges from 0–1 and value close to 1 represents more variance. What is Factor Rotation? Rotation is a tool for better interpretation of factor analysis. Rotation can be orthogonal or oblique. It re-distributed the commonalities with a clear pattern of loadings. Kaiser criterion is an analytical approach, which is based on the more significant proportion of variance explained by a factor that will be selected. The eigenvalue is a good criterion for determining the number of factors. Generally, an eigenvalue greater than 1 will be considered as the selection criteria for the feature. The graphical approach is based on the visual representation of factors’ eigenvalues also called scree plots. This scree plot helps us to determine the number of factors where the curve makes an elbow.  Let’s perform factor analysis on BFI (dataset based on personality assessment project), which were collected using a 6 point response scale: 1 Very Inaccurate, 2 Moderately Inaccurate, 3 Slightly Inaccurate 4 Slightly Accurate, 5 Moderately Accurate, and 6 Very Accurate. You can also download this dataset from the following the link: https://vincentarelbundock.github.io/Rdatasets/datasets.html Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means “can we found the factors in the dataset?”. There are two methods to check the factorability or sampling adequacy: Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis. In Bartlett’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix. Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variables. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. The value of KMO less than 0.6 is considered inadequate. The overall KMO for our data is 0.84, which is excellent. This value indicates that you can proceed with your planned factor analysis. For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues. Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables). The scree plot method draws a straight line for each factor and its eigenvalues. Number eigenvalues greater than one considered as the number of factors. Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables). Let’s perform a factor analysis for 5 factors. Total 42% cumulative Variance explained by the 5 factors. Factor analysis explores large datasets and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, It helps in improving the questionnaire for future surveys. Factors make for more natural data interpretation. The results of the factor analysis are controversial. Its interpretations can be debatable because more than one interpretation can be made of the same data factors. After factor identification and naming of factors requires domain knowledge. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned what factor analysis is. The different types of factor analysis, how does factor analysis work, basic factor analysis terminology, choosing the number of factors, comparison of principal component analysis and factor analysis, implementation in Python using Python FactorAnalyzer package, and pros and cons of factor analysis. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/introduction-factor-analysis Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
20,Machine Learning,Introduction to Cluster Analysis ,"Classification of objects or cases into groups is one of the most significant concepts in Data Science and Machine Learning. Cluster analysis is a classification technique that is used to group similar objects into respective categories called clusters. The objects are grouped such that for those in the same group, their degree of association is greater as compared to two objects in two different groups. It can be depicted using a simple diagram: The figure gives a diagrammatic representation of clustering. Thus, we can say that there are three clusters present in the data. Similar data points are grouped together, and data points from different clusters are highly dissimilar. Distances between two clusters (i.e., inter-cluster distances) are maximized, while distances between two objects of a cluster (i.e., intra-cluster distances) are minimized. The distance mentioned is generally Euclidean distance or its square. There are various other distance or similarity metrics that can be used, such as Manhattan distance, Mahalanobis distance, etc. Their expressions for n-dimensional space is: Euclidean Distance: Manhattan Distance: In cluster analysis, there is no prior information about the group membership for the objects of the data. Clustering, hence, can be called as an unsupervised classification as the labels are derived from data. Cluster analysis is significant in a wide variety of Data Science applications as it helps identify and define patterns between data elements. Cluster analysis can also handle high dimensional data. The general way of the workflow of Cluster Analysis is: There are various different methods of Cluster Analysis. Some popular methods are: We will look into these in the upcoming articles. This article was an introduction to one of the most popular Data Analytics concepts – Cluster Analysis. In the upcoming articles, we will look into details about the clustering methods, the algorithms, etc."
21,Machine Learning,K-Means Clustering ,"k-Means Clustering is the Partitioning-based clustering method and is the most popular and widely used method of Cluster Analysis. The unsupervised learning method works on multidimensional data. k-Means is a very fast and efficient algorithm. It is simple and easy to understand. k-means is a centroid-based method, where the number k is defined to refer to the number of centroids (or clusters, as centroid is the center of the cluster) in the data. It then assigns the data points to the nearest cluster. The data points within a cluster should be similar to each other. Hence, we need to minimize the distance between the data points within a cluster using the distance metrics. In K-means, the ‘means’ implies averaging of the data to find the centroid and thus define the cluster. There are certain stopping criteria that can be used to stop the k-Means algorithm: Let’s take a look at an example of k-Means Clustering in Python. The function kMeans() is present in Python’s sklearn library. Consider the following data: Here, we are creating data with three clusters. The function used to create these data points is the make_blobs() function. The parameters for this function for our data are – 300 sample data points, the standard deviation of a cluster is 0.6, the centers are as defined in the code. We can visualize these data points using matplotlib, as: The scatter plot is: Now, let’s apply the k-Means algorithm to the above data points. The python code to do so is: Here, for the k-Means algorithm, we have specified the number of clusters = 3. Let’s visualize how the points are classified in the clusters: The scatter plot is: We can see that the algorithm has identified three clusters. We can also view the centers of these clusters, as: The above code plots the center points in red, as: In this article, we focused on K-means Clustering. In the next article, we will look into a Spectral Clustering."
22,Machine Learning,DBSCAN Clustering ,"Cluster Analysis comprises of many different methods, of which one is the Density-based Clustering Method. DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. For a given set of data points, the DBSCAN algorithm clusters together those points that are close to each other based on any distance metric and a minimum number of points. DBSCAN works on the idea that clusters are dense groups of points. These dense groups of data points are separated by low-density regions. The real-world data contains outliers and noise. It can also have arbitrary shapes (as shown in figures below), due to which the commonly used clustering algorithms (like k-means) fail to perform properly. For such arbitrary shaped clusters or data containing noise, the density-based algorithms such as DBSCAN are more efficient. In the above figures, data points are arbitrarily shaped. Density-based clustering algorithms are used to find the high-density regions and group them into clusters DBSCAN Clustering Algorithm requires two parameters: Hence, after the DBSCAN algorithm, based on these two parameters, there are three types of points: Algorithm: Let’s take a look at an example of DBSCAN Clustering in Python. The function DBSCAN() is present in Python’s sklearn library. Consider the following data: Here, we are creating data with four clusters. The function used to create these data points is the make_blobs() function. The parameters for this function for our data are – 300 sample data points, the standard deviation of a cluster is 0.4, the centers are as defined in the code. We can visualize these data points using matplotlib, as: The scatter plot is: Now, let’s apply the DBSCAN algorithm to the above data points. The python code to do so is: Here, for the DBSCAN algorithm, we have specified ‘eps = 0.3’ and ‘minPts = 25’. Let’s visualize how the points are classified in the clusters: The scatter plot is: We can see that the algorithm has identified four clusters, marked with Yellow, Green, Dark Blue, and Light Blue colors. The Violet points outside each cluster are the noise/outlier points as detected by the algorithm. By modifying the eps and minPts values, we can get different configurations of the clusters. In this article, we focused on DBSCAN Clustering. In the next article, we will look into a Partitioning-based clustering algorithm k-means Clustering."
23,Machine Learning,Spectral Clustering ,"Spectral Clustering is gaining a lot of popularity in recent times, owing to its simple implementation and the fact that in a lot of cases it performs better than the traditional clustering algorithms. The data points are treated as nodes that are connected in a graph-like data structure. Thus, the Cluster Analysis problem is transformed into a graph-partitioning problem. Hence, the problem is to identify the community of nodes based on connected edges, i.e., these connected communities of nodes is mapped such they form clusters. Special matrices such as Adjacency Matrix, Degree Matrix, Laplacian Matrix, etc. are derived from the data set or corresponding graph. Spectral Clustering then uses the information from the eigenvalues (the spectrum) of these special matrices. Spectral Clustering is fast, efficient, and can also be applied to non-graphical data. Eigenvalues are important to understand the concept of Spectral Clustering. Consider a matrix A. If there exist a non-zero vector x and a scalar value λ such that Ax = λx, then λ is said to be the eigenvalue of A for the corresponding eigenvector x. Adjacency matrix (A) is used for the representation of a graph. Data points can be represented in form of a matrix where the row and column indices represent the nodes, and the values in the matrix represent the presence or absence of edge(s) between the nodes. Affinity Matrix is similar to an Adjacency Matrix. The values for a pair of points represent their affinity to each other, i.e., whether they are identical (value=1) or dissimilar (value=0). Degree Matrix (D) is a diagonal matrix where the value at diagonal entry (i,i) is the degree of node i. The degree of a node is the number of edges connect to it. Laplacian Matrix (L) is obtained by subtracting the Adjacency Matrix from the Degree Matrix (L = D – A). Eigenvalues of L, attribute to the properties leveraged by Spectral Clustering. The purpose of calculating the Graph Laplacian is to find eigenvalues and eigenvectors for it. This is then used to project the data points into a low-dimensional space. To identify optimal clusters in data, the Graph Laplacian matrix should approximately be a block diagonal matrix (as shown in the figure). Each block of such a matrix represents a cluster. For example, if we have 3 optimal clusters, then we would expect the Laplacian matrix to look like: In this case, the three lowest eigenvalue-eigenvector pair of the Laplacian correspond to different clusters. The way the Spectral Clustering algorithm works is: Let’s take a look at an example of Spectral Clustering in Python. The function SpectralClustering() is present in Python’s sklearn library. Consider the following data: Here, we are creating data with points distributed in two concentric circles. The function used to create these data points is the make_circles() function. The parameters for this function for our data are – 1000 sample data points, factor=0.5, and noise=0.06 as defined in the code. We can visualize these data points using matplotlib, as: The scatter plot is: If we used the kMeans algorithm on this data, it would not segregate the clusters in the way we want. Let’s look at the clusters formed with kMeans, then we will compare it with Spectral Clustering. The plot using kMeans is: Now let’s apply the Spectral Clustering algorithm to the above data points. The python code to do so is: Here, for the algorithm, we have specified the number of clusters = 2. Let’s visualize how the points are classified in the clusters: The scatter plot with Spectral Clustering is: We can see that the algorithm has identified the clusters. In this article, we focused on Spectral Clustering. In the next article, we will focus on evaluating various Clustering methods."
24,Machine Learning,Agglomerative Clustering ,"There are various different methods of Cluster Analysis, of which the Hierarchical Method is one of the most commonly used. In this method, the algorithm builds a hierarchy of clusters, where the data is organized in a hierarchical tree, as shown in the figure below: Hierarchical clustering has two approaches − the top-down approach (Divisive Approach) and the bottom-up approach (Agglomerative Approach). In this article, we will look at the Agglomerative Clustering approach. In Agglomerative Clustering, initially, each object/data is treated as a single entity or cluster. The algorithm then agglomerates pairs of data successively, i.e., it calculates the distance of each cluster with every other cluster. Two clusters with the shortest distance (i.e., those which are closest) merge and create a newly formed cluster which again participates in the same process. The algorithm keeps on merging the closer objects or clusters until the termination condition is met. This results in a tree-like representation of the data objects – dendrogram. In the above dendrogram, we have 14 data points in separate clusters. In the dendrogram, the height at which two data points or clusters are agglomerated represents the distance between those two clusters in the data space. The first step in agglomerative clustering is the calculation of distances between data points or clusters. Let’s look at some commonly used distance metrics: It is the shortest distance between two points. In n-dimensional space:  The linkage creation step in Agglomerative clustering is where the distance between clusters is calculated. So basically, a linkage is a measure of dissimilarity between the clusters. There are several methods of linkage creation. Some of them are: In Single Linkage, the distance between the two clusters is the minimum distance between clusters’ data points. In Complete Linkage, the distance between two clusters is the maximum distance between clusters’ data points. In Average Linkage, the distance between clusters is the average distance between each data point in one cluster to every data point in the other cluster. Let’s take a look at an example of Agglomerative Clustering in Python. The function AgglomerativeClustering() is present in Python’s sklearn library. Consider the following data: The DataFrame is: Let’s view the dendrogram for this data. The python code to do so is: In this code, Average linkage is used. The dendrogram is: Agglomerative Clustering function can be imported from the sklearn library of python. Looking at three colors in the above dendrogram, we can estimate that the optimal number of clusters for the given data = 3. Let’s create an Agglomerative clustering model using the given function by having parameters as: The labels_ property of the model returns the cluster labels, as: The labels for the above data are: To visualize the clusters in the above data, we can plot a scatter plot as: Visualization for the data and clusters is: The above figure clearly shows the three clusters and the data points which are classified into those clusters. In this article, we focused on Agglomerative Clustering. In the next article, we will look into DBSCAN Clustering."
25,Machine Learning,Evaluating Clustering Methods ,"Predicting optimal clusters is of utmost importance in Cluster Analysis. For a given data, we need to evaluate which Clustering model will best fit the data, or which parameters of a model will give optimal clusters. We often need to compare two clusters or analyze which model would be optimal to deal with outliers. Different performance and evaluation metrics are used to evaluate clustering methods. In this article, we will look at some of these evaluation metrics. Consider the following data: The data is: Now let’s apply the k-Means clustering on this data, as: The clusters are: Changing the model parameters will give different scores for the various evaluation metrics. This could be used to choose optimal parameters and arrive at an optimal model. For now, let’s see how to use the various evaluation metrics on this model. DBI stands for Davies Bouldin Index. It is an internal evaluation method for evaluating clustering algorithms. Lower the value of this metric better is the clustering algorithm. The evaluation of how well the clustering is done by using features inherent to the dataset. Where k is the number of clusters. ∆Ci is the distance within the cluster Ci. δ(Ci, Cj) is the distance between the clusters Ci and Cj. To calculate the DBI for the above kMeans clustering model, the python code is: Output: The Silhouette score is the measure of how similar a data point is to its own cluster as compared to other clusters. A higher Silhouette score value indicates that the data point is better matched to its own cluster and badly matched to other clusters. The best score value is 1 and -1 is the worst. Silhouette coefficient for a sample is defined as: Where a is the average intra-cluster distance, and b is the average nearest-cluster distance. The Silhouette score is the mean of all such samples. To calculate the Silhouette Index on the above kMeans Clustering model, the Python code is: Output: CHS stands for Calinski and Harabasz Score (also known as Variance Ratio Criterion). This metric is the ratio of intra-cluster dispersion and inter-cluster dispersion. To calculate the CHS for the above kMeans clustering model, the python code is: Output: The Rand score is the measure of similarity between two clusters, considering all pairs of samples and using the count of the sample pairs that are assigned in the same or different clusters in the original and predicted cluster. The sklearn function for doing so is adjusted_rand_score() where the Rand score (RS) is adjusted to create Adjusted Rand Score (ARS), as: It returns a value of 1.0 for identical clusterings and 0.0 for random labeling. To calculate the Rand score for the above kMeans clustering model, the python code is: Output: Jaccard Score is defined as the size of the intersection of two labeled sets divided by the size of the union of those two sets. It is a similarity coefficient score that is used to compare a set of predicted labels to the corresponding set of labels in the original value of y. To calculate the Jaccard score for the above kMeans clustering model, the python code is: Output: Since, this is the multiclass case, the average should be specified as None, ‘micro‘, ‘macro‘, or ‘weighted‘. None returns the scores for each class. Other three determine the type of averaging which is to be performed on the data. ‘Micro’ counts the total true positives, false negatives, and false positives for calculating the metrics globally. ‘Macro’ finds the unweighted mean by calculating metrics for each label. ‘Weighted’ calculates metrics for each label and the average which is weighted by support. For binary class ‘binary’ is used. So, for example, using the ‘micro’ option for the above kMeans model: Output: Summary In this article, we looked at various Clustering evaluation methods. In the next article, we will focus on Dimensionality Reduction using PCA."
26,Machine Learning,Grid Search in scikit-learn ,"The performance of our Machine Learning model is largely based on the hyperparameter values for the model. Hence, hyperparameter tuning is a significant step in order to determine the optimal values for our model. This is what is done using Grid Search. Thus, grid search performs an exhaustive search over parameter values specified to find optimal parameters for an estimator. Sklearn’s GridSearchCV function loops through predefined hyperparameters. It fits the model on the training dataset and selects the most optimal parameters for the number of cross-validation times. In Python, grid search is performed using the scikit-learn library’s sklearn.model_selection.GridSearchCV function. Here, we will work with the sklearn’s wine dataset to look into tuning hyperparameters for our model. The first step is to load the dataset: This is a simple multi-class classification dataset for wine recognition. It has three classes and the number of instances is equal to 178. It has 13 attributes. Now, split the data into training and test samples: This splits the wine dataset into training and test sets in a ratio of 70:30. Now let’s create a model with Decision Tree Classifier to classify the wine data, as: Now we define the hyperparameters for DecisionTreeClassifier which we want to try out: These parameters will depend on the type of model/estimator we are using. The dictionary parameters holds all the hyperparameters which we want to test out. Now let’s create an object of GridSearchCV, as: estimator is the model, which is defined as DecisionTreeClassifier() above. param_grid takes the parameter dictionary. scoring defines the type of evaluation metric which we want to use. cv is used to specify the number of cross-validations. verbose=1 gives detailed output then the data is fit to GridSearchCV. Fit the data into the GridSearchCV object: Now, this starts giving output on the screen. While the data is being fitted, it gives something like: We can now extract the best estimator as: Output: To get the score of the optimal model on the testing data: Output: This gives the accuracy of the model on testing data. In this article, we looked at Grid Search in scikit learn. In the next article, we will focus on Recursive Feature Elimination and SelectKBest features."
27,Machine Learning,Cross-Validation in scikit-learn ,"Cross-validation is a statistical method used in Machine Learning for estimating the performance of models. It is very important to know how the model will work on unseen data. The situation of overfitting will cause the model to work perfectly on the training data, but the model loses stability when tested on unknown data. For this purpose, we must ensure that the model learns optimal parameters and gets correct patterns from the training data. This is done using cross-validation. The cross-validation technique splits the whole data several times into different train sets and test sets and then returns the mean value of the prediction scores for all sets. The model is trained on a subset of data and then tested on the complementary subset of data. This process is repeated several times. Thus, the steps involved in cross-validation are: Some of the common methods of cross-validation are: In this, method the entire dataset is split into two parts – the training dataset and the testing dataset. The data is randomly shuffled and then split. The model is trained on the training dataset and then tested on the testing dataset. In the Leave-one-out cross-validation method, training is performed on the whole dataset except one data point on which it is tested. The algorithm then iterates for each data points validating them. The time complexity of this algorithm is huge In the K-fold cross-validation method, the data randomly split into k subsets (or folds). For each fold in the dataset, the model is built on (k – 1) folds of the dataset. The model is then tested for the kth fold. This process is repeated until each of the k-folds has become the testing set. An example of k=5: Stratification is the process in which the data is rearranged in such a way that each fold is a good representative of the whole data. This is exactly what is done in the stratified k-folds method. In Python, Cross-validation can be performed using the scikit-learn library. Here, we will work with the sklearn’s wine dataset. The first step is to load the dataset: This is a simple multi-class classification dataset for wine recognition. It has three classes and the number of instances is equal to 178. It has 13 attributes. In scikit-learn, the simple holdout method can be performed by randomly splitting the dataset into training and test sets using the train_test_split function, as: This splits the wine dataset into training and test sets in a ratio of 70:30. We can have a look at the shape of training and testing variables: Output: Output: Now let’s create a model with SVM classifier, as: For the load_wine dataset, we will need SVM’s linear kernel. Cross-validation can be used on it by calling sklearn’s cross_val_score function on the estimator and the dataset. This can be done as: This is how the estimated accuracy of the model on the dataset is calculated – by splitting the data into train and test, fitting a model, and computing the score cv number of consecutive times, with different splits each time. Here cv=5. The output of the above code is: The mean of the above scores data would give the estimated accuracy of the model. We can compute it as: Output: If the cv argument of the cross_val_score function takes an integer, then the cross_val_score uses the KFold cross-validation or StratifiedKFold method by default. Other cross-validation methods can also be used by passing a cross-validation iterator. K-fold cross-validation can also be performed by using the KFold function from sklearn.model_selection. Similarly, sklearn also provides separate functionalities for Stratified KFold, Leave-one-out cross-validation, and other methods. In this article, we looked at Cross-Validation in scikit-learn. In the next article, we will focus on Grid Search in scikit learn."
28,Machine Learning,"Feature Scaling: MinMax, Standard and Robust Scaler ","Feature Scaling is performed during the Data Preprocessing step. Also known as normalization, it is a method that is used to standardize the range of features of data. Most of the Machine Learning algorithms (for example, Linear Regression) give a better performance when numerical input variables (i.e., numerical features) are scaled to a standard range. Feature Scaling is important as the scale of the input variables of the data can have varying scales. Python’s sklearn library provides a lot of scalers such as MinMax Scaler, Standard Scaler, and Robust Scaler. MinMax Scaler is one of the most popular scaling algorithms. It transforms features by scaling each feature to a given range, which is generally [0,1], or [-1,-1] in case of negative values. For each feature, the MinMax Scaler follows the formula: It subtracts the mean of the column from each value and then divides by the range, i.e, max(x)-min(x). This scaling algorithm works very well in cases where the standard deviation is very small, or in cases which don’t have Gaussian distribution. Let’s look at an example of MinMax Scaler in Python. Consider, the following data: Now, let’s scale this data using sklearn’s MinMax Scaler: Output: We can see that the data points have been scaled to values between 0 and 1. We can also modify the code to scale the values between other range, such as [-2,-2], as: Output: Standard Scaler follows the Standard Normal Distribution, i.e., it assumes a normal distribution for data within each feature. The scaling makes the distribution centered around 0, with a standard deviation of 1 and the mean removed. For a data sample x, its score for Standard Scaler is calculated as: Where sd is the standard deviation of x. Consider the same previous data: Now, let’s scale this data using sklearn’s Standard Scaler: Output: Let’s take another example, a more properly distributed data for this case: Fit Standard Scaler onto this data: We can see the mean value, as: Output: Now apply the transformation: Output: Robust Scaler algorithms scale features that are robust to outliers. The method it follows is almost similar to the MinMax Scaler but it uses the interquartile range (rather than the min-max used in MinMax Scaler). The median and scales of the data are removed by this scaling algorithm according to the quantile range. It, thus, follows the following formula: Where Q1 is the 1st quartile, and Q3 is the third quartile. Let’s see the effect of applying the Robust Scaler on the original data: Output: Robust Scaler uses the Inter Quartile Range by default, which is the range between the 1st quartile and the 3rd quartile. The quantile range can also be manually specified by using the quantile_range parameter, as: Output: In this article, we looked at Feature Scaling, and different scalers such as MinMax, Standard, and Robust Scaler. In the next article, we will focus on Cross-Validation in scikit learn."
29,Machine Learning,Outlier Detection using Isolation Forests ,"For a dataset, an outlier is a data point that behaves differently from the other data points. Outliers cause huge performance losses in Machine Learning algorithms. So to obtain an optimal Machine Learning model, outliers need to be taken care of – either by removing the outlier points or modifying them to some reasonable value for the dataset. For either of these actions, it is necessary to detect the outliers first. Outliers greatly impact the mean, variance, and standard deviation of the dataset. This can result in an increased error in classification or regression models. Thus, it becomes very important to detect and deal with outliers. In this article, we will look at the use of Isolation Forests for the detection of outliers in a dataset. Isolation forest is an unsupervised learning algorithm for outlier detection. The algorithm is based on Decision Trees. The algorithm isolates the observations by selecting a feature randomly. It then randomly chooses a split value between the maximum and minimum values of the feature selected. This can be represented by a decision tree structure. The path length from root to leaf in such decision trees is a measure of normality of the point. This path length is equivalent to the number of splittings required to isolate a sample. Outliers are a minority as compared to other regular data points. In the feature space, they lie further away from the regular data points. Hence, using such kind of decision tree and random partitioning, these points should be identified closer to the root of the tree. This is because outliers have shorter average path length. When several such random decision trees are aggregated into a forest, they most likely produce shorter path lengths for outlier points. Isolation Forests is a fast algorithm and also requires less memory as compared to other outlier detection algorithms. The algorithm can also be scaled for handling high-dimensional datasets. Outlier detection can be done in Python using sklearn’s function sklearn.ensemble.IsolationForest(). Let’s look at a simple example for the following data: Let’s visualize this data: Output: We can see that most of the data points are clustered together, while there are a few points, such as [20,30] which are outliers (or anomalies) for this data. Let’s detect the outliers in this data using Isolation forest: n_estimators is the number of base estimators in the ensemble. Its default value is 100. contamination is the proportion of outliers in the dataset. It defines the threshold on the scores of the samples. By default, it is ‘auto’, but we can also set a value, as in this case = 0.1 fit() trains the algorithm and finds the outliers. Use the predict() method to find the outliers. The outliers detected are assigned a value of -1, while the other points are assigned a value of 1. We can get the outlier points as: Output: We can see that the outliers are detected. In this article, we looked at Outlier Detection using Isolation Forests. In the next article, we will focus on XGBoost Algorithm using Python."
30,NLP,Building Movie Recommender System using Text Similarity ," In this tutorial, we will focus on the movie recommender system using the NLP technique. With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload. In this large amount of information, how to find the right information which meets customer needs. In this context, Recommender System can help us to deal with such huge information. Also, with the increase in user options and rapid change in user preferences, we need some online systems that quickly adapt and recommend the relevant items.  A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings. For example, Netflix is a streaming platform that recommends movies and series and keeps the consumer engaged on their platform. This engagement motivates customers to renew their subscriptions.  Content-based recommender system uses descriptive details products in order to make recommendations. For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using movie description or overview text to discover similar movies for recommendation using text similarity. In this tutorial, we are going to cover the following topics:   In this tutorial, we will build a movie recommender system using text similarity measures. You can download this dataset from here. Let’s load the data into pandas dataframe: Output: In the above code snippet, we have loaded The Movie Database (TMDb) data in Pandas DataFrame. In this section, we can explore the text overview of given movies. for doing exploratory analysis, the best way is to use Wordcloud and understand the most frequent words in the overview of the movie.  In the above code block, we have imported the wordcloud, stopwords, and matplotlib library. First, we created the combined text of all the movie overview descriptions and created the wordcloud on white background.  In the Text Similarity Problems, If we are applying cosine similarity then we have to convert texts into the respective vectors because we directly can’t use text for finding similarity. Let’s create vectors for given movie reviews using the TF-IDF approach.  TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. TF-IDF normalizes the document weights. The higher value of TF-IDF for a word represents a higher occurrence in that document. Output:  In the above code block, Scikit-learn TfidfVectorizer is available for generating the TF-IDF Matrix. Cosine similarity measures the cosine angle between two text vectors. Its value implies that how two documents are related to each other. Cosine similarity can be computed for the non-equal size of text documents. In the above code, we have computed the cosine similarity using the cosine_similarity() method of sklearn.metrics module. Let’s make a forecast using computed cosine similarity on movie description data. In the above code, we have generated the Top-10 movies based on similar movie overview descriptions. Congratulations, you have made it to the end of this tutorial! In the last decade, the use of recommendation systems is increasing rapidly in lots of business ventures such as online retail business, learning, tourism, fashion, and library portals. The recommendation system assists in choosing the right thing from a large number of items by focusing on item features and user profiles. In this tutorial, we have built the movie recommender system using text similarity. In upcoming articles, we will write more articles on different recommender systems using Python. "
31,NLP,Analyzing Sentiments of Restaurant Reviews ,"Perform sentiment analysis on restaurant review dataset.  The online presence of restaurants gives them chance to reach more and more customers. Nowadays restaurants want to understand people’s opinions about their food and service. It helps them to know what customers are thinking about their restaurant? Restaurants can quantify the customer opinion and analyze the data you can quantify such information with good accuracy using sentiment analysis. Traditionally, people take suggestions from acquaintances, friends, and family. But in the 21st-century internet has become one of the most important platforms for seeking advice. On these online platforms, Customer can also share their feedback and view other people’s feedback about restaurant service, food, taste, ambiance, and price. These reviews help customers to make the decision for choosing the restaurant and make a trust because it is based on mouth publicity. In this tutorial, you will analyze the text data and perform sentiment analysis using Text classification. This is how you learn sentiment and text classification with a single example. In this tutorial, you will perform text analytics and sentiment analysis on restaurant reviews using Text Classification.  Here, you can use the “Restaurants Reviews” dataset available on Kaggle. you can download it from the following link: The dataset is a tab-separated file. Dataset has two columns Review and Liked. This data has 2 sentiment labels: 0 — Not liked 1 — Liked  Let’s load the data into pandas dataframe: Output:  Output: In this dataset, we have two columns Review and Liked. Review is the restaurant review and Liked is the sentiment 0 and 1. 1 means the customer has liked the restaurant and 0 means not liked. Wordcloud is the pictorial representation of the most frequent keywords in the dataset. In a wordcloud, the font size of words is directly proportional to the frequency of the word. In order to create a wordcloud we have to first install the module wordcloud using pip command. Let’s create a wordcloud using wordcloud module: Output: In the above word cloud, we can see that the most frequent words are food, service, place, good, great, time, restaurant, and friendly words are used.Let’s create a word cloud for positive restaurant reviews: Output: In this above word cloud, we are showing the frequent keywords of positive reviews. we can see that the most frequent words such as good, great, service, place, friendly, delicious, nice, and amazing.  Output: In the above word cloud, we are showing the frequent keywords of negative reviews. we can see that the most frequent words such as food, place, time, food service, bad, worst, slow, minutes, and disappointed.  In-Text Classification Problem, we have to generate features from the textual dataset. because we directly can’t apply the machine learning models to text data. We need to convert these text into some numbers or vectors of numbers. We are generating a Bag-of-words(BoW) for extracting features from the textual dataset. BoW converts text into the matrix of the frequency of words within a document.  Output:  (1000, 1834) Convert CountVectorized vector into Pandas DataFrame. Output: In order to assess the model performance, we need to split the dataset into a training set and a test set.  Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Here, random_state simply sets seed to the random generator, so that your train-test splits are always deterministic. If you don’t set seed, it is different each time. Let’s build the Text Classification Model using the Multinomial Naive Bayes Classifier. First, import the MultinomialNB module and create a Multinomial Naive Bayes classifier object using MultinomialNB() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Output: MultinomialNB Accuracy: 0.7333333333333333 Well, you got a classification rate of 73.34% using BoW features, which is a good accuracy. We can still improve this performance by trying different feature engineering and classification methods.  We can make predictions on a trained model using predict() method but before predict we must need to convert the input text into the vector using the transform() method.  Output: array([0], dtype=int64) The given input review us predicted as 0 or not liked because of its slow service.  Congratulations, you have made it to the end of this tutorial! In this tutorial, you have performed restaurant review sentiment analysis using text classification with the Scikit-learn library. Learn More about sentiment analysis in this article Sentiment Analysis using Python. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it."
32,NLP,Text Analytics for Beginners using Python NLTK ,"Learn how to analyze text using NLTK.  In today’s area of the internet and online services, data is generating at incredible speed and amount. Generally, Data analysts, engineers, and scientists are handling relational or tabular data. These tabular data columns have either numerical or categorical data. Generated data has a variety of structures such as text, image, audio, and video. Online activities such as articles, website text, blog posts, social media posts are generating unstructured textual data. Corporate and businesses need to analyze textual data to understand customer activities, opinions, and feedback to successfully derive their business. To compete with big textual data, text analytics is evolving at a faster rate than ever before. Text Analytics has lots of applications in today’s online world. by analyzing tweets on Twitter, we can find tending news, people’s reactions to a certain event. Amazon can understand user feedback or review on a certain product. BookMyShow can find people’s opinions about the movie. Youtube can also analyze understand people’s viewpoints on a video. For more such tutorials and courses visit DataCamp: In this tutorial, you are going to cover the following topics: Text communication is one of the most popular forms of day-to-day conversion. We chat, message, tweet, share status, email, write blogs, share opinions, and feedback in our daily routine. These all activities are generating text in a large amount, which is unstructured in nature. In the area of the online marketplace and social media, It is extremely important to analyze large quantities of data, to understand people’s opinions. NLP enables the computer to interact with humans in a natural manner. It helps the computer to understand the human language and derive meaning from it. NLP is applicable in several problems from speech recognition, language translation, classifying documents to information extraction. Analyzing movie reviews is one of the classic examples to demonstrate a simple NLP Bag-of-words model. on movie reviews. Text mining is also referred to as text analytics. Text mining is a process of exploring large textual data and find patterns. Text Mining process the text itself, while the NLP process the underlying metadata. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. Natural language processing is one of the components of text mining. NLP helps identified sentiment, finding entities in the sentence, and categorize of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithms are used to classify information. NLTK is a powerful Python package that provides a set of diverse natural language algorithms. It is free, open source, easy to use, large community, and well documented. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analyze, preprocess, and understand the written text. Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs. Sentence tokenizer breaks text paragraph into sentences. Here, the given text is tokenized into sentences. Word tokenizer breaks text paragraphs into words. Stopwords are considered noise in text. Text may contain stop words such as is, am, are, this, a, an, the, etc. In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words. Lexicon normalization considered as another type of noise in text. for example, connection, connected, connecting word reduce to a common word “connect”. It reduces derivationally related forms of a word to a common root word. Stemming is a process of linguistic normalization, which reduces words to their word root word or chops off the derivational affixes. for example, connection, connected, connecting word reduce to a common word “connect”. Lemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms the root word with the use of vocabulary and morphological analysis. lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word “better” has “good” as its lemma. This thing will miss by stemming because it requires a dictionary look-up. The main target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word. Named entity recognition is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. We can detect entities using ne_chunk() function available in NLTK. Let’s see the following code block: Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned What is Text Analytics, NLP, and text mining?, Basics of text analytics operations using NLTK such as Tokenization, Normalization, Stemming, Lemmatization, and POS tagging.  I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
33,NLP,Text Similarity Measures ,"In this tutorial, we will focus on text similarity measures such as Jaccard and Cosine Similarity. Also, learn how to create a small search engine. Text similarity is used to discover the most similar texts. It used to discover similar documents such as finding documents on any search engine such as Google. We can also use text similarity in document recommendations. Some Q&A websites such as Quora and StackOverflow can also use text similarity to find similar questions. Let’s see the text similarity measures. In this tutorial, we are going to cover the following topics: Jaccard Similarity is the ratio of common words to total unique words or we can say the intersection of words to the union of words in both the documents. it scores range between 0–1. 1 represents the higher similarity while 0 represents the no similarity. Let’s see the formula of Jaccard similarity: Jaccard similarity considers only the unique set of words for each sentence and doesn’t give importance to the duplication of words. Lets see the implementation of Jaccard Simialrity in Python: Output:  {‘like’, ‘i’, ‘dogs.’} {‘hate’, ‘i’, ‘dogs.’}0.5 Cosine similarity measures the cosine of the angle between two vectors. Here vectors can be the bag of words, TF-IDF, or Doc2vec. Let’s the formula of Cosine Similarity: Cosine similarity is best suitable for where repeated words are more important and can work on any size of the document. Let’s see the implementation of Cosine Similarity in Python using TF-IDF vector of Scikit-learn: Output: [[1. 0.33609693] [0.33609693 1. ]] We can also use cosine using Spacy similarity method: Output: 0.880 We can also implement using Scipy: Output: 0.94949 Let’s create a search engine for finding the most similar sentence using Cosine Similarity. In this search engine, we find thje similar sentences for the given input query. Output: I like pizza  Most Similar:I am tired of this stuff. Most Similar Documents: I am tired of this stuff.I love this sandwich.what an awesome viewmy boss is horrible.I do not like this restaurant Congratulations, you have made it to the end of this tutorial! In this article, we have learned text similarity measures such as Jaccard and Cosine Similarity. We have also created one small search engine that finds similar sentences for the given input query. Of course, this is just the beginning, and there’s a lot more that we can do using Text Similarity Measures in Information Retrieval and Text Mining."
34,NLP,Text Analytics for Beginner using Python TextBlob ,"TextBlob is a python library for text analytics and natural language processing operations such as PoS tagging, noun phrases, sentiment analysis, parsing, and text classification.  TextBlob is easy to learn and code for beginners. TextBlob is built using NLTK and Pattern. It provides a few extra functionalities with better results. NLP Operations such as semantic parsing, noun phrase extraction, sentiment analysis, and spell correction perform better with TextBlob than NLTK. In this tutorial, we will focus on the TextBlob library. We will perform tokenization, noun phrase extraction, sentiment analysis, spell correction, translation, and text classification using TextBlob. If you want to learn Spacy and NLTK you can check here for SpaCy and NLTK articles. In this tutorial, we are going to cover the following topics: We will need to install TextBlob before proceeding further. We can do this using the following command-line command: you can also install TextBlob in Juypter Notebook using ! in front of each command to let the Jupyter notebook know that it should be read as a command-line command. Tokenization is the process of splitting text documents into small pieces, known as tokens. It will ignore punctuations and spaces from the text document. Let’s see a word tokenization example in the below code: Output: Output: [Sentence(“I want to be remembered not only as an entertainer but as a person who cared a lot, and I gave the best that I could.”), Sentence(“I tried to be the best role model that I possibly could.”)] A noun phrase is a set of words that belongs to a noun. It can be a subject or object in the sentence. Let’s see an example in the below code: Output: [‘role model’] Part of speech or PoS defines the function of any sentence. For example, the verb identifies the action, noun or adjective identifies the object. Discovering such labels into the data is called PoS tagging. Let’s see an example in the below code: Output: [(‘I’, ‘PRP’), (‘want’, ‘VBP’), (‘to’, ‘TO’), (‘be’, ‘VB’), (‘remembered’, ‘VBN’), (‘not’, ‘RB’), (‘only’, ‘RB’), (‘as’, ‘IN’), (‘an’, ‘DT’), (‘entertainer’, ‘NN’), (‘but’, ‘CC’), (‘as’, ‘IN’), (‘a’, ‘DT’), (‘person’, ‘NN’), (‘who’, ‘WP’), (‘cared’, ‘VBD’), (‘a’, ‘DT’), (‘lot’, ‘NN’), (‘and’, ‘CC’), (‘I’, ‘PRP’), (‘gave’, ‘VBD’), (‘the’, ‘DT’), (‘best’, ‘JJS’), (‘that’, ‘IN’), (‘I’, ‘PRP’), (‘could’, ‘MD’), (‘I’, ‘PRP’), (‘tried’, ‘VBD’), (‘to’, ‘TO’), (‘be’, ‘VB’), (‘the’, ‘DT’), (‘best’, ‘JJS’), (‘role’, ‘NN’), (‘model’, ‘NN’), (‘that’, ‘IN’), (‘I’, ‘PRP’), (‘possibly’, ‘RB’), (‘could’, ‘MD’)] Lemmatization is a process of normalizing the text in a linguistic manner. It chops the given input text and provides the root word of a given word with the use of a vocabulary and morphological analysis. Let’s see an example in the below code: Output: care Output: remember TextBlob has a find() function for searching the word and a count() function for counting the occurrence of any word. Let’s see an example in the below code:  Output: 71 n-grams or bag of word model is used to find the frequency of words in a given text document. Let’s see an example in the below code: Output: 5 In TextBlob, sentiment property returns two scores(polarity, subjectivity) in namedtuple. The polarity score lies between -1 to +1. Negative values show negative sentiment or opinion while positive values show positive opinion or sentiment. The Subjectivity range between 0 and 1. Here, zero means objective and 1 means subjective opinion. TextBlob offers two implementations of sentiment analysis. One is based on a pattern library and the other is based on an NLTK classifier trained on a movie reviews corpus. Let’s see an example in the below code: Output: Sentiment(polarity=0.5, subjectivity=0.65) TextBlob offers spell correction using the correct() function. Let’s see an example in the below code: Output: I have good spelling! TextBlob offers detect_language() function for detection languages and translate() for translate text from one language to another language. It uses Google Translate API. To run these functions, requires an internet connection. Output: hiHello. How are you? In this section, we will focus on text classification which is one of the most important NLP techniques. Text classification will help us in various applications such as document classification, sentiment classification, predicting review rating, spam filtering, support tickets classification, and fake news classification. In this section, our main objective is to prepare a dataset. Let’s prepare data by writing sentences and their sentiment in a tuple: In this section, we are going to create a NaiveBayes classifier using TextBlob. Let’s create a NaiveBayes classifier and train the model. Let’s make prediction on the given input sentence in the below code: Output: ‘pos’ Output: ‘neg’ Let’s evaluate the model performance using the accuracy method: Output: 0.8334 In the above code, we have assessed the performance using accuracy measure and we have got 83.33 % accuracy.  Let’s retrain the model using the update method. First, we will prepare the new dataset and then update the previously trained model.  Output: pos  We can also calculate the probabilities for predicted classes using the prob_classify(text) function.  Let’s see the example below for detailed understanding: Output:   Let’s train the model using the Decision Tree Classifier using TextBlob and evaluate the model performance using the accuracy method. Output: 0.834 Let’s train the model using Maximum Entropy Classifier using TextBlob and evaluate the model performance using the accuracy method. Output: 1.0 TextBlob is built on top of the NLTK and Pattern library. It provides a simple intuitive interface for beginners. It also offers language detection, language translation (powered by Google Translate), Sentiment analysis, and easy-to-use Text Classification functionality. TextBlob is slower than Spacy but faster than NLTK. It does not offer a few NLP tasks such as word vectorization and dependency parsing.  Congratulations, you have made it to the end of this tutorial! In this article, we have learned the basics of the TextBlob library. We have performed various NLP operations such as PoS tagging, noun phrases, sentiment analysis, parsing, spell correction, language detection, language translation, and text classification using Naive Bayes and Decision Tree. Of course, this is just the beginning, and there’s a lot more than TextBlob has to offer for Python data scientists. "
35,NLP,Building a Chatbot using Chatterbot in Python ,"In this tutorial, you’ll learn how to build a chatbot using chatterbot in Python. Are you tired of waiting in long queues for your call to be connected to the customer service executive? Does reading FAQ’s make you feel lethargic? Then you are on the right page. Can you remember the last time you communicated to a customer service agent via chat for the wrong item being delivered to you? There is a high probability that you were being communicated to by a bot rather than a customer service representative. So what exactly are bots? How do we build one? What source of code does it require? These are some of the questions which will be answered in this blog post! Artificial intelligence, which brings into play machine learning and Natural language Processing (NLP) for building a bot or chatbot, is specifically designed to unravel the smooth interaction between humans and computers. Chatbots are everywhere, be it a banking website, pizza store, to e-commerce shopping stores, you will find chatbots left, right, and center. Chatbots provide real-time customer service assistance on a range of pre-defined questions related to the domain it is built on. It adapts natural human language and converses with humans in a human-like manner. To simplify the chatbot’s definition, we can say chatbots are the evolution of Question Answer systems employing natural language processing. As per sources by the year 2024, the global conversation market’s size will grow to $15.7 billion, with 30.2% being the annual growth rate. For instance, amidst the CoronaVirus Pandemic, we have witnessed thousands of hoaxes circulating on WhatsApp, such as what can be used to treat COVID or what can be beneficial in increasing immunity, or whether the virus was developed in a lab. Putting an end to such hoaxes, Facebook launched a chatbot that works as a fact-checker. In this tutorial, we’ll learn Chatbot building in detail, including: Originally published on https://www.datacamp.com/community/tutorials/building-a-chatbot-using-chatterbot The term “chatterbot” came into existence in 1994 when Michael Mauldin created his first chatbot named “Julia”. As per the Oxford Dictionary, a chatbot is defined as “A computer program designed to simulate conversation with human users, especially over the internet.” It can be looked upon as a virtual assistant that communicates with users via text messages and helps businesses in getting close to their customers. It is a program designed to imitate the way humans communicate with each other. It can be done through a chat interface or by voice call. Developers usually design chatbots so that it is difficult to tell users whether they are communicating with a person or a robot. Chatbots helps any business/organization in accomplishing the following goals: Source Chatbots are nothing but software applications that have an application layer, a database, and APIs. To simplify the working of the chatbot, we can say it works on pattern matching to classify text and produce a suitable response for the questions/queries addressed by the user. The chatbot responds to the user as per the program that has been fed in it. Chatbots are of different types, depending on how they are used. Mainly there are three types of chatbots, and they are as follows: Many platforms offer customized chatbots with automation making seamless, always on-time, best-in-class support services available to customers whenever they need it without the box conversational capacities. Customers are also keen to purchase from a business that they can easily connect over messages. Listing down the AI chatbot building platform in 2020: By now, you must be curious to build a chatbot of your own. And what’s better than a customizable NLP chatbot? Let’s get started on building our very own chatbot in Python using library chatterbot. As the name suggests, chatterbot is a python library specifically designed to generate chatbots. This algorithm uses a selection of machine learning algorithms to fabricate varying responses to users as per their requests. Chatterbot makes it easier to develop chatbots that can engage in conversations. It starts by creating an untrained chatterbot that has no prior experience or knowledge regarding how to communicate. As the users enter statements, the library saves the request made by the user as well as it also saves the responses that are sent back to the users. As the number of instances increases in chatterbot, the accuracy of the responses made by chatterbot also increases. Chatterbot is trained to search the closest analogous response by finding the closest analogous request made by users that is equivalent to the new request made. Then it selects a response from the already existing responses. The USP of chatterbot is that it enables developers to create their own dataset and structures at ease. Let’s begin by installing the chatterbot library. For creating chatbot also need to install chatterbot corpus. Corpus — literal meaning is a collection of words. This contains a corpus of data that is included in the chatterbot module. Each corpus is nothing but a prototype of different input statements and their responses. These corpus are used by bots to train themselves. The most recommended method for installing chatterbot and chatterbot_corpus is by using pip. Installation commands for terminal: Installation commands for Jupyter Notebook: Let’s first import the Chatbot class of the chatterbot module. Now, it’s time for the most interesting part i.e., naming your chatbot by creating a Chatbot object. You can choose any name you want. This single line of code generates our very own new bot named Buddy. We need to specify some more parameters before running our first program. You can position the storage adapter with the chatbot object. Storage Adapters allow you to connect to a particular storage unit or network. For using a storage adapter, we need to specify it. We will position the storage adapter by assigning it to the import path of the storage we want to use. Here we are using SQL Storage Adapter, which permits chatbot to connect to databases in SQL. By using the database parameter, we will create a new SQLite Database. Please follow the code below, for creating a new database for chatbot. You can also position the logical adapter with a chatbot object. As the name implies, Logical Adapter regulates the logic behind the chatterbot, i.e., it picks responses for any input provided to it. This parameter contains a list of logical operators. Chatterbot allows us to use a number of logical Adapters. When more than one logical adapter is put to use, the chatbot will calculate the confidence level, and the response with the highest calculated confidence will be returned as output. Here we have used two logical adapters: BestMatch and TimeLogicAdapter. Now the final step in making a chatbot is to train the chatbot using the modules available in chatterbot. Training a chatbot using chatterbot is as simple as providing a conversation into the chatbot database. As soon as the chatbot is given a dataset, it produces the essential entries in the chatbot’s knowledge graph to represent the input and output in the right manner. Firstly, let’s import the ListTrainer, create its object bypassing the Chatbot object, and call the train() the method by passing a list of sentences. The last step of this tutorial is to test the chatterbot’s conversational skills. For testing its responses, we will call the get_responses() method of Chatbot instance. We will create a while loop for our chatbot to run in. When statements are passed in the loop, we will get an appropriate response for it, as we have already entered data into our database. If we get “Bye” or “bye” statement from the user, we can put an end to the loop and stop the program. Congratulations, you have made it to the end of this tutorial! This article was based on learning how to make a chatbot in Python using the ChatterBot library. Building a chatbot with ChatterBot was not only simple but also, the results were accurate. With Artificial Intelligence and Machine Learning, in advancement, everything and anything is possible to achieve whether it is creating bots with conversational skills like humans or be it anything else. Originally published on https://www.datacamp.com/community/tutorials/building-a-chatbot-using-chatterbot Do you want to learn data science, check out on DataCamp."
36,NLP,Latent Dirichlet Allocation using Scikit-learn ,"In this tutorial, we will focus on Latent Dirichlet Allocation (LDA) and perform topic modeling using Scikit-learn. LDA is an unsupervised learning algorithm that discovers a blend of different themes or topics in a set of documents. Latent Dirichlet Allocation is the most popular technique for performing topic modeling. LDA is a probabilistic matrix factorization approach. LDA decomposes large dimensional Document-Term Matrix(DTM) into two lower dimensional matrices: M1 and M2.  In vector space, we can represent any text document as a document-term matrix. Here, m*n matrix has m documents D1, D2, D3 … Dm and vocabulary size of n words W1, W2, W3 .. .Wn. Each cell value is the frequency count of word Wj in Document Di. LDA iterates for each word and tries to assign it to the best topic. The main idea behind LDA is that a document is a combination of topics and each topic is a combination of words.  LDA uses two probabilities: First, probability of words in document d that currently assigned to topic t. Second, probability of assignment of topic t to over all documents.  P1: p(topic t |document d) = Proportion of words in document d that are currently assigned to topic t. P2: p(word w |topic t) = Proportion of assignments to topic t over all documents that come from this word w. In this step, you will load the dataset. You can download data from the following link: In this step, you will generate the TF-IDF matrix for given documents. Here, you will also perform preprocessing operations such as tokenization, and removing stopwords. Scikit-learn offers LatentDirichletAllocation for performing LDA on any Document Term Matrix(DTM). Let’s see the example below(This example will take approx 25 mins on the local machine with 8GB RAM): After performing LDA, we need to extract the topics from the component matrix. Let’s see the example below: Output: In the above example, you can see the 5 topics. If you see keywords of Topic 1([‘election’, ‘new’, ‘rural’, ‘national’, ‘labor’, ‘pm’, ‘says’]) represents Election and Rural Issues. Similarly, Topic 2([‘man’, ‘police’, ‘crash’, ‘charged’, ‘court’, ‘missing’, ‘murder’]) is about Crime and Topic 3 is about Health and Water planning. This is how you can identify topics from the list of tags. Here we have taken 5 topics you can try with different topics and check the performance How it is making sense. For choosing a number of topics you can also use topic coherence explained in Discovering Hidden Themes of Documents article but this article is using the LSI. In this tutorial, you covered Latent Dirichlet Allocation using Scikit learn. LSA is faster and easy to implement. LSA unable to capture the multiple semantic of words. Its accuracy is lower than LDA( Latent Dirichlet Allocation). Topic modeling offers various use cases in Resume Summarization, Search Engine Optimization, Recommender System Optimization, Improving Customer Support, and the healthcare industry."
37,NLP,Latent Semantic Indexing using Scikit-Learn ,"In this tutorial, we will focus on Latent Semantic Indexing or Latent Semantic Analysis and perform topic modeling using Scikit-learn. If you want to implement topic modeling using Gensim then you can refer to this Discovering Hidden Themes of Documents article.  Topic Modelling is an unsupervised technique for discovering the themes of given documents. It extracts the set of co-occurring keywords. These co-occurring keywords represent a topic. For example, stock, market, equity, mutual funds will represent the ‘stock investment’ topic. Latent Semantic Indexing(LSI) or Latent Semantic Analysis (LSA) is a technique for extracting topics from given text documents. It discovers the relationship between terms and documents. LSI concept is utilized in grouping documents, information retrieval, and recommendation engines. LSI discovers latent topics using Singular Value Decomposition.  In this step, you will load the dataset. You can download data from the following link: In this step, you will generate the TF-IDF matrix for given documents. Here, you will also perform preprocessing operations such as tokenization, and removing stopwords. SVD is a matrix decomposition technique that factorizes matrix in the product of matrices. Scikit-learn offers TruncatedSVD for performing SVD. Let’s see the example below: After performing SVD, we need to extract the topics from the component matrix. Let’s see the example below: Output: In the above example, you can see the 10 topics. If you see keywords of Topic 0([‘s’, ‘trump’, ‘said’, ‘EU’, ‘t’]) represents US Politics and Europe. Similarly, Topic 1 is about US Elections and Topic 2 is about Football League. This is how you can identify topics from the list of tags. Here we have taken 10 topics you can try with different topics and check the performance How it is making sense. For choosing a number of topics you can also use topic coherence explained in Discovering Hidden Themes of Documents article. In this tutorial, you covered Latent Semantic Analysis using Scikit learn. LSA is faster and easy to implement. LSA unable to capture the multiple semantic of words. Its accuracy is lower than LDA( latent Dirichlet allocation). Topic modeling offers various usecases in Resume Summarization, Search Engine Optimization, Recommender System Optimization, Improving Customer Support, and the healthcare industry."
38,NLP,Discovering Hidden Themes of Documents ,"Latent Semantic Analysis using Python Discovering topics are very useful for various purposes such as for clustering documents, organizing online available content for information retrieval and recommendations. Various content providers and news agencies are using topic models for recommending articles to readers. Similarly recruiting firms are using in extracting job descriptions and mapping them with candidate skill set. If you see the data scientist job, which is all about extracting the ‘knowledge’ from a large amount of collected data. Mostly collected data is unstructured in nature. you need powerful tools and techniques to analyze and understand a large amount of unstructured data. Topic modeling is a text mining technique that provides methods to identify co-occurring keywords to summarize large collections of textual information. It helps in discovering hidden topics in the document, annotate the documents with these topics, and organize a large amount of unstructured data. In this tutorial, you will cover the following topics: For more such tutorials, projects, and courses visit DataCamp: Topic Modelling automatically discovers the hidden themes from given documents. It is an unsupervised text analytics algorithm that is used for finding a group of words from the given document. These group of words represents a topic. There is a possibility that a single document can associate with multiple themes. for example, group words such as ‘patient’, ‘doctor’, ‘disease’, ‘cancer’, ad ‘health’ will represent the topic ‘healthcare’. Topic Modelling is a different game compared to rule-based text searching that uses regular expressions. Text classification is a supervised machine learning problem, where a text document or article classified into a pre-defined set of classes. Topic modeling is the process of discovering groups of co-occurring words in text documents. These group co-occurring related words makes “topics”. It is a form of unsupervised learning so the set of possible topics are unknown. Topic modeling can be used to solve the text classification problem. Topic modeling will identify the topics presents in a document” while text classification classifies the text into a single class. LSA (Latent Semantic Analysis) also known as LSI (Latent Semantic Index) LSA uses a bag of word(BoW) model, which results in the term-document matrix (occurrence of terms in a document). rows represent terms and columns represent documents.LSA learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition. LSA is typically used as a dimension reduction or noise-reducing technique. SVD is a matrix factorization method that represents a matrix in the product of two matrices. It offers various useful applications in signal processing, psychology, sociology, climate, and atmospheric science, statistics, and astronomy. Identity matrix: It is a square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros. Diagonal Matrix: It is a matrix in which the entries other than the main diagonal are all zero. Singular Matrix: A matrix is singular if its determinant is 0.or A square matrix that does not have a matrix inverse. What is the best way to determine k (number of topics) in topic modeling? Identify the optimum number of topics in a given corpus text is a challenging task. We can use the following options for determining the optimum number of topics: Topic Coherence measure is a widely used metric to evaluate topic models. It uses the latent variable models. Each generated topic has a list of words. In topic coherence measure, you will find the average/median of pairwise word similarity scores of the words in a topic. The high value of the topic coherence score model will be considered as a good topic model. Let’s first create a data load function for loading articles.csv. You can download data from the following link: After data loading function, you need to preprocess the text. The following steps are taken to preprocess the text: The next step is to prepare the corpus. Here, you need to create a document term matrix and dictionary of terms. After corpus creation, you can generate a model using LSA. Another extra step needs to taken to optimize results by identifying the optimum number of topics. Here, you will generate a coherence score to determine an optimum number of topics. Let’s plot coherence score values You can easily evaluate this graph. Here, you have a number of topics on the X-axis and a coherence score on the Y-axis. The number of topics 7 has the highest coherence score, so the optimum number of topics is 7. Output:Total Number of Documents: 4551[(0, ‘0.361“trump” + 0.272“say” + 0.233“said” + 0.166“would” + 0.160“clinton” + 0.140“peopl” + 0.136“one” + 0.126“campaign” + 0.123“year” + 0.110“time”‘), (1, ‘-0.389“citi” + -0.370“v” + -0.356“h” + -0.355“2016” + -0.354“2017” + -0.164“unit” + -0.159“west” + -0.157“manchest” + -0.116“apr” + -0.112“dec”‘), (2, ‘0.612“trump” + 0.264“clinton” + -0.261“eu” + -0.148“say” + -0.137“would” + 0.135“donald” + -0.134“leav” + -0.134“uk” + 0.119“republican” + -0.110“cameron”‘), (3, ‘-0.400“min” + 0.261“eu” + -0.183“goal” + -0.152“ball” + -0.132“play” + 0.128“said” + 0.128“say” + -0.126“leagu” + 0.122“leav” + -0.122“game”‘), (4, ‘0.404“bank” + -0.305“eu” + -0.290“min” + 0.189“year” + -0.164“leav” + -0.153“cameron” + 0.143“market” + 0.140“rate” + -0.139“vote” + -0.133“say”‘), (5, ‘0.310“bank” + -0.307“say” + -0.221“peopl” + 0.203“trump” + 0.166“1” + 0.164“min” + 0.163“0” + 0.152“eu” + 0.152“market” + -0.138“like”‘), (6, ‘0.570“say” + 0.237“min” + -0.170“vote” + 0.158“govern” + -0.154“poll” + 0.122“tax” + 0.115“statement” + 0.115“bank” + 0.112“budget” + -0.108“one”‘)] Here, 7 Topics were discovered using Latent Semantic Analysis. Some of them are overlapping topics. For Capturing multiple meanings with higher accuracy we need to try LDA( latent Dirichlet allocation). I will leave this as an exercise for you, try it out using Gensim, and share your views. LSA algorithm is the simplest method that is easy to understand and implement. It also offers better results compared to the vector space model. It is faster compared to other available algorithms because it involves document term matrix decomposition only. The latent topic dimension depends upon the rank of the matrix so we can’t extend that limit. LSA decomposed matrix is a highly dense matrix so It is difficult to index individual dimensions. LSA unable to capture the multiple meanings of words. It is not easier to implement compared to LDA( latent Dirichlet allocation). It offers lower accuracy than LDA. Simple applications in which this technique is used are documented clustering in text analysis, recommender systems, and information retrieval. More detailed use-cases of topic modeling are: In this tutorial, you covered a lot of details about Topic Modeling. you have learned what is the Topic Modeling, what is Latent Semantic Analysis, how to build respective models, how topics generated using LSA. Also, you covered some basic concepts such as the Singular Value Decomposition, topic coherence score. Hopefully, you can now utilize topic modeling to analyze your own datasets. Thanks for reading this tutorial! For more such tutorials, projects, and courses visit DataCamp: Originally published at https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/ WRITTEN BYAvinash Navlani"
39,NLP,Text Summarization using Python ,"Learn how to summarize text using extractive summarization techniques such as TextRank, LexRank, LSA, and KL-Divergence.  A summary is a small piece of text that covers key points and conveys the exact meaning of the original document. Text summarization is a method for concluding a document into a few sentences. It can be performed in two ways: The abstractive method produces a summary with new and innovative words, phrases, and sentences. The extractive method will take the same words, phrases, and sentences from the original summary. Extractive methods can be considered as important sentence selection in the given text. In this tutorial, our main focus is on extractive summarization techniques such as Text Rank, Lex Rank, LSA, and KL Divergence. Text Rank is a kind of graph-based ranking algorithm used for recommendation purposes. TextRank is used in various applications where text sentences are involved. It worked on the ranking of text sentences and recursively computed based on information available in the entire text. TextRank builds the graph related to the text. In a graph, each sentence is considered as vertex and each vertex is linked to the other vertex. These vertices cast a vote for another vertex. The importance of each vertex is defined by the higher number of votes. This importance is Here goal is to rank the sentences and this rank. TextRank works in the following steps:1. Tokenize documents into sentences.2. Preprocess each sentence in the document.3. Count key phrases and normalize them or produce TFIDF Matrix, you can also use any kind of vectorization such as spacy vectors.4. Calculate the Jaccard Similarity between sentences and key phrases.5. Rank the sentences with higher significance. TextRank is a graph-based algorithm, easy to understand and implement. Its results are less semantic. Let’s do hands-on using gensim and sumy package. gensim package is used for natural language processing and information retrievals tasks such as topic modeling, document indexing, wro2vec, and similarity retrieval. Here we are using it for text summarization. Install gensim using pip:  Output: sumy is an automatic text summarized library that is a simple library and easy to use. It has implemented various summarization algorithms such as TextRank, Luhn, Edmundson, LSA, LexRank, KL-Sum, SumBasic, and Reduction. Let’s see an example of a TextRank algorithm. Install sumy using pip:  LexRank is another graph-based method for summarization. It is similar to TextRank and unsupervised in nature. LexRank uses Cosine similarity instead of Jaccard Similarity between two sentences. This similarity score will be used to build a weighted graph for all the sentences in the document. LexRank also takes care of the chosen top most sentences will not too similar to each other. LexRank is a graph-based algorithm, easy to understand and implement. Its results are less semantic. Latent Semantic Analysis is based on Singular value decomposition(SVD). It reduces the data into lower-dimensional space. It performs spatial decomposition and captures information in a singular vector and the magnitude of so singular vector will represent the importance. LSA has the capability to extract semantically related sentences but its computation is complex. KL-Divergence calculates the difference between two probability distributions. Probability distribution P to an arbitrary probability distribution Q. It measures between the unigram probability distributions learned from seen document set p(w/R) and new document set q(w/N). KL divergence approach takes a matrix of the KLD value of sentences from the input document. We select the sentences with lower KLD values for creating a summary. KL divergence generates good summaries that are intuitively similar to the original document. Let’s see hands-on using sumy package. In this tutorial, we have seen Text summarization, its types, and some of the extractive algorithms such as TextRank, LexRank, LSA, and KL Divergence. we have performed hands-on on these algorithms using sumy package. Also, we have used gensim module for the TextRank algorithm. In this summarization, we can’t say which algorithm is working better than others because in some cases one algorithm will show the better and in some other cases other algorithms will show better results. For a more intuitive summary, I will recommend you can try with a hybrid approach or go for abstractive summarization."
40,NLP,Sentiment Analysis using Python ,"Analyze people’s sentiments and classify movie reviews Nowadays companies want to understand, what went wrong with their latest products? what users and the general public think about the latest feature? you can quantify such information with good accuracy using sentiment analysis. Quantifying the user’s content, idea, belief, and opinion are known as sentiment analysis. User’s online post, blogs, tweets, feedback of product helps business people to the target audience and innovate in products and services. sentiment analysis helps in understanding people in a better and more accurate way, It is not only limited to marketing, but it can also be utilized in politics, research, and security. Human communication just not limited to words, it is more than words. Sentiments are combination words, tone, and writing style. As a data analyst, It is more important to understand our sentiments, what it really means? There are mainly two approaches for performing sentiment analysis In this tutorial, you will use the second approach(Machine learning-based approach). This is how you learn sentiment and text classification with a single example. Text classification is one of the important tasks of text mining. It is a supervised approach. Identifying category or class of given text such as blog, book, web page, news articles, and tweets. It has various applications in today’s computer world such as spam detection, task categorization in CRM services, categorizing products on E-retailer websites, classifying the content of websites for a search engine, sentiments of customer feedback, etc. In the next section, you will learn how you can do text classification in python. Till now, you have learned data preprocessing using NLTK. Now, you will learn Text Classification. you will perform Multi-Nomial Naive Bayes Classification using scikit-learn. In the model the building part, you can use the “Sentiment Analysis of Movie, Reviews” dataset available on Kaggle. The dataset is a tab-separated file. Dataset has four columns PhraseId, SentenceId, Phrase, and Sentiment. This data has 5 sentiment labels: 0 — negative 1 — somewhat negative 2 — neutral 3 — somewhat positive 4 — positive Here, you can build a model to classify the type of cultivar. The dataset is available on Kaggle, you can download it from the following link: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data In-Text Classification Problem, we have a set of texts and their respective labels. but we directly can’t use text for our model. you need to convert these text into some numbers or vectors of numbers. Bag-of-words model(BoW ) is the simplest way of extracting features from the text. BoW converts text into the matrix of the occurrence of words within a document. This model concerns whether given words occurred or not in the document. Example: There are three documents: Doc 1: I love dogs. Doc 2: I hate dogs and knitting. Doc 3: Knitting is my hobby and passion. Now, you can create a matrix of documents and words by counting the occurrence of words in a given document. This matrix is known as the Document-Term Matrix(DTM). This matrix is using a single word. It can be a combination of two or more words, which is called the bigram or trigram model and the general approach is called the n-gram model. You can generate a document term matrix by using scikit-learn’s CountVectorizer. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Let’s build the Text Classification Model using TF-IDF. First, import the MultinomialNB module and create Multinomial Naive Bayes classifier object using MultinomialNB() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Well, you got a classification rate of 60.49% using CountVector(or BoW), which is not considered as good accuracy. We need to improve this. In Term Frequency(TF), you just count the number of words that occurred in each document. The main issue with this Term Frequency is that it will give more weight to longer documents. Term frequency is basically the output of the BoW model. IDF(Inverse Document Frequency) measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents. TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. Word with high tf-idf in a document, it is most of the time that occurred in given documents and must be absent in the other documents. So the words must be a signature word. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly. Let’s build the Text Classification Model using TF-IDF. First, import the MultinomialNB module and create Multinomial Naive Bayes classifier object using MultinomialNB() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Well, you got a classification rate of 58.65% using TF-IDF features, which is not considered as good accuracy. We need to improve accuracy using some other preprocessing or feature engineering. Let’s suggest in the comment box some approach for accuracy improvement. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned What is sentiment analysis, Feature Engineering, and text classification using scikit-learn. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it.  Originally published at https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
41,NLP,Text Classification using Python spaCy ,"In the previous two articles on text analytics, we’ve looked at some of the cool things spaCy can do in general. In this article, we will learn how to derive meaningful patterns and themes from text data. This is useful in a wide variety of data science applications: spam filtering, support tickets, social media analysis, contextual advertising, reviewing customer feedback, and more. In this article, We’ll dive into text classification using spacy, specifically Logistic Regression Classification, using some real-world data (text reviews of Amazon’s Alexa smart home speaker). Let’s look at a bigger real-world application of some of these natural language processing techniques: text classification. Quite often, we may find ourselves with a set of text data that we’d like to classify according to some parameters (perhaps the subject of each snippet, for example) and text classification is what will help us to do this. The diagram below illustrates the big-picture view of what we want to do when classifying text. First, we extract the features we want from our source text (and any tags or metadata it came with), and then we feed our cleaned data into a machine learning algorithm that does the classification for us. We’ll start by importing the libraries we’ll need for this task. We’ve already imported spaCy, but we’ll also want pandas and scikit-learn to help with our analysis. Above, we have looked at some simple examples of text analysis with spaCy, but now we’ll be working on some Logistic Regression Classification using scikit-learn. To make this more realistic, we’re going to use a real-world data set—this set of Amazon Alexa product reviews. This data set comes as a tab-separated file (.tsv). It has five columns: rating, date, variation, verified_reviews, feedback. rating denotes the rating each user gave the Alexa (out of 5). date indicates the date of the review, and variation describes which model the user reviewed. verified_reviews contains the text of each review, and feedback contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t). This dataset has consumer reviews of amazon Alexa products like Echos, Echo Dots, Alexa Firesticks, etc. What we’re going to do is develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the feedback column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative. Let’s start by reading the data into a pandas dataframe and then using the built-in functions of pandas to help us take a closer look at our data. Now that we know what we’re working with, let’s create a custom tokenizer function using spaCy. We’ll use this function to automatically strip information we don’t need, like stopwords and punctuation, from each review. We’ll start by importing the English models we need from spaCy, as well as Python’s string module, which contains a helpful list of all punctuation marks that we can use in string.punctuation. We’ll create variables that contain the punctuation marks and stopwords we want to remove, and a parser that runs input through spaCy‘s English module. Then, we’ll create a spacy_tokenizer() a function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words. This is similar to what we did in the examples earlier in this tutorial, but now we’re putting it all together into a single function for preprocessing each user review we’re analyzing. To further clean our text data, we’ll also want to create a custom transformer for removing initial and end spaces and converting text into lower case. Here, we will create a custom predictors class wich inherits the TransformerMixin class. This class overrides the transform, fit and get_parrams methods. We’ll also create a clean_text() function that removes spaces and converts text into lowercase. When we classify text, we end up with text snippets matched with their respective labels. But we can’t simply use text strings in our machine learning model; we need a way to convert our text into something that can be represented numerically just like the labels (1 for positive and 0 for negative) are. Classifying text in positive and negative labels is called sentiment analysis. So we need a way to represent our text numerically. One tool we can use for doing this is called Bag of Words. BoW converts text into the matrix of the occurrence of words within a given document. It focuses on whether given words occurred or not in the document, and it generates a matrix that we might see referred to as a BoW matrix or a document term matrix. We can generate a BoW matrix for our text data by using scikit-learn‘s CountVectorizer. In the code below, we’re telling CountVectorizer to use the custom spacy_tokenizer function we built as its tokenizer and defining the ngram range we want. N-grams are combinations of adjacent words in a given text, where n is the number of words that included in the tokens. for example, in the sentence “Who will win the football world cup in 2022?” unigrams would be a sequence of single words such as “who”, “will”, “win” and so on. Bigrams would be a sequence of 2 contiguous words such as “who will”, “will win”, and so on. So the ngram_range parameter we’ll use in the code below sets the lower and upper bounds of our ngrams (we’ll be using unigrams). Then we’ll assign the ngrams to bow_vector. We’ll also want to look at the TF-IDF (Term Frequency-Inverse Document Frequency) for our terms. This sounds complicated, but it’s simply a way of normalizing our Bag of Words(BoW) by looking at each word’s frequency in comparison to the document frequency. In other words, it’s a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document. We can represent this with the following mathematical equation: Of course, we don’t have to calculate that by hand! We can generate TF-IDF automatically using scikit-learn‘s TfidfVectorizer. Again, we’ll tell it to use the custom tokenizer that we built with spaCy, and then we’ll assign the result to the variable tfidf_vector. We’re trying to build a classification model, but we need a way to know how it’s actually performing. Dividing the dataset into a training set and a test set the tried-and-true method for doing this. We’ll use half of our data set as our training set, which will include the correct answers. Then we’ll test our model using the other half of the data set without giving it the answers, to see how accurately it performs. Conveniently, scikit-learn gives us a built-in function for doing this: train_test_split(). We just need to tell it the feature set we want it to split (X), the labels we want it to test against (ylabels), and the size we want to use for the test set (represented as a percentage in decimal form). Now that we’re all set up, it’s time to actually build our model! We’ll start by importing the LogisticRegression module and creating a LogisticRegression classifier object. Then, we’ll create a pipeline with three components: a cleaner, a vectorizer, and a classifier. The cleaner uses our predictors class object to clean and preprocess the text. The vectorizer uses countvector objects to create the bag of words matrix for our text. A classifier is an object that performs the logistic regression to classify the sentiments. Once this pipeline is built, we’ll fit the pipeline components using fit(). Let’s take a look at how our model actually performs! We can do this using the metrics module from scikit-learn. Now that we’ve trained our model, we’ll put our test data through the pipeline to come up with predictions. Then we’ll use various functions of the metrics module to look at our model’s accuracy, precision, and recall. The documentation links above offer more details and more precise definitions of each term, but the bottom line is that all three metrics are measured from 0 to 1, where 1 is predicting everything completely correctly. Therefore, the closer our model’s scores are to 1, the better. In other words, overall, our model correctly identified a comment’s sentiment 94.1% of the time. When it predicted a review was positive, that review was actually positive 95% of the time. When handed a positive review, our model identified it as positive 98.6% of the time Congratulations, you have made it to the end of this tutorial! In this article, we have built our own machine learning model with scikit-learn. Of course, this is just the beginning, and there’s a lot more that both spaCy and scikit-learn have to offer Python data scientists.  This article is originally published at https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
42,NLP,Text Analytics for Beginners using Python spaCy Part-2 ,"In the previous article on text analytics for beginners using Python part-1, we’ve looked at some of the cool things spaCy can do in general. We have seen what is natural language processing (NLP)? Then we have seen text analytics basic operations for cleaning and analyzing text data with spaCy. In this article, we will learn other important topics of NLP: entity recognition, dependency parsing, and word vector representation using spaCy.  Entity detection, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text since you can quickly pick out important topics or identify key sections of text. Let’s try out some entity detection using a few paragraphs from this recent article. We’ll use .label to grab a label for each entity that’s detected in the text, and then we’ll take a look at these entities in a more visual format using spaCy‘s displaCy visualizer. Using this technique, we can identify a variety of entities within the text. The spaCy documentation provides a full list of supported entity types, and we can see from the short example above that it’s able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc. Using displaCy we can also visualize our input text, with each identified entity highlighted by color and labeled. We’ll use style = ""ent"" to tell displaCy that we want to visualize entities here. Dependency parsing is a language processing technique that allows us to better determine the meaning of a sentence by analyzing how it’s constructed to determine how the individual words relate to each other. Consider, for example, the sentence “Bill throws the ball.” We have two nouns (Bill and ball) and one verb (throws). But we can’t just look at these words individually, or we may end up thinking that the ball is throwing Bill! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words and their parts of speech. Doing this is quite complicated, but thankfully spaCy will take care of the work for us! Below, let’s give spaCy another short sentence pulled from the news headlines. Then we’ll use another spaCy called noun_chunks, which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to. This output can be a little bit difficult to follow, but since we’ve already imported the displaCy visualizer, we can use that to view a dependency diagram using style = ""dep"" that’s much easier to understand: Of course, we can also check out spaCy‘s documentation on dependency parsing to get a better understanding of the different labels that might get applied to our text depending on how each sentence is interpreted. When we’re looking at words alone, it’s difficult for a machine to understand connections that a human would understand immediately. Engine and car, for example, have what might seem like an obvious connection (cars run using engines), but that link is not so obvious to a computer. Thankfully, there’s a way we can represent words that captures more of these sorts of connections. A word vector is a numeric representation of a word that communicates its relationship to other words. Each word is interpreted as a unique and lengthy array of numbers. You can think of these numbers as being something like GPS coordinates. GPS coordinates consist of two numbers (latitude and longitude), and if we saw two sets of GPS coordinates that were numerically close to each other (like 43,-70, and 44,-70), we would know that those two locations were relatively close together. Word vectors work similarly, although there are a lot more than two coordinates assigned to each word, so they’re much harder for a human to eyeball. Using spaCy‘s en_core_web_sm model, let’s take a look at the length of a vector for a single word, and what that vector looks like using .vector and .shape. There’s no way that a human could look at that array and identify it as meaning “mango,” but representing the word this way works well for machines because it allows us to represent both the word’s meaning and its “proximity” to other similar words using the coordinates in the array. Congratulations, you have made it to the end of this tutorial! In this tutorial, we have gone entity recognition, entity recognition, dependency parsing, and word vector representation with spaCy. In the next article, we will build our own machine learning model with scikit-learn and perform text classification. Text Classification using Python spaCy This article is originally published at https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
43,NLP,Text Analytics for Beginners using Python spaCy Part-1 ,"Learn the basics of the most powerful NLP library Spacy. Text is an extremely rich source of information. Each minute, people send hundreds of millions of new emails and text messages. There’s a veritable mountain of text data waiting to be mined for insights. But data scientists who want to glean meaning from all of that text data face a challenge: it is difficult to analyze and process because it exists in unstructured form. In this tutorial, we’ll take a look at how we can transform all of that unstructured text data into something more useful for analysis and natural language processing, using the helpful Python package spaCy (documentation). Specifically, we’re going to take a high-level look at natural language processing (NLP). Then we’ll work through some of the important basic operations for cleaning and analyzing text data with spaCy.  In this tutorial, we are going to cover the following topics: Natural language processing (NLP) is a branch of machine learning that deals with processing, analyzing, and sometimes generating human speech (“natural language”). There’s no doubt that humans are still much better than machines at determining the meaning of a string of text. But in data science, we’ll often encounter data sets that are far too large to be analyzed by a human in a reasonable amount of time. We may also encounter situations where no human is available to analyze and respond to a piece of text input. In these situations, we can use natural language processing techniques to help machines get some understanding of the text’s meaning (and if necessary, respond accordingly). For example, natural language processing is widely used in sentiment analysis, since analysts are often trying to determine the overall sentiment from huge volumes of text data that would be time-consuming for humans to comb through. It’s also used in advertisement matching — determining the subject of a body of text and assigning a relevant advertisement automatically. And it’s used in chatbots, voice assistants, and other applications where machines need to understand and quickly respond to input that comes in the form of natural human language. spaCy is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently. First, let’s take a look at some of the basic analytical tasks spaCy can handle. We’ll need to install spaCy and its English-language model before proceeding further. We can do this using the following command line commands:   We can also use spaCy in a Juypter Notebook. It’s not one of the pre-installed libraries that Jupyter includes by default, though, so we’ll need to run these commands from the notebook to get spaCy installed in the correct Anaconda directory. Note that we use ! in front of each command to let the Jupyter notebook know that it should be read as a command-line command. Tokenization is the process of breaking text into pieces, called tokens, and ignoring characters like punctuation marks (,. “ ‘) and spaces. spaCy‘s tokenizer takes input in form of Unicode text and outputs a sequence of token objects. Let’s take a look at a simple example. Imagine we have the following text, and we’d like to tokenize it: There are a couple of different ways we can approach this. The first is called word tokenization, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require input in the form of individual words rather than long strings of text. In the code below, we’ll import spaCy and its English-language model, and tell it that we’ll be doing our natural language processing using that model. Then we’ll assign our text string to text. Using nlp(text), we’ll process that text in spaCy and assign the result to a variable called my_doc. At this point, our text has already been tokenized, but spaCy stores tokenized text as a doc, and we’d like to look at it in list form, so we’ll create a for loop that iterates through our doc, adding each word token it finds in our text string to a list called token_list so that we can take a better look at how words are tokenized. As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as shouldn’t actually represent two distinct words, and it has thus broken them down into two distinct tokens. First, we need to load language dictionaries, Here in the above example, we are loading an English dictionary using the English() class and creating NLP object. “nlp” object is used to create documents with linguistic annotations and various NLP properties. After creating a document, we are creating a token list. If we want, we can also break the text into sentences rather than words. This is called sentence tokenization. When performing sentence tokenization, the tokenizer looks for specific characters that fall between sentences, like periods, exclamation points, and newline characters. For sentence tokenization, we will use a preprocessing pipeline because sentence preprocessing using spaCy includes a tokenizer, a tagger, a parser, and an entity recognizer that we need to access to correctly identify what’s a sentence and what isn’t. In the code below,spaCy tokenizes the text and creates a Doc object. This Doc object uses our preprocessing pipeline’s components tagger, parser, and entity recognizer to break the text down into components. From this pipeline, we can extract any component, but here we’re going to access sentence tokens using the sentencizer component. Again, spaCy has correctly parsed the text into the format we want, this time outputting a list of sentences found in our source text. Most text data that we work with is going to contain a lot of words that aren’t actually useful to us. These words, called stopwords, are useful in human speech, but they don’t have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process). Let’s take a look at the stopwords spaCy includes by default. We’ll import spaCy and assign the stopwords in its English-language model to a variable called spacy_stopwords so that we can take a look. As we can see, spaCy‘s a default list of stopwords includes 312 total entries, and each entry is a single word. We can also see why many of these words wouldn’t be useful for data analysis. Transition words like nevertheless, for example, aren’t necessary for understanding the basic meaning of a sentence. And other words like somebody are too vague to be of much use for NLP tasks. If we wanted to, we could also create our own customized list of stopwords. But for our purposes in this tutorial, the default list that spaCy provides will be fine. Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section. Our text is already stored in the variable text, so we don’t need to define that again. Instead, we’ll create an empty list called filtered_sent and then iterate through our doc variable to look at each tokenized word from our source text. spaCy includes a bunch of helpful token attributes, and we’ll use one of them called is_stop to identify words that aren’t in the stopword list and then append them to our filtered_sent list. It’s not too difficult to see why stopwords can be helpful. Removing them has boiled our original text down to just a few words that give us a good idea of what the sentences are discussing: learning data science, and discouraging challenges and setbacks along that journey. Lexicon normalization is another step in the text data cleaning process. In the big picture, normalization converts high dimensional features into low dimensional features that are appropriate for any machine learning model. For our purposes here, we’re only going to look at lemmatization, a way of processing words that reduces them to their roots. Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself. One method for doing this is called stemming. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple stemming is often all that’s needed, but lemmatization — which actually looks at words and their roots (called lemma) as described in the dictionary — is more precise (as long as the words exist in the dictionary). Since spaCy includes a built-in way to break a word down into its lemma, we can simply use that for lemmatization. In the following very simple example, we’ll use .lemma_ to produce the lemma for each word we’re analyzing. A word’s part of speech defines its function within a sentence. A noun, for example, identifies an object. An adjective describes an object. A verb describes the action. Identifying and tagging each word’s part of speech in the context of a sentence is called Part-of-Speech Tagging, or POS Tagging. Let’s try some POS tagging with spaCy! We’ll need to import its en_core_web_sm model, because that contains the dictionary and grammatical information required to do this analysis. Then all we need to do is load this model with .load() and loop through our new docs variable, identifying the part of speech for each word using .pos_. (Note u in u""All is well that ends well."" signifies that the string is a Unicode string.) Hooray! spaCy has correctly identified the part of speech for each word in this sentence. Being able to identify parts of speech is useful in a variety of NLP-related contexts because it helps more accurately understand input sentences and more accurately construct output responses. Over the course of this tutorial, we’ve gone from performing some very simple text analysis operations with spaCy. Of course, this is just the beginning, and there’s a lot more in thespaCy. In the next tutorial, we will see other important topics of NLP such as entity recognition, dependency parsing, and word vector representation. Text Analytics for Beginners using Python spaCy Part-2 This article is originally published at https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
44,NLP,Custom Entity Recognition Model using Python spaCy ,"Train your Customized NER model using spaCy In the previous article, we have seen the spaCy pre-trained NER model for detecting entities in text. In this tutorial, our focus is on generating a custom model based on our new dataset. The entity is an object and a named entity is a “real-world object” that’s assigned a name such as a person, a country, a product, or a book title in the text that is used for advanced text processing. Entity recognition identifies some important elements such as places, people, organizations, dates, and money in the given text. Recognizing entities from text is helpful for analysts to extract useful information for decision-making. You can understand the entity recognition from the following example in the image: Let’s create the NER model in the following steps: In this step, we will load the data, initialize the parameters, and create or load the NLP model. Let’s first import the required libraries and load the dataset. Now, we will create a model if there is no existing model otherwise we will load the existing model. Let’s see the code below: In this step, we will create an NLP pipeline. First, we check if there is any pipeline existing then we use the existing pipeline otherwise we will create a new pipeline. Let’s see the code below: In this step, we will add entities’ labels to the pipeline. First, we iterate the training dataset and then we add each entity to the model. Let’s see the code below: In this step, we will train the NER model. First, we disable all other pipelines and then we go only for NER training. In NER training, we will create an optimizer. after that, we will update nlp model based on text and annotations in the training dataset. This process continues to a defined number of iterations. Let’s see the code below: In this step, we will save and test the NER custom model. to save the model we will use to_disk() method. For testing, first, we need to convert testing text into nlp object for linguistic annotations. Let’s see the code below for saving and testing the model: Congratulations, you have made it to the end of this tutorial! In this tutorial, we have seen how to generate the NER model with custom data using spaCy. spaCy is built on the latest techniques and utilized in various day-to-day applications. It offers basic as well as NLP tasks such as tokenization, named entity recognition, PoS tagging, dependency parsing, and visualizations. For more such tutorials, projects, and courses visit DataCamp Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
45,NLP,Explore Python Gensim Library For NLP ,"In this tutorial, we will focus on the Gensim Python library for text analysis.  Gensim is an acronym for Generate Similar. It is a free Python library for natural language processing written by Radim Rehurek which is used in word embeddings, topic modeling, and text similarity. It is developed for generating word and document vectors. It also extracts the topics from textual documents. It is an open-source, scalable, robust, fast, efficient multicore Implementation, and platform-independent.  In this tutorial, we are going to cover the following topics:  Gensim is one of the powerful libraries for natural language processing. It will support Bag of Words, TFIDF, Word2Vec, Doc2Vec, and Topic modeling. Let install the library using pip command: In this section, we will start gensim by creating a dictionary object. First, we load the text data file. you can download it from the following link.  Now, we tokenize and preprocess the data using the string function split() and simple_preprocess() function available in gensim module.  Output:  In the above code block, we have tokenized and preprocessed the hamlet text data.  After tokenization and preprocessing, we will create gensim dictionary object for the above-tokenized text.  Output: Here, gensim dictionary stores all the unique tokens. Now, we will see how to save and load the dictionary object.  Output: The Bag-of-words model(BoW ) is the simplest way of extracting features from the text. BoW converts text into the matrix of the occurrence of words within a document. This model concerns whether given words occurred or not in the document. Let’s create a bag of words using function doc2bow() for each tokenized sentence. Finally, we will have a list of tokens with their frequency. In the above code, we have generated the bag or words. In the output, you can see the index and frequency of each token. If you want to replace the index with a token then you can try the following script:  Here, you can see the list of tokens with their frequency.  Let’s generate the TF-IDF features for the given BoW corpus.  Word2vec is a two-layer neural net that processes text by “vectorizing” words. Its input is a text corpus and its output is a set of vectors: feature vectors that represent words in that corpus. While Word2vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand. There are two main methods for woed2vec: Common Bag Of Words (CBOW) and Skip Gram.  Continuous Bag of Words (CBOW) predicts the current word based on four future and four history words. Skip-gram takes the current word as input and predicts the before and after the current word. In both types of methods, the neural network language model (NNLM) is used to train the model. Skip Gram works well with a small amount of data and is found to represent rare words well. On the other hand, CBOW is faster and has better representations for more frequent words. Let’s implement gensim Word2Vec in python:  In the above code, we have built the Word2Vec model using Gensim. Here is the description for all the parameters:  In this section, we will see how Google’s pre-trained Word2Vec model can be used in Python. We are using here gensim package for an interface to word2vec. This model is trained on the vocabulary of 3 million words and phrases from 100 billion words of the Google News dataset. The vector length for each word is 300. You can download Google’s pre-trained model here. Let’s load Google’s pre-trained model and print the shape of the vector:  Output: (300,)  GloVe stands for Global Vectors for Word Representation. It is an unsupervised learning algorithm for generating vector representations for words. You can read more about Glove in this research paper. It is a new global log-bilinear regression model for the unsupervised learning of word representations. You can use the following list of models trained on the Twitter dataset: FastText is an improvement in the Word2Vec model that is proposed by Facebook in 2016. FastText spits the words into n-gram characters instead of using the individual word. It uses the Neural Network to train the model. The core advantage of this technique is that can easily represent the rare words because some of their n-grams may also appear in other trained words. Let’s see how to use FastText with Gensim in the following section. Doc2vec is used to represent documents in the form of a vector. It is based on the generalized approach of the Word2Vec method. In order to deep dive into doc2vec, First, you should understand how to generate word to vectors (word2vec). Doc2Vec is used to predict the next word from numerous sample contexts of the original paragraph. It addresses the semantics of the text.   In the above code, we have built the Doc2Vec model using Gensim. Here is the description for all the parameters: Congratulations, you have made it to the end of this tutorial! In this article, we have learned Gensim Dictionary, Bag of Words, TFIDF, Word2Vec, and Doc2Vec. We have also focused on Google’s Word2vec, Standford’s Glove, and Facebook’s FastText. We have performed all the experiments using gensim library. Of course, this is just the beginning, and there’s a lot more that we can do using Gensim in natural language processing. You can check out this article on Topic modeling. "
46,Statistics,Measures of Central Tendency ,"Central tendency stands for summary values in the data. It represents the central value for the whole dataset. It provides a single value instead of multiple values that are helpful to understand the system. For example, the average monthly sale is a good indicator of sales for any product.  Let’s discuss the following central tendency measures.  Here are some key points about central tendency measures: The harmonic mean is the reciprocal of the arithmetic mean of reciprocals. Average speed and flow of liquid are examples of harmonic mean. The geometric mean multiplies the numbers together and then takes a square root. It offers a good application where units of the indicator are different. Compound interest from year to year is an example of a geometric mean. Let’s see a very interesting relationship between arithmetic, geometric, and harmonic mean using geometry: We can easily conclude here AM>GM>HM. Let’s see a summary table for all the central tendency measure. Central tendency measures are very important metrics for exploratory data analysis. It has the capability to summarize the set of observations around a central value. In summary, we can say that mean is an average value median is the center value, and mode is the most frequent value in the set of observations. "
47,Statistics,Measures of Dispersion ,"To understand the data well, only studying measures of central tendency is not enough. One essential measure is how the data is scattered or dispersed. Measures of dispersion indicate how the data is spread or scattered from the measures of central tendency. Measures Of dispersion is also known as “Measures of Variability” because it indicates the variability of the data that how much we still do not know about the data. In this blog we will discuss about four commonly used measures of dispersion. The simplest measure of dispersion is Range; it is the difference between the highest value and lowest value in the dataset. It offers a crude insight into the spread of the data, but very susceptible to outliers. The range is helpful when you want to focus on extreme values in the dataset. The formula of Range is: Range = Highest value – lowest value Let’s understand with an example of weather report, the temperature is measured every three hours during a given day. As the table shows the temperature which is measured every three hours, the green highlighted row shows the minimum value for the temperature was 6 ⁰C at 3.00 hours and the red highlighted row shows the maximum value for the temperature was 27 ⁰C at 15.00 hours. This temperature is an important measure when the temperature was one of the deciding factors for the open-air events. The interquartile range is a measure of dispersion, as it also measures the variability of the data, IQR indicates how the data in a series is dispersed from the mean. It measures the difference between the third quartile and the first quartile of the data. It means IQR measure the spread of the middle 50% of the dataset. As the IQR goes up the data points are more spread out and if the IQR is small they assumed to be data is spread around the mean. IQR is also very helpful to determine the outlier in the datasets. To calculate IQR first we have to sort the data in ascending order. The Formula of IQR is: IQR = Third Quartile – First Quartile Let’s understand how to find the interquartile range: Suppose we have a data series 88,89,89,89,90,91,91,91,92 So, to find out the IQR first we have to sort the data on ascending order as the data is already sorted so we don’t need to sort it. Now next find the median (middle value) of the data this is identified as Q2, the middle value of the dataset is 90. 88,89,89,89,90,91,91,91,92 As the dataset is divided into two parts, now find the middle value of the first half which is identified as Q1 is 89 and second half which is identified as Q3 is 91. So, the IQR is – = 91-89= 2 Visualization of inter quartile range through box-plot: We used IQR when we are more interested in middle value and less interested in extremes.  Variance is one of the important measures of dispersion, Variance measure the variability of the data around its mean or average. In other words, variance indicates how the data is deviated or dispersed from its mean or average. High variance means there is more variability or we can say that the data deviates more from its mean whereas low variance means there is less variability. If the variance is zero that means all the values in the data are identical. Variance can never be negative. It is denoted by (sigma square). Formula for population variance: where N is the population size and the X are data points and μ is the population mean. Formula for sample variance: where n is the sample size and X are the data points and x̄ (X-bar) is the sample mean. Let’s understand variance with an example Suppose I am traveling from Indore to Bhopal by car, my car speed data is 0,30,60,50,80,100 the average speed of the car is 53.33. Now we calculate the variance of car speed data, we get the variance 1055.55(by population formula). As we see variance is too far from its average which indicates our variance is too high which means my car speed is fluctuating a lot. So as a conclusion we say that the driver driving a car roughly that means he is not a good driver because the car speed data varying a lot. Standard deviation is an important measure of dispersion and frequently used in statistics. Standard deviation is simply the square root of variance. It indicates how far away the dispersion of the dataset from its mean. It is denoted by (sigma). Simply standard deviation helps us to find the spread of the data about its mean or average. A low Standard deviation indicates that the data are less spread from their average where a high standard deviation indicates the data are more spread out from its average. The formula of standard deviation for population: where n is the sample size and X are the data points and μ is the sample mean. The formula of standard deviation for sample: where n is the sample size and X are the data points and x̄ (X-bar) is the sample mean. Let’s take the above example of car speed data, the variance is 1055.55, we calculate the standard deviation which is 32.48, so this indicates that our data is fluctuate in between 53.33 ± 32.48 (if take one standard deviation, that is 68% of the total data). In financial risk management, investors often worry about the volatility of return i.e. how much the return spreads from the average. Standard deviation helps to provide a measure of the volatility of return and is considered to be a very important measure of risk. In this tutorial, we have discussed the measures of dispersion or measures of variability. we have discussed the Range, Inter-quartile range (IQR), Variance, and Standard deviation with a real-life example. "
48,Statistics,Introduction to Factor Analysis in Python ,"In this tutorial, you’ll learn the basics of factor analysis and how to implement it in Python. Factor Analysis (FA) is an exploratory data analysis method used to search influential underlying factors or latent variables from a set of observed variables. It helps in data interpretations by reducing the number of variables. It extracts maximum common variance from all variables and puts them into a common score. Factor analysis is widely utilised in market research, advertising, psychology, finance, and operation research. Market researchers use factor analysis to identify price-sensitive customers, identify brand features that influence consumer choice, and helps in understanding channel selection criteria for the distribution channel. In this tutorial, you are going to cover the following topics: For more such tutorials, projects, and courses visit DataCamp: Factor analysis is a linear statistical model. It is used to explain the variance among the observed variable and condense a set of the observed variable into the unobserved variable called factors. Observed variables are modeled as a linear combination of factors and error terms (Source). Factor or latent variable is associated with multiple observed variables, who have common patterns of responses. Each factor explains a particular amount of variance in the observed variables. It helps in data interpretations by reducing the number of variables. Factor analysis is a method for investigating whether a number of variables of interest X1, X2,……., Xl, are linearly related to a smaller number of unobservable factors F1, F2,..……, Fk. Source: This image is recreated from an image that I found in factor analysis notes. The image gives a full view of factor analysis. Assumptions: The primary objective of factor analysis is to reduce the number of observed variables and find unobservable variables. These unobserved variables help the market researcher to conclude the survey. This conversion of the observed variables to unobserved variables can be achieved in two steps: What is a factor? A factor is a latent variable that describes the association among the number of observed variables. The maximum number of factors is equal to a number of observed variables. Every factor explains a certain variance in observed variables. The factors with the lowest amount of variance were dropped. Factors are also known as latent variables or hidden variables or unobserved variables or Hypothetical variables. What are the factor loadings? The factor loading is a matrix that shows the relationship of each variable to the underlying factor. It shows the correlation coefficient for observed variables and factors. It shows the variance explained by the observed variables. What is Eigenvalues? Eigenvalues represent variance explained each factor from the total variance. It is also known as characteristic roots. What are Communalities? Commonalities are the sum of the squared loadings for each variable. It represents the common variance. It ranges from 0–1 and value close to 1 represents more variance. What is Factor Rotation? Rotation is a tool for better interpretation of factor analysis. Rotation can be orthogonal or oblique. It re-distributed the commonalities with a clear pattern of loadings. Kaiser criterion is an analytical approach, which is based on the more significant proportion of variance explained by a factor that will be selected. The eigenvalue is a good criterion for determining the number of factors. Generally, an eigenvalue greater than 1 will be considered as the selection criteria for the feature. The graphical approach is based on the visual representation of factors’ eigenvalues also called scree plots. This scree plot helps us to determine the number of factors where the curve makes an elbow.  Let’s perform factor analysis on BFI (dataset based on personality assessment project), which were collected using a 6 point response scale: 1 Very Inaccurate, 2 Moderately Inaccurate, 3 Slightly Inaccurate 4 Slightly Accurate, 5 Moderately Accurate, and 6 Very Accurate. You can also download this dataset from the following the link: https://vincentarelbundock.github.io/Rdatasets/datasets.html Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Factorability means “can we found the factors in the dataset?”. There are two methods to check the factorability or sampling adequacy: Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test found statistically insignificant, you should not employ a factor analysis. In Bartlett’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix. Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. It determines the adequacy for each observed variable and for the complete model. KMO estimates the proportion of variance among all the observed variables. Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. The value of KMO less than 0.6 is considered inadequate. The overall KMO for our data is 0.84, which is excellent. This value indicates that you can proceed with your planned factor analysis. For choosing the number of factors, you can use the Kaiser criterion and scree plot. Both are based on eigenvalues. Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables). The scree plot method draws a straight line for each factor and its eigenvalues. Number eigenvalues greater than one considered as the number of factors. Here, you can see only for 6-factors eigenvalues are greater than one. It means we need to choose only 6 factors (or unobserved variables). Let’s perform a factor analysis for 5 factors. Total 42% cumulative Variance explained by the 5 factors. Factor analysis explores large datasets and finds interlinked associations. It reduces the observed variables into a few unobserved variables or identifies the groups of inter-related variables, which help the market researchers to compress the market situations and find the hidden relationship among consumer taste, preference, and cultural influence. Also, It helps in improving the questionnaire for future surveys. Factors make for more natural data interpretation. The results of the factor analysis are controversial. Its interpretations can be debatable because more than one interpretation can be made of the same data factors. After factor identification and naming of factors requires domain knowledge. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned what factor analysis is. The different types of factor analysis, how does factor analysis work, basic factor analysis terminology, choosing the number of factors, comparison of principal component analysis and factor analysis, implementation in Python using Python FactorAnalyzer package, and pros and cons of factor analysis. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. Originally published at https://www.datacamp.com/community/tutorials/introduction-factor-analysis Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
49,Statistics,Demystifying Mathematical Concepts for Deep Learning ,"Explore basic math concepts for data science and deep learning such as Scalar and Vector, Determinant, Singular Value Decomposition(SVD), Hadamard product, Entropy, Kullback-Leibler Divergence, and Gradient Descent Data science is an interdisciplinary field that uses mathematics and advanced statistics to make predictions. All data science algorithms directly or indirectly use mathematical concepts. A solid understanding of math will help you develop innovative data science solutions such as a recommender system. If you are good at mathematics, it will make your transition into data science easier. As a data scientist, you have to utilize the fundamental concepts of mathematics to solve problems. Apart from mathematics, you also need domain knowledge, programming skills, business skills, analytical skills, and a curious mindset. There is no way of escaping mathematics for a data scientist. You have to inculcate and teach yourself the basics of mathematics and statistics to become a data scientist. In this tutorial, you are going to explore basic math concepts for data science (or especially deep learning) such as: Image Source Dot Product of two vectors represents the projection of one vector on another vector and Cross Product of two vectors enables one vector to identify the plane on which both the vectors can lie. Image source The determinant is a scalar value that represents a factor of a matrix by which length(in 1-dimension), area(2-dimension), volume (3-dimension) can be explored. If the determinant is 2, it is twice the volume(in 3-dimension), or if the determinant is 1, it will not affect the volume(in 3-dimension). If the determinant is 0, it has no inverse because zero multiplied with anything will give you zero. Properties: If your matrix represents a stretchable toy than a determinant of this matrix shows how much you’ve stretched your toy. An eigenvector of square matrix A is a nonzero vector such that multiplication by A alters only the scale of v. Image Source The eigenvector is also known as the characteristic vector. It is a non-zero vector that only changes with the scalar factor when a linear transformation is applied to it. Eigenvectors are rotational axes of the linear transformation. These axes are fixed in direction, and eigenvalue is the scale factor by which the matrix is scaled up or down. Eigenvalues are also known as characteristic values or characteristic roots. In other words, you can say eigenvectors are fixed line or plane which limits the behavior of the linear transformation, and eigenvalues of a linear transformation is the factor of distortion. Determinant tells you the area of shapes that scaled up and down in linear transformation. That’s why the multiplication of eigenvalues is equal to the determinant. Originally published at https://www.datacamp.com/community/tutorials/demystifying-mathematics-concepts-deep-learning Sometimes you want to measure the size of a vector. Norm function helps you measure the size of a vector. It assigns a strictly positive length to a vector in a vector space except for zero vector. It includes the L^p norm. It maps vectors to non-negative values. It is equivalent to Euclidean distance for a vector and a matrix. It is equal to the largest singular value. Matrix factorization, also known as matrix decomposition. It is used to split a matrix into its constituent parts. Matrix factorization is equivalent to the factoring of numbers, such as the factoring of 10 into 2 x 5. It is used to solve linear equations. The following matrix factorization techniques are available: In the previous section, we have seen the eigendecomposition of a matrix that decomposes into eigenvectors and eigenvalues. Singular value decomposition is a type of matrix factorization method that decomposes into singular vectors and singular values. It offers various useful applications in signal processing, psychology, sociology, climate, atmospheric science, statistics, and astronomy. Pseudoinverse of a matrix is generalized of the inverse matrix. It is utilized in computing the least-square solutions. Moore-Penrose inverse is the most popular form of matrix pseudoinverse. Hadamard product or Schur product is an element-wise product of two same dimensional original matrices. It is also known as the element-wise product. It is simpler than the matrix product. Hadamard product is utilized in JPEG lossy compression algorithms. Hadamard product is commutative, associative, and distributive. It easily obtains the inverse and simplifies the computation of power matrices. Hadamard product is utilized in various fields such as code correction in satellite transmissions, information theory, cryptography, pattern recognition, neural network, maximum likelihood estimation, JPEG lossy compression, multivariate statistical analysis, and linear modeling. “The entropy of a random variable is a function which attempts to characterize the unpredictability of a random variable.” (Entropy and Mutual Information)” It is used for the construction of an automatic decision tree at each step of tree building; feature selection is done such using entropy criteria. Model selection is based on the principle of the maximum entropy, which states from the conflicting models, the one with the highest entropy is the best. “If a random variable X takes on values in a set χ={x1, x2,…, xn}, and is defined by a probability distribution P(X), then we will write the entropy of the random variable as,” (Entropy and Mutual Information) “If the log in the above equation is taken to be to the base 2, then the entropy is expressed in bits. If the log is taken to be the natural log, then the entropy is expressed in nats. More commonly, entropy is expressed in bits.” (Entropy and Mutual Information) Kullback–Leibler divergence is the relative entropy of two probability distributions. It measures the distance(similarity or dissimilarity) of one distribution from another reference probability distribution. 0 Value of Kullback–Leibler divergence indicates that both distributions are identical. It can be expressed as, It does sound like a distance measure, but it is not. This is because it is asymmetric in nature, which means the metric isn’t commutative. In general, you can say that D(p, q) ≠D(q, p). KL divergence is frequently used in the unsupervised machine learning technique “Variational Autoencoders”. Gradient descent is one of the most well-known algorithms used to optimize coefficients and bias for linear regression, logistic regression, and neural networks. It is an iterative process that finds the minimum of any given function. Image Source Image Source Three types of the gradient descent algorithm: full batch, stochastic, and mini-batch gradient descent. Full batch gradient descent uses the whole dataset for computing gradient while stochastic gradient descent uses a sample of the dataset for computing gradient. Mini-batch gradient descent is a combination of both stochastic and batch gradient descent. The training set is split into various small groups called batches. These small batches compute the loss one by one and average the final loss result. Congratulations, you have made it to the end of this tutorial! You learned basic mathematical concepts for deep learning such as scalar, vector, matrix, tensor, determinant eigenvalues, eigenvectors, NORM function, singular value decomposition(SVD), Moore-Penrose Pseudoinverse, Hadamard product, Entropy Kullback-Leibler Divergence, and Gradient Descent. Along the road, you have also practiced these concepts in python using NumPy and SciPy. For more such tutorial and courses visit DataCamp: Originally published at https://www.datacamp.com/community/tutorials/demystifying-mathematics-concepts-deep-learning"
50,Interview,Data Science Interview Questions Part-8(Deep Learning) ,"Top-25 frequently asked data science interview questions and answers on Deep Learning for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this 8th article, we are focusing on Deep Learning interview questions. Let’s see the interview questions and answers in detail. Deep Learning is a subdomain of Machine Learning. In deep learning, a large number of layers in the architecture. These successive layers learn more complex patterns in the data. Deep Learning offers various applications in text, voice, image, and video data.  It is a first-order iterative optimization technique for finding the minimum of a function. It is an efficient optimization technique to find a local or global minimum. In gradient descent, gradient and step are taken at each point. It takes the current value of parameters and updates it with the help of gradient and step width. the gradient is recomputed again and steps decremented in each iteration. This process continues until the convergence achieved.  Types of Gradient Descent The main objective of the neural network is to find the optimal set of weights and biases by minimizing the cost function. Cost function or loss function is a measure used to measure the performance of the neural network on test data set. It measures the ability to estimate the relationship between X and y. An example of cost functions is the mean square error.  Activation functions are mathematical functions that transform the output of a neural network on a certain scale. It means it normalizes the output between range 0 and 1 or -1 and 1. Activation functions in neural networks introduce non-linearity. It helps neural networks to handle non-linear relationships. Sigmoid function bounded between 0 and 1. It is differentiable, non-linear, and produces non-binary activations. But the problem with Sigmoid is the vanishing gradients.  ReLu(Rectified Linear Unit) is like a linearity switch. If you don’t need it, you “switch” it off. If you need it, you “switch” it on. ReLu avoids the problem of vanishing gradient. ReLu also provides the benefit of sparsity and sigmoids result in dense representations. Sparse representations are more useful than dense representations. The main problem with ReLU is, it is not differentiable at 0 and may result in exploding gradients. To resolve this problem Leaky ReLu was introduced that is differentiable at 0. It provides small negative values when input is less than 0. When ReLu provides output zero for any input(large negative biases). This problem will occur due to a high learning rate and large negative bias. Leaky ReLU is a commonly used method to overcome a dying ReLU problem. It adds a small negative slope to prevent the dying ReLU problem. Tanh function is called a shifted version of the sigmoid function. The output of Tanh centers around 0 and sigmoid’s around 0.5. Tanh Convergence is usually faster if the average of each input variable over the training set is close to zero.  When you struggle to quickly find the local or global minimum, in such case Tanh can be helpful in faster convergence. The derivatives of Tanh are larger than Sigmoid that causes faster optimization of the cost function. Tanh and Sigmoid both suffered from vanishing gradient problems. Model parameters are internal and can be estimated from data. Model hyperparameters are external to the model and can not be estimated from data. Backpropagation or backward pass traverses in the reverse direction. It computes the gradient(or delta rule) of parameters(weights and biases) in order to map the output layer to the input layer. The main objective of backpropagation is to minimize the error. This process will repeat until the error is minimized and final parameters will be used for producing the output. Forward propagation or forward pass computes the intermediate values in order to map the input and output layer. Dropout is a technique for normalization. It drops out or deactivates some neurons from the neural network to remove the problem of overfitting. In other words, it introduces the noise in the neural network so that model is capable to generalize the model.  Normalization is used to reduce the algorithm time that spends on the oscillation on different values. It brings all the features on the same input scale. Batch Normalization is also normalizing the values but at hidden states on small batches of data. The research has shown that removing Dropout with Batch Normalization improves the learning rate without loss in generalization.  Recurrent neural networks(RNN) work on sequential input data with a good amount of accuracy. It uses a backpropagation network for back-propagating the error and gradient. In RNN, backpropagation is known as backpropagation through time(BPTT). RNN follows the chain rule in its backpropagation. When one of the gradients approaches zero then all the gradient will move towards zero. This small value is not sufficient for training the model. Here, a small gradient means that weights and biases of the neural network will not be updated effectively. Also, at hidden layers activation functions such as sigmoid function and Tanh causes small derivatives that decrease the gradient.  The solution to Vanishing Gradient Descent Exploding gradient is just an opposite situation of vanishing gradient. A too-large value of RNN causes powerful training. We can overcome this problem by using Truncated Backpropagation, penalties, Gradient Clipping. LSTM is a special type of RNN. It also uses a chain-like structure but it has the capability to learn and remember long-term sequences. LSTM handles the issue of vanishing gradient. It keeps gradient step enough and therefore the short training and the high accuracy. It uses gated cells to write, read, erase the value. It has three gates: input, forget, and output gate. BiLSTM learns sequential long terms in both directions. It captures the information from both the previous and next states. Finally, it merges the results of two states and produces output. It memorizes information about a sentence from both directions. LSTM has three gates: input, forget, and output gate. The input gate is used to add the information to the network, forget used to discard the information, and the output gate decides which information pass to the hidden and output layer. GRU is also a type of RNN. It is slightly different from LSTM. The main difference between LSTM and GRU Gates is the number of gates.  CNN is Feedforward neural network. CNN filters the raw image detail patterns and classifies them using a traditional neural network. Convolution focuses on small patches in the image and represents a weighted sum of image pixel values. It offers applications in Image recognition and object detection. It works in the following steps:  The convolution layer is inspired by the visual cortex. It converts the image into layers, transforms into small images, and extracts features from images. It will sum up the results into a single output pixel. It captures the relationship between pixels and detects edge, blur, and sharpen features. Sometimes filter unable to fit the input image perfectly. We have two strategies for padding: Zero padding and valid padding. Zero paddings add zero so that the image filter fits the image. Valid padding drops the part of the image. (Drop the part of the image) Pooling is used to reduce the spatial size and selects the important pixel values as features. It is also known as Downsampling. It also makes faster computation by reducing its dimension. Pooling summarizes the sub-region and captures rotational and positional invariant features.  Autoencoders are unsupervised deep learning techniques that reduce the dimension of data to encode. Autoencoders encoded the data on one side and decoded it on another side. After encoding, it transforms data into a reduced representation called code or embedding(also known as latent-space representation). This embedding then transformed into the output. Autoencoders can do dimensionality reduction and improve the performance of the algorithm.   Boltzmann machines are stochastic(non-deterministic models) and generative neural networks. It has the capability to discover the interesting features that represent complex patterns in the data. Boltzmann Machine uses many layers for feature detectors that makes it slower network. Restricted Boltzmann Machines (RBMs) have a single layer of feature detectors that makes it faster compared to Boltzmann Machine.  RBM is a neural network model are also known as Energy Based Models. RBM offers various applications in recomender system, classification, regression, topic modeling and dimensionality reduction. GAN (Generative Adversarial Network) is unsupervised deep learning that trains two networks at the same time. It has two components: Generator and Discriminator. The generator generates the images close to the real image and the discriminator determines the difference between fake and real images. GAN is able to produce new content. In this article, we have focused on Deep Learning interview questions and answers. In the next article, we will focus on the interview questions related to Big Data. Data Science Interview Questions Part-9(BigData) "
51,Interview,Data Science Interview Questions Part-7(Statistics) ,"Top-25 frequently asked data science interview questions and answers on Statistics for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Let’s see the interview questions. Descriptive statistics describe the input data and provide initial findings for data. It will give the mean, median, standard deviation, etc. For example, the manager of a supermarket wants to see the waiting time for billing.  Inferential statistics allow us to make inferences from a smaller sample of data from the population. It is used when we have limited time, cost, and other constraints. For example, the LED bulb manufacturing firm wants to check the average working hours of LED bulbs. In this case, the firm can’t test all the bulbs they need to take a small sample of bulbs and infer the results from the sample for all the bulbs. Standard deviation is the average distance from the mean or variation from the mean. A small value of standard deviation shows closeness in data points while a larger value shows the higher scatteredness of data. Standard variance is the square of standard deviation. It also describes the data variation from the mean. For comparison purposes, the standard deviation will be preferred because its unit is the same as the mean.  EDA(Exploratory Data Analysis) is the most crucial step of the data analysis process. It provides an initial understanding of data. It discovers insights and answers to business queries and questions. It also assesses the quality of data and finds missing values and outliers.  The arithmetic mean is the average value of given data points. It can be expressed as the ratio of the sum of all the observations to the total number of observations. For example, the average of runs scored by a cricket player. The harmonic mean is the reciprocal of the arithmetic mean of reciprocals. Average speed and flow of liquid are examples of harmonic mean. The geometric mean multiplies the numbers together and then takes a square root. It offers a good application where units of the indicator are different. Compound interest from year to year is an example of a geometric mean. Boxplot or Box-whisker plot is used to understand the distribution of a variable. This plot also shows the quartiles, minimum, maximum, and outliers. It uses a rectangular box or whiskers to show the quartile.  Minimum (or Lower Limit) = Q1–1.5 IQR Maximum (or Upper Limit) = Q3 + 1.5 IQR Any value that will be more than the upper limit or lesser than the lower limit will be considered as the outliers. Mean is the average value of a set of observations. It is used when data is normally distributed. Median is the middle value of a given set of observations. It is used when data is the skewed or long tail. For example, Income variable, income is highly skewed in the real world. If you use mean then the value of the mean will be dominated by the outliers.  Skewness describes the distribution symmetry. It shows the deviation from the normal distribution. Skewness can be left or right skewness. Left skewed or negative skewed has a tail towards the left side and right-skewed or positive-skewed has a tail towards the right side. Kurtosis measures the thickness of the tail. The high value of kurtosis means heavy-tailed which indicates more outliers. The low value of kurtosis means less tailed which means less number of outliers in the observations. Outliers are abnormal observations that deviate from the norm. Outliers do not fit in the normal behavior of the data. We can detect outliers using the following methods: Covariance is used to quantify the relationship between a pair of variables. IT shows if we change one variable how it will impact the other. Covariance ranges from -infinity to + infinity.  Correlation also quantifies the relationship between two variables but it is normalized and ranges between -1 to +1. It also ensures that a certain degree between two variables. In other words, we can say covariance is boundary-less but the correlation has a boundary. It makes it easier when we compare the two results.  The Pareto principle is also known as the 80/20 rule. It says that 80% of the results come from 20% of the effort. Eg. 80% of sales come from 20% of customers. In inferential statistics, the Population is the entire set of respondents and the sample is a subset from that entire set of respondents. Whenever we do any survey we select a few respondents to collect the data. Sample distribution is the distribution of all the values of the sample and sampling distribution displays all the values of possible samples from the population. Sampling is of types: Probability and Non-probability Sampling. Probability sampling selects the samples randomly(with equal chances) and Non-probability samples select the samples non-randomly(with unequal chances). Sampling can be of the following types: Normal distribution fits into all kinds of real-life scenarios such as heights, exam scores, and blood pressure. The standard normal distribution is a specific distribution with a mean 0 and a standard deviation of 1. It is also known as the Gaussian distribution and the bell curve. Standardizing normal distribution makes it easier to compare with other metrics. It all boils down to the central limit theorem. The standard normal distribution uses Z values that can be easily compared and interpreted by a trained statistician. Binomial distribution has two possible outcomes Success or Failure (Yes or No). It is the probability of both outcomes over multiple experiments. Bernoulli distribution is the binomial distribution that has a value of n=1. It has the following conditions: Here, p=probability of a success, q=probability of a failure, n-stands for the number of times the experiment runs. Here are the following properties of the normal distribution: It follows the empirical rule that finds what percentage of your data falls within a certain number of standard deviations.  The main objective of hypothesis testing is to determine the statistical significance in favor of a certain assumption. For example, We want to find statistical evidence that from a selected random sample of customers 15% will purchase the product. Similarly, you want to check the effectiveness of the Covid vaccine on the selected sample.  The central limit theorem is a sampling distribution approach that states that with an increase in sample size distribution approaches to a normal. It means the mean of the sample gets closer to the population mean and the standard deviation of the sample will reduce.  The hypothesis is the assumption that formulates a conclusion about the population. The null hypothesis is the exact opposite of what a researcher expects. An Alternative hypothesis is a statement used to contradict the null hypothesis. A p-value is used to test a hypothesis testing. It helps us to accept or reject the null hypothesis. P-value provides evidence against the null hypothesis. The larger the p-value causes week evidence against the null hypothesis. It means a large value indicates acceptance of the null hypothesis or fail to reject the null hypothesis.  One-tailed hypothesis in unidirectional or one-sided tests because we can test effects in only one direction while Two-tailed hypothesis tests are also known as nondirectional and two-sided tests because it can test effects in both directions. The one-tailed test has less than(<) and greater than(>) conditions while the two-tailed test has an equal(=) sign.  Degree of freedom or DF is the number of independent variables required for performing an analysis. Variables that are dependent are not considered to be free. For instance, if we are calculating a sample variance using the sample mean, we lose degree of freedom because we’ve forced the sample mean to be a particular value.  It is mostly used in t-distribution and not with the z-distribution. If DF will increase then t-distribution will be closer to normal distribution. A statistical significance test provides an outcome yes/no, reject/fail to reject. significance level (or alpha )is the probability of rejecting the Null Hypothesis and expressed as a percentage. Significance level = 1 − Confidence level The confidence level tells us how sure we can be and is expressed as a percentage. or A confidence interval is an interval within which you are confident the true value lies to some degree of accuracy. For example, a 95% confidence level indicates 95% certainty and a 5% significance level indicates the risk of concluding that a difference exists when there is no actual difference. The bell curve or bell-shaped term is used for normal distribution which is also known as Gaussian distribution. The normal distribution is a type of continuous probability distribution. It is symmetric from the mean. In this article, we have focused on Statistics interview questions. In the next article, we will focus on the interview questions related to Deep Learning. Data Science Interview Questions Part-8(Deep Learning)"
52,Interview,Data Science Interview Questions Part-6 (NLP & Text Mining) ,"Top-25 frequently asked data science interview questions and answers on NLP and Text Mining for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this sixth article, we are focusing on NLP and Text Mining questions. Let’s see the interview questions. Natural language is a human language such as Hindi, English, and German. Designing computers that can understand natural languages is a challenging problem.  Computer language is a set of instructions that used to produce the desired output such as C, C++, Python, Julia, and Scala. Natural language processing is one of the components of text mining. NLP helps identified sentiments, finding entities in the sentence, and category of blog/article. Text Mining is about exploring large textual data and find patterns. It includes finding frequent words, the length of the sentence, and the presence/absence of specific words. NLP is a combination of NLU(Natural Language Understanding) and NLG (Natural Langauge Generation). NLU is used to understand the meaning of a given input text. It understands based on the grammar and context of the text. NLU focuses on sentiment, semantics, context, and intent. For example, the questions “what’s the weather like outside?” and “how’s the weather?” are both asking the same thing. NLG generates text based on structured data. It takes data from a search result and returns it into understandable language. The business organization wants to understand the opinion of customers and the public. For example, what went wrong with their latest products? what users and the general public think about the latest feature? Quantifying the user’s content, idea, belief, and opinion are known as sentiment analysis. It is not only limited to marketing, but it can also be utilized in politics, research, and security. The sentiment is more than words, it is a combination of words, tone, and writing style.  Tokenization is the process of splitting text into small pieces, called tokens such as words, or sentences. It ignoring characters like punctuation marks (,. “ ‘) and spaces. Word tokenization is breaking up the text into individual words and Sentence tokenization is breaking up the text into individual sentences. The Bag-of-words model(BoW ) is the simplest way of extracting features from the text. BoW converts text into the matrix of the occurrence/frequency of words within a document. This model concerns whether given words occurred or not in the document. It can be a combination of two or more words, which is called the bigram or trigram model and the general approach is called the n-gram model. n-gram creates a matrix is known as the Document-Term Matrix(DTM) or Term-Document Matrix(TDM). TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. In Term Frequency(TF), you just count the number of words that occurred in each document. IDF(Inverse Document Frequency) measures the amount of information a given word provides across the document. IDF is the logarithmically scaled inverse ratio of the number of documents that contain the word and the total number of documents. TF-IDF is the multiplication of TF and IDF. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. lemmatization looks at words and their roots (called lemma) as described in the dictionary. It is more precise than stemming. Stemming reduces word-forms to (pseudo)stems, whereas lemmatization reduces the word-forms to linguistically valid lemmas. The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up. A word’s part of speech defines its function within a sentence. A noun, for example, identifies an object. An adjective describes an object. A verb describes the action. Identifying and tagging each word’s part of speech in the context of a sentence is called Part-of-Speech Tagging, or POS Tagging. Named Entity recognition, also called entity detection, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within an input string of text. This is really helpful for quickly extracting information from text since you can quickly pick out important topics or identify key sections of text. Topic modeling is a text mining technique that provides methods to identify co-occurring keywords to summarize large collections of textual information. It helps in discovering hidden topics in the document, annotate the documents with these topics, and organize a large amount of unstructured data. There is a possibility that a single document can associate with multiple themes. for example, group words such as ‘patient’, ‘doctor’, ‘disease’, ‘cancer’, ad ‘health’ will represent the topic ‘healthcare’. Topic Modelling is a different game compared to rule-based text searching that uses regular expressions. LSA (Latent Semantic Analysis) also known as LSI (Latent Semantic Index) LSA uses a bag of word(BoW) model, which results in the term-document matrix (occurrence of terms in a document). rows represent terms and columns represent documents.LSA learns latent topics by performing a matrix decomposition on the document-term matrix using Singular value decomposition. LSA is typically used as a dimension reduction or noise-reducing technique. LSA uses SVD(Singular Value Decomposition) for factorizing the Term document matrix(TDM) into 3 matrices singular, diagonal, and singular matrix.  Latent Dirichlet Allocation(LDA) is an unsupervised algorithm for topic modeling. It helps users to detect the relationship between words and discover groups in those words. The core idea behind the topic modeling is that each document is represented by the distribution of topics and the topic is represented by the distribution of words. In other words, we can say first we connect words to the topic, and then topics will be connected by each document. It is a kind of “generative probabilistic model”. LDA consists of two tables or matrices. The first table describes the probability of selecting a particular part when sampling a particular topic. The second table describes the chance of selecting a particular topic when sampling a particular document or composite.  Masked language modeling is an example of autoencoding language modeling. It is a kind of fill-in-the-blank task. It uses context words surrounded by mask tokens and predicts what word should be in the place of the mask. Masked language modeling is useful when trying to learn deep representations. BERT (Bidirectional Encoder Representations from Transformers) pre-trained model used the Masked Language model for model training.  In natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts [Wikipedia]. Perplexity is a way to express a degree of confusion a model has in predicting. Perplexity is the exponentiation of the entropy. Low perplexity is good and high perplexity is bad since the perplexity is the exponentiation of the entropy.  Text Preprocessing involves the cleaning, normalizing, and noise removal from the given input text. text preprocessing involves the following steps: Word embedding converts text data (e.g words, sentences, and paragraphs) into some kind of vector representation. A word having the same meaning has a similar representation. Word embedding uses an embedding layer to learn vector representation of the given text using Backpropagation. Some examples of word embedding are Word2Vec, Sent2Vec, and Doc2Vec. There are two models for learning word2vec models: Word embeddings use very shallow Language Models and do not consider the context of the word into account. LSTM stands for Long Short term memory. It is an extension in RNN(Recurrent Neural Networks). It mitigates the problem of vanishing gradient descent problem of RNN. In RNN, the neural network weights become very smaller or close to zero as we move backward in the network. Due to this problem neurons in the earlier layers learn very slow. This is also the reason that we don’t use the sigmoid and Tanh activation function. Nowadays, mostly we are using ReLu(Rectified Linear Unit) in hidden layers and sigmoid at the output layer.  LSTM uses gates to overcome this problem. It uses three gates: Input, Output, and Forget gate. Here, the gates control whether to store or delete (remember or forget) the information.  In-Text Classification Problem, we have a set of texts and their respective labels. but we directly can’t use text for our model. you need to convert this text into some numbers or vectors of numbers. There are various ways to convert your text into some vector: NLP offers the following real-life applications: Parsing helps us to understand the structure of text data. It has capability to analyze any sentence using the parse tree and verify the grammar of a sentence. SpaCy library implemented the dependency parsing.  We can find a similarity between two texts using Cosine and Jaccard similarity. Regular Expressions or Regex are searching patterns that used to search in textual data. It is helpful in extracting email addresses, hashtags, and phone numbers. BiLSTM is a Bidirectional LSTM that processed signals in both forward and backward directions. It is a sequential processing model that consists of two LSTM. This bidirectional nature helps us to increase the more contextual information for the algorithm. It shows quite good results when it understands the context better. Syntactic analysis is analyzing the sentence and understand the grammar rules and order of sentence. It includes the following tasks parsing, word segmentation, morphological segmentation, stemming, and lemmatization. Semantic analysis analyses the meaning and interpretation of a text. It includes the following tasks such as Named entity recognition, word sense ambiguation, and Natural language generation.  In this article, we have focused on NLP and Text Analytics interview questions. In the next article, we will focus on the interview questions related to Basic Statistics. Data Science Interview Questions Part-7(Statistics)"
53,Interview,Data Scientist explores new problems and improves products/solutions every day. – Avinash Navlani ,"Avinash NavlaniSr. Data ScientistLinkedin: https://www.linkedin.com/in/avinash-navlani/ Learning and sharing new things,  Exploring new problems and improving products/solutions every day. Predicting satisfaction and frustration level of a candidate in an online exam using mouse movement data. I don’t know because things are changing so rapidly you can’t say about 10 years.  but I think I will start my own consulting services in Data Science and AI and maybe Professorship at any institute. Or I am also interested in visiting faculty roles. Procrastination, laziness, need more focus Cricket, Write blogs on data science; sometimes lectures in university Academics because you are dealing with young blood, they have a rebellious attitude and dare to do things that are a must thing to do anything. Coding standards, using Github, explaining things to whom, who have no experience in  data science. Consistency, honesty, daily reading of maths, stats, programming, cloud and big data.  Source: http://educlasses.co.in/interview-avinash-navlani-data-scientist-anayltics-consultant-indore.html "
54,Interview,Data Science Interview Questions Part-5 (Data Preprocessing) ,"Top-15 frequently asked data science interview questions and answers on Data preprocessing for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this fifth article, we are focusing on Data Preprocessing questions. Let’s see the interview questions. Features are the core characteristics of any prediction that impact the results. Feature engineering is the process of creating a new feature, transforming a feature, and encoding a feature. Sometimes we also use the domain knowledge to generate new features. It prepares the data that easily input to the model and improves model performance.  In feature scaling, we change the scale of features to convert it into the same range such as (-1,1) or (0,1). This process is also known as data normalization. There are various methods for scaling or normalizing data such as min-max normalization, z-score normalization(standard scaler), and Robust scaler.  Min-max normalization performs a linear transformation on the original data and converts it into a given minimum and maximum range. z-score normalization (or standard scaler) normalizes the data based on the mean and standard deviation. In the data cleaning process, we can see there are lots of values that are missing or not filled or collected during the survey. WE can handle such missing values using the following methods: Outliers are abnormal observations that deviate from the norm. Outliers do not fit in the normal behavior of the data. We can detect outliers using the following methods: We can treat outlier by removing from the data. After detecting outliers we can filter the outliers using Z-score, Percentile, or 1.5 times of IQR.  Feature split is an approach to generate a few other features from the existing one to improve the model performance. for example, splitting name into first and last name.  We can handle skewed data using transformations such as square, log, square, square root, reciprocal (1/x), and Box-Cox Transform. Data transformation consolidated or aggregate your data columns. It may impact your machine learning model performance. There are the following strategies to transform data: We can select the important features using random forest, or remove redundant features using recursive feature elimination. Let’s all the categories of such methods. You can get lots of other important features from the date such as day of the week, day of the month, day of the quarter, and day of the year. Also, you can extract the date, month, and year. These all features can impact your prediction for example sales can be impacted by month or day of the week.  Multicollinearity is a high correlation among two or more predictor variables in a multiple regression model. It is high intercorrelations or inter-association among the independent variables. It is caused by the inaccurate use of dummy variables and the repetition of the same kind of variable. Multicollinearity impacts change in the signs as well as in the magnitudes of the regression coefficients. We can detect multicollinearity using the Correlation coefficient, Variance inflation factor (VIF), and Eigenvalues. The easiest way to compute is the correlation coefficient. Let’s understand this with an example:  you have a dataset with the following columns: suppose body surface area or weight have high correlation: Which variable needs to remove to overcome multicollinearity. In this case, you can see the weight is easy to measure compared to body surface area(BSA). Removing a variable that has a high correlation with others depends upon domain knowledge.  heteroscedasticity is a situation where the variability of a variable is unequal across the range of values of a second variable that predicts it. We can detect heteroscedasticity using graphs or statistical tests such as Breush-Pagan test and NCV test. You can remove heteroscedasticity Box-Cox transformations and log transformations. Box-Cox transformation is a kind of power transformation that transforms data into a normal distribution.   In Regression analysis, we need to convert all the categorical columns into binary variables. Such variables are known as dummy variables. The dummy variable is also known as an indicator variable, design variable, Boolean indicator, categorical variable, binary variable, or qualitative variable. It takes only 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome. K categories will takes K-1 dummy variables. For example, you can see the eye color and gender columns converted into one-hot encoded binary values. Label encoding is a kind of integer encoding. Here, each unique category value is replaced with an integer value so that the machine can understand.  Ordinal encodings is a label encoding with an order in the encoded values.  One hot encoding is used to encode the categorical column. It replaces a categorical column with its labels and fills values either 0 or 1. For example, you can see the “color” column, there are 3 categories such as red, yellow, and green. 3 categories labeled with binary values.  In this article, we have focused on the data preprocessing interview questions. In the next article, we will focus on the interview questions related to NLP and Text Analytics. Data Science Interview Questions Part-6 (NLP and Text Analytics)"
55,Interview,Data Science Interview Questions Part-4 (Unsupervised Learning) ,"Top-20 frequently asked data science interview questions and answers on Unsupervised Learning for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this fourth article, we are focusing on unsupervised learning questions.  Let’s see the interview questions. Clustering is unsupervised learning because it does not have a target variable or class label. Clustering divides s given data observations into several groups (clusters) or a bunch of observations based on certain similarities. For example, segmenting customers, grouping super-market products such as cheese, meat products, appliances, etc. Dimensionality reduction is the process of reducing the number of attributes from large dimensional data. There are lots of methods for reducing the dimension of the data: Principal Components Analysis(PCA), t-SNE, Wavelet Transformation, Factor Analysis, Linear Discriminant Analysis, and Attribute Subset Selection. Kmeans algorithm is an iterative algorithm that partitions the dataset into a pre-defined number of groups or clusters where each observation belongs to only one group.  K-means algorithm works in the following steps:  Elbow Criteria: This method is used to choose the optimal number of clusters (groups) of objects. It says that we should choose a number of clusters so that adding another cluster does not add sufficient information to continue the process. The percentage of variance explained is the ratio of the between-group variance to the total variance. It selects the point where marginal gain will drop. You can also create an elbow method graph between the within-cluster sum of squares(WCSS) and the number of clusters K. Here, the within-cluster sum of squares(WCSS) is a cost function that decreases with an increase in the number of clusters. The Elbow plot looks like an arm, then the elbow on the arm is an optimal number of k. There are the following disadvantages:  The cluster can be evaluated using two types of measures intrinsic and extrinsic evaluation parameters. Intrinsic does not consider the external class labels while extrinsic considers the external class labels. Intrinsic cluster evaluation measures are the Davie-Bouldin Index and Silhouette coefficient. Extrinsic evaluation measures are Jaccard and Rand Index.  There are some clustering algorithms that can generate random or arbitrary shape clusters such as Density-based methods such as DBSCAN, OPTICS, and DENCLUE. Spectral clsutering can also generate arbitrary or random shape clusters.  Euclidean measures the ‘as-the-crow-flies’ distance and Manhattan distance is also known as a city block. It measures the distance in blocks between any two points in a city. (or city block).  It is based on standard linear algebra. Spectral Clustering uses the connectivity approach to clustering. It easy to implement, faster especially for the sparse datasets, and can generate non-convex clusters. Spectral clustering kind of graph partitioning algorithm. The spectral algorithm works in the following steps. t-SNE stands for t-Distributed Stochastic Neighbor Embedding which considers the nearest neighbors for reducing the data. t-SNE is a nonlinear dimensionality reduction technique. With a large dataset, it will not produce better results. t-SNE has quadratic time and space complexity. The t-SNE algorithm computes the similarity between pairs of observations in the high dimensional space and low dimensional space. And then it optimizes both similarity measures. In simple words we can say, it maps the high-dimensional data into a lower-dimensional space. After transformation input features can’t be inferred from the reduced dimensions. It can be used in recognizing feature expressions, tumor detection, compression, information security, and bioinformatics.  PCA is the process of reducing the dimension of input data into a lower dimension while keeping the essence of all original variables. It used is used to speed up the model generation process and helps in visualizing the large dimensional data. There are three methods for deciding the number of components: Eigenvectors are rotational axes of the linear transformation. These axes are fixed in direction, and eigenvalue is the scale factor by which the matrix is scaled up or down. Eigenvalues are also known as characteristic values or characteristic roots and eigenvectors are also known as the characteristic vector. SVM works better with lower-dimensional data compared to large dimensional data. When the number of features is greater than the number of observations, then performing dimensionality reduction will generally improve the SVM. t-SNE in comparison to PCA:  Benefits:  Limitations:  The main idea is to create clusters and add objects as long as the density in its neighborhood exceeds some threshold. The density of any object measured by the number of objects closed to that. It connects the main object with its neighborhoods to form dense regions as clusters. You can also define density as the size of the neighborhood €. DBSCAN also uses another user-specified parameter, MinPts, that specifies the density threshold of dense regions. Hierarchical method partition data into groups at different levels such as in a hierarchy. Observations are group together on the basis of their mutual distance. Hierarchical clustering is of two types: Agglomerative and Divisive. Agglomerative methods start with individual objects like clusters, which are iteratively merged to form larger clusters. It starts with leaves or individual records and merges two clusters that are closest to each other according to some similarity measure and form one cluster. It is also known as AGNES (AGglomerative NESting). Divisive methods start with one cluster, which they iteratively split into smaller clusters. It divides the root cluster into several smaller sub-clusters, and recursively partitions those clusters into smaller ones. It is also known as DIANA (DIvisive ANAlysis). In this article, we have focused on unsupervised learning interview questions. In the next article, we will focus on the interview questions related to data preprocessing. Data Science Interview Questions Part-5 (Data Preprocessing) "
56,Interview,Data Science Interview Questions Part-3 (Classification) ,"Top-20 frequently asked data science interview questions and answers on c classification for fresher and experienced Data Scientist, Data analyst, statistician, and machine learning engineer job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this third article, we are focusing on basic data science questions related to classification techniques.  Let’s see the interview questions. Precision is the percentage of the correct positive predictions from total predicted positives. In other words, we can say, the percentage of retrieved items is correct. It is a measure of exactness. How much junk there is in your predicted positives.  The recall is the percentage of the correct positive predictions from total actual. . In other words, we can say, the percentage of the correct item received. It is a measure of completeness. How much of the true positives you found. When it is actually yes, how often is it predict yes? ROC curve is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes or not. Support Vector Machines algorithm used in both types of classification and regression problems. SVM creates a hyperplane in multidimensional space to distinguish the different classes. SVM generates optimal hyperplane repeatedly and minimizes an error. The main concept of SVM is to discover a maximum marginal hyperplane(MMH) that suitably distinguishes the dataset into classes. Logistic Regression is one of the most simple, easy to implement, widely used classification techniques binary classification. It is used in various applications such as spam detection, churn prediction, and diabetes prediction. It uses the log of odds as the dependent variable and predicts the probability of occurrence of a binary event using a sigmoid function. Bias is the error that we captured. How far the predicted value from the actual value. Bias tends to decrease as model complexity increases. As complexity increases the chances of overfitting increase i.e. variance increases and bias decreases. Variance is the variability of model prediction for a given data point or a value that tells us the spread of our data. A model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before.  ROC curve is derived from signal detection theory. A ROC curve for a given model represents the trade-off between the true positive rate (TPR) and the false positive rate(FPR). TPR or Recall is the ratio of positive tuples that are correctly labeled and FPR or (1-Specificity) is the ratio of negative tuples that mislabelled as positive. The precision-recall curve shows the tradeoff between precision and recall. ROC curves recommended for balanced class problems while Precision recall curves recommended for the imbalance class problems. Underfitted models have a high bias, less complex, and less variance.It can be overcome by increasing complexity and adding more parameters to the model. The Overfitted models have less bias, highly complex, and high variance. It can be overcome by reducing complexity and introduce regularization.  In class imbalance problems, the main class of interest is rare. A data set distribution reflects a significant majority of the negative class and a minority positive class. For example, fraud detection applications, the class of interest (or positive class) is “fraud,” which occurs much less frequently than the negative “non-fraudulent” class. Another example, in medical data, there may be a rare class, such as “cancer.” Bagging is a Bootstrap Aggregation. It creates multiple models in a parallel manner. Bagging algorithms reduce the variance, suitable for high volume and low bias models. Examples of bagging are random forest and Extra tree algorithm.  Boosting creates models in a sequential manner. Boosting algorithms reduce the bias, suitable for low variance and high bias models. Example of boosting are XGBoost AdaBoost. Ada-boost or Adaptive Boosting is one of the ensembles boosting classifiers. AdaBoost is an iterative ensemble method that combined multiple poorly performing classifiers so that you will get high accuracy strong classifier. The basic concept behind Adaboost is to set the weights of classifiers and training data samples in each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as a base classifier if it accepts weights on the training set.  XGBoost stands for eXtreme Gradient Boosting. It trains models in isolation from one another and trained in a sequential manner. Here, each trained new model corrects the errors made by the previous ones using Gradient Descent. It is more flexible and faster. It is more prone to overfitting. It penalizes the model using regularization. The discriminative models are based on the decision boundary. Such type of models uses conditional probability distribution. Examples, Log reg, SVM, NN, CRF. Discriminative models consume less time compare to generative models.  The generative models are based on the actual distribution of each class. Such type of model uses the joint probability. Examples, NB, Bayesian Network, HMM, clustering.  Anomaly detection is the process of discovering an unusual pattern in the event of a dataset. The term anomaly can also be referred to as an outlier. Outliers differ from normal events. Anomaly detection can be a supervised or unsupervised problem bu tin most of the situation it is like an unsupervised problem. For example, One-class SVM, Isolation forest, and Local Outlier Factor(LOF). False positives are the negative tuples that were incorrectly labeled as positive but it is actually negative. False negatives are the positive tuples were incorrectly labeled as negative but actually, it is positive.  In the case of cancer prediction, false negatives are more important than false positives because it is worse saying someone does not have cancer but actually he has.  Yes, a random forest is a better choice compared to a support vector machine.  Cross-validation or K-fold cross-validation is used to assess the model’s robustness and effectiveness. It is a kind of resampling technique to assess model performance. It also helps us to check whether our model is Under-fitting or Over-fitting. k-fold cross-validation splits data into k parts or batches and in each iteration, it considers 1 batch for test and remaining for training purpose. This is how we will get accuracy on each batch and finally, we can check the variation or average accuracy. If the model is very similar to all the batches, then it is likely that it is a fairly accurate model. Yes, a random forest is better than a decision tree because the random forest is an ensemble bagging method it combines multiple decision trees and creates one combined classifier which is more robust and accurate.  SVM uses a kernel function to distinguish the classes. kernel converts the lower-dimensional space into the required higher dimensional space this method is known as the kernel trick. It is used to separate linear as well as nonlinear separation problems. we can try linear, polynomial, and RBF kernel.  Type 1 error is classified as a false positive. Here, incorrectly labeled as positive but it is actually negative. For example, False fire alarm. The alarm rings but there is no fire.Type 2 error is classified as a false negative. Here, positive tuples were incorrectly labeled as negative but actually, it is positive. A confusion matrix is a useful tool to assess and evaluate the machine learning classification model. It is a table of n*n, where n is the number of classes in the classifier.  In this article, we have focused on the classification technique interview questions. In the next article, we will focus on the interview questions related to unsupervised learning. "
57,Interview,How to find an internship in Data Science? ,"Your guide to getting a data scientist internship Landing a job in data science is not an easy thing to do because of its wide skillset. Most of the companies don’t know what skillset they are looking for. On another side, some companies need a very specific requirement for domain, skill, and experience. Data science is a new field and companies are also facing problems in finding the correct candidates. For an internship role in data science, you need to be prepared with the skillset. If you are good with python and machine learning then apply for the same kind of roles or if your good in computer vision than apply for such roles. The core intention is to be good at 2–3 core skills of data science (such as R, Python, SQL, Excel, Statistics, ML, NLP, DL, Hadoop, Spark) because data science has a variety of skills and as a fresher, you can’t be good at everything. Data science has 3 core job roles Data Scientist, Data Analyst, and Data Engineer. Data scientist roles also have Core Machine Learning, NLP, and Computer vision domain. You need to be clear about which role you want. Also, not every start-up needs to offer an internship position. Some of them will offer and some of them will not. In my opinion, you should do the following things: It can be possible some of the start-ups may directly hire a fresher for the required position. So prepare your resume and cover letter, start applying and prepare yourself for the interview. Do some mock interviews with your friends and classmates. Thanks for reading 🙂"
58,Interview,Data Science Interview Questions Part-2 (Regression Analysis) ,"Top frequently asked data science interview questions(Regression Analysis) and answers for fresher and experienced Data Scientist, Data Analyst, and Machine Learning Engineer job roles.  Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. In the previous article (Data Science Interview Questions Part-1) in this series, we have focused on basic data science interview questions related to domain search. So in this second article, we are focusing on basic data science questions related to Regression Analysis. Let’s see the interview questions. Regression analysis is a supervised statistical technique used to determine the relationship between a dependent variable and a series of independent variables. The linear regression model follows the linear relationship between the dependent and independent variables. It uses a linear equation, Y = a +bx, where x is the independent variable and Y is the dependent variable. Linear regression is easy to use, and interpret. Non-linear regression doesn’t follow Y = a +bx equation. Non-linear regression is much more flexible in curve fitting. It can be represented as the polynomial of k degrees. Mean Squared Error(MSE) is the average of squared errors of all values. Or in other words, we can say it is an average of squared differences between predicted and actual value. RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values. RMSE increases are larger than MAE as the test sample size increases. In general, MAE is steady and RMSE increases as the variance of error magnitudes increases. Mean Absolute Error(MAE) is the average of absolute or positive errors of all values. Or in other words, we can say it is an average of absolute or positive differences between predicted and actual value. MAPE (Mean Absolute Percent Error) computes the average absolute error in percentage terms. It can be defined as the percentage average of absolute or positive errors. R-square or coefficient of determination is the measure of the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model. The main problem with the R-squared is that it will always be the same or increase with adding more variables. Here Adjusted R square can help. Adjusted R-square penalizes you for adding variables that do not improve your existing model. correlation measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point. Regression measures how one variable affects another. Regression is about model fitting. It captures the causality and shows cause and effect. It is visualized by line. Multicollinearity can also be known as collinearity. It is a phenomenon where two or more independent variables are highly correlated i.e. one variable can be linearly predicted from the other variables. It measures the inter-correlations and inter-association among independent variables. Multicollinearity is caused due to the inaccurate use of dummy variables or due to any variable which is computed from the other variable in the data. It impacts regression coefficients and causes high standard errors. We can detect using the correlation coefficient, Variance inflation factor (VIF), and Eigenvalues. Variance inflation factors (VIF) measure how much the variance of an estimated regression coefficient is increased because of collinearity. It computes how much multicollinearity exists in a regression analysis. It performs ordinary least square regression that has Xi as a function of all the other explanatory or independent variables and then calculates VIF using the formula: Heteroscedasticity refers to the situation where the variability of a variable is unequal across the range of values of a second variable that predicts it. WE can detect heteroscedasticity using graphs or statistical tests such as Breush-Pagan test and NCV test. Box-cox transformation is a mathematical transformation of the variable to make it approximate to a normal distribution. Box-cox is used to transform skewed data into normally distributed data. Linear regression has the following assumptions: The core objective of linear regression is to find coefficients(α and β) by minimizing the error term. The model tries to minimize the sum of squared errors. This process is known as OLS. The OLS(Ordinary Least Squares) method corresponds to minimizing the sum of square differences between the observed and predicted values. A normal distribution is a bell-shaped curve. is a distribution that occurs naturally in many situations. For example, the bell curve is seen in tests like the SAT and GRE. The bulk of students will score the average ©, while smaller numbers of students will score a B or D. The normal distribution is the most important probability distribution in statistics because it fits many natural phenomena. For example, heights, blood pressure, measurement error, and IQ scores follow the normal distribution. It is also known as the Gaussian distribution and the bell curve. A dummy variable is a categorical independent variable. In Regression analysis, such variables are known as dummy variables. It is also known as an indicatorvariable, categorical variable, binary variable, or qualitative variable. n categories in a column always have n-1 dummy variables. Random forest is a bagging algorithm that runs multiple decision trees independently in parallel. We select some samples from the data set and for each sample decision tree will be generated. In a classification problem, performs the majority voting on final predicted values of multiple trees. In a regression problem, finds the mean of the final predicted values from multiple decision trees. It is a first-order iterative optimization technique for finding the minimum of a function. It is an efficient optimization technique to find a local or global minimum. Types of Gradient Descent The main disadvantage of linear regression is the assumption of linearity. It assumes a linear relationship between the input and output variables and fails to fit complex problems. It is sensitive to noise and outliers. It gets affected by multicollinearity. Regularization is used to handle overfitting problems. It tries to balance the bias and variance. It penalizes the learning for more complex and flexible models. L1 and L2 have Commonly used regularization techniques. L1 or LASSO(Least Absolute Shrinkage and Selection Operator) regression adds the absolute value of the magnitude of coefficient as a penalty term to the loss function. L2 or Ridge regression adds a squared magnitude of coefficient as a penalty term to the loss. In this article, we have focused on the regression analysis interview questions. In the next article, we will focus on the interview questions related to the classification techniques. Data Science Interview Questions Part-3(Classification) "
59,Interview,Data Science Interview Questions Part-1 ,"Top frequently asked data science interview questions and answers for fresher and experienced Data Scientist job role. Data Science is an interdisciplinary field. It uses statistics, machine learning, databases, visualization, and programming. So in this first article, we are focusing on basic data science questions related to domain definitions. Let’s see frequently asked interview questions for Data Scientist and Data Analyst Role.  Machine learning is the science of getting computers to act without being explicitly programmed. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.  “Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.” — Arthur Samuel  “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” — Tom Mitchell Statistics is a branch of mathematics dealing with the collection analysis interpretation and presentation of numerical data. Statistics is the discipline that concerns the collection, organization, displaying, analysis, interpretation, and presentation of data. “Statistics a body of methods for making wise decisions in the face of uncertainty.” — W.A. Wallis What is supervised and unsupervised learning? Data mining- Discovers the hidden patterns in the data. Data Mining is a set of knowledge discovery tools that aims to explore data and extract patterns and correlations.  Classification is a type of supervised learning. A classification problem is used when your target or dependent variable is categorical. It extracts the model and describes the classes. for example, a bank loan officer needs to analyze the loan applications as “safe” or “risky”. Here, safe and risky are two classes. Similarly, the sales manager wants to identify the customer who will purchase their products. Here purchase and not purchase are two classes. Classification may have multiple classes such as news article classification that may have multiple classes such as sports, politics, entertainment, business, and technology. Regression is a type of supervised learning. A regression problem is used when your target or dependent variable is continuous. It extracts the model and describes the continuous behavior. for example, a retail agent wants to predict the price of a property. Here property price is a continuous variable. Similarly, stock price, temperature, and oil price.  Linear regression is a type of regression algorithm while logistic regression is a type of classification algorithm. linear regression is used to forecast continuous variables while logistic regression is used to predict categorical variables. An example of a continuous output is house price and stock price. An example of the discrete output is predicting whether a patient has cancer or not, predicting whether the customer will churn. Linear regression is estimated using Ordinary Least Squares (OLS) while logistic regression is estimated using the Maximum Likelihood Estimation (MLE) approach. Linear regression follows normal distribution while logistic regression follows binomial regression. Deep Learning is a branch of neural network that deals with a ‘layered’ architecture to learn complex and complicated structures using multiple layers. It imitates the human brain to process the data and create non-linear models for decision making. Deep learning is used to train tasks such as speech, text, and image analytics. It attempts to top model a high level of abstraction in data using layer architecture.  Machine learning models are something that is created in the training process. The model is the output of the machine learning algorithm. The model comprises the model parameters, coefficients, and weights captured during the training process. Data wrangling sometimes referred to as data munging In recent years, unstructured data and diverse format data is increasing. Data Wrangling is the process of cleaning, arranging, and transforming data into the desired structure for further analysis.  In Business Intelligence, Data Wrangling is converting raw data into a form useful for aggregation/consolidation during data analysis. Data Wrangling: the process of transforming and mapping data from one “raw” dataform into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics. [Wikipedia] Feature engineering is all about creating, and transforming features to make a better prediction. for example, sales of a product can be impacted by day of the week so we can incorporate the day of the week from the date of purchase.  In feature engineering, we perform operations such as handling missing values, handling outliers, transformation and encoding, feature split and scaling. The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering. — Luca Massaron Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself.[Wikipedia] RDBMS stands for Relational Database Management System. RDBMS is a type of database management system used for tabular data. A relational database stores data in a structured format of rows and columns and creates a table.  NoSQL stands for Not only SQL. Here, data is nontabular and store data in non-relational formats such as key-value, columnar, and graph format. NoSQL databases are schema-less, easy to scale and handle 21st-century web data.  Model performance is a very important aspect of machine learning where we assess or evaluate the build model quality. It helps us to compare the regressor or classifier. Regressors compared based on R-Square, RMSE, MAE, and MAPE. Classifiers compared based on accuracy, error, precision, recall, and f1-score. Apart from these parameters, other factors are also important such as speed, robustness, scalability, and interoperability.  Continuous variables are numeric variables that have an infinite number of values between any two values.  Categorical variables have a finite number of distinct groups. For example, gender, marital status, and payment mode. NLP or Natural Language Processing is broadly used to automate the processing of natural language data such as speech and textual data. It helps in extracting the meaning from human languages. NLP is applicable in several problematic from speech recognition, language translation, classifying documents to information extraction. Analyzing movie reviews and finding sentiments of review is one of the classic examples of NLP. Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.[Wikipedia] Recommender systems provide product suggestions to the consumers that arelikely to be of interest to the user such as movies, books, news articles, and other services. Recommender systems can be of three types content-based, collaborative, and hybrid recommended systems.  The content-based approach recommends items that are similar to items theuser preferred or queried in the past. It relies on product features and textual item descriptions.  The collaborative method is based on the user’s social environment. It recommends items based on the opinions of other customers who have similar tastes or preferences as the user.  A data warehouse is a repository of information collected from multiple sources, stored under a unified schema, and usually residing at a single site. Data warehouses are constructed via a process of data cleaning, data integration, data transformation, data loading, and periodic data refreshing. “A data warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management’s decision-making process” Datamart: A data mart contains a subset of corporate-wide data that is of value to a specific group of users. For example, a marketing data mart may confine its subjects to customers, items, and sales.  Clustering is unsupervised learning because it does not have a target variable or class label. Clustering divides s given data observations into several groups (clusters) or a bunch of observations based on certain similarities. For example, segmenting customers, grouping super-market products such as cheese, meat products, appliances, etc. In this article, we have focused on the basic data science questions related to domain definitions. In the next article, we will focus on the interview questions related to the Regression Analysis.  Data Science Interview Questions Part-2(Regression Analysis)"
60,Python,Let’s Start with Pandas Library: Introduction and Installation ,"Pandas is one of the most fundamental Python libraries for Data Science and Analysis on tabular data. It is an open-source library that provides numerous functionalities for various operations involving the preprocessing, analysis, and manipulation of tabular data with high speed and performance. It was developed by Wes McKinney in 2008. Python Pandas is the backbone of most data analytics and Machine Learning projects, having applications in various domains, such as statistics, finance, sales, and market research, etc. The simplest way of installing and working with Pandas is using Anaconda. Anaconda is a cross-platform (Windows, Linux, Mac) Python distribution for scientific computing, data processing, and analysis. The greatest advantage is that it will also give access to various other tools and packages. If you don’t have Anaconda Navigator on your system you can install from https://www.anaconda.com, on whichever system you are using (i.e., Windows, Linux, Mac, etc.) The user can then install the packages in anaconda, or can simply open up the terminal or command line and enter the command: conda install pandas pip install pandas﻿ sudo pip install pandas﻿ sudo pip install pandas This is the introductory article on pandas and its installation. In upcoming article, we will focus on more detailed operations in pandas."
61,Python,Pandas DataFrame ,"A DataFrame is a two-dimensional labeled data structure, containing heterogeneous data. The data is arranged in a tabular format – with both a row and a column index. DataFrame is, in fact, the most widely used data structure of Pandas. It can be visualized as a spreadsheet or a collection of series, for example, This dataset is represented in rows and columns, with attributes such as Student_Name, Age, Marks, and Grade. The columns of a DataFrame can be of different data types. For example, in the above table Student_Name is a String, Age is an Integer, Marks is Float, and Grade is a String data type. DataFrame is size mutable as well as data mutable. The syntax for creating a Pandas DataFrame is: data, index, columns, dtype, copy are its parameters – Pandas DataFrame can be created from the lists, maps, dictionary, ndarray, series, or another DataFrame. In the real world applications, however, the data in DataFrame is generally loaded from CSV files, SQL databases, etc. Let us look at some basic examples of creating a DataFrame. The output of the above code would be: The output of the above code would be: Similarly, we can create DataFrames with several rows and columns using a list of lists, as: The output of the above is: The output of the above is: In this article, we have looked at the main data structures of Pandas – DataFrame. In the upcoming articles, we will focus on one more advanced data structure of pandas- Series. "
62,Python,Pandas Series ,"A Series is a one-dimensional labeled data structure that can contain data of any type (integer, float, etc.). The labels are known as index. A Series contain homogeneous data. A Series can be visualized as a one-dimensional NumPy array, for example: The size of a Series is immutable but data values are mutable. The syntax for creating a Pandas Series is: data, index, columns, dtype, copy are its parameters – Pandas Series can be created from constant values, lists, maps, dictionary, ndarray, etc. Let us look at some basic examples of creating a Series. The output obtained is: This gives the following output: Output is: The output obtained is as follows: The missing values are filled with NaN. In this article, we have looked at the main data structures of Pandas – Series. In the upcoming articles, we will focus on more operations on DataFrame and Series."
63,Python,Pandas Basic Operations ,"In this tutorial, we will focus on Pandas Basic operations such as read/write CSV, JSON, and Excel files. After that we will focus on Pandas Dataframes basic operations such as head(), tail(), info(), shape, dtype, and value_counts().   Pandas has various operations for the manipulation and analysis of data. In this article, we will look at very basic but also important operations of Pandas. For this article, we will consider the following dataset to test the various operations: This gives the following dataframe as output: Pandas can read various kinds of data, such as CSV, TSV, JSON, etc. Data can be simply loaded from these file formats into the DataFrame. Pandas functions for reading the files have the general format as .read_filetype(), where filetype is the type of file we are supposed to read, such as CSV, JSON, Excel file. To read a CSV file Pandas.read_csv() function is used. For example, to load data from a file filename.csv: Similarly to read the data from a JSON file or an Excel file, the functions are .read_json() and .read_excel() respectively. JSON file, which is generally a stored Python dictionary can be read using Pandas as: For reading an excel file, you need to make sure that the Python package xlrd is installed. If not, you can install it using conda as: Or using pip as: After this, you can read the excel file as: Pandas can also write data and labels to a file. Pandas functions for writing data to files have the general format as .to_filetype(), where filetype is the type of file we are supposed to read, such as CSV, JSON, Excel file. So, if we want to write the above student DataFrame df into a CSV file, we can do that as: A file by the name of student_record.csv is created in the current working directory. The file contents are: Similarly to write the data to a JSON file or an Excel file, the functions are .to_json() and .to_excel() respectively. For writing data to an excel file packages xlwt, openpyxl, and xlsxwriter must be installed and can be installed using either conda or pip. The above commands will create similar JSON and excel files of the student records. The .head() operation displays the first few rows of the DataFrame along with the column headers. It is an important function as it allows us to view the first few records which can be helpful in the analysis of the structure and type of data we are dealing with, especially in case of very large datasets. By default, .head() shows the first 5 rows. We can also modify the function to display the required number of rows from the beginning by passing that number as a parameter to this function. For example, df.head(7) would show the first 7 rows of the data. For the above student record, it will give output as: Just as the .head() operation shows the first few rows of the data, in a similar way the .tail() operation shows the last few rows of the data. By default, .tail() also shows the last 5 rows. We can also modify the function to display the required number of rows from the end by passing that number as a parameter to this function. For example, df.head(4) would show the last 4 rows of the data. For the above student record, it will give output as: The .info() operation gives a quick summary of the dataset. For example, For the above student record, it will give output as: pandas.core.frame.DataFrame gives the information that the data type of the df variable is DataFrame. Data columns shows the number of columns and RangeIndex gives the total entries in the dataset. The info shows data type and null values of various columns. It also gives information on memory usage. The shape() operation is used to analyze the shape of the dataset, i.e., how many rows and columns are present in the dataset. For example for the above student_record dataset, The output will be Here we can see that the dataset has 10 rows and 3 columns. This operation is significantly helpful in creating Machine Learning and Data Science models, in cases where we need to gauze the exact dimensions of the data available to us. The .dtypes operation gives a view of the type of data contained in each column. For example, This will produce the following result: The DataFrame.value_counts() operation returns a series that contains counts of unique rows in the DataFrame. The syntax is: The parameters are: For example, Output is: The Series.value_counts() is an equivalent operation on Series that returns a series containing counts of unique values. The output is in descending order showing the most frequent values first. Its syntax is: The parameters are: For example, This will produce the following output: Summary In this article, we looked at some basic operations of Pandas. In the next one, we will focus more on Data manipulation techniques and other operations."
64,Python,Data Manipulation using Pandas ,"In this article, will look at certain ways to modify Pandas DataFrames. We will consider the following dataset of student_records: This gives the following dataframe as output: To select a column(s) in Pandas DataFrame, we can access the columns by their columns’ names. For example, This will only select the columns ‘Name’ and ‘Marks’. To retrieve rows from a DataFrame, DataFrame.loc[] method is used. They can also be selected by passing an integer location to an iloc[] function. DataFrame.ix[] is used for both label and integer-based locations. You can select rows based on the specified conditions. For example, in the student_records, if we want to select students whose age is 14, then: The output is: We can also select students, whose marks are >=80, then Output is: Suppose you want to filter only certain rows (or columns) of the data for analysis. This often occurs in data analytics, that we are concerned with only certain rows or columns and not the entire dataset. DataFrame.filter() function is for this purpose. It is used to subset certain rows or columns based on the labels in the specified index. Its syntax is: The parameters are: For example, If you want to filter the student_records dataset by selecting only the columns ‘Name’ and ‘Marks’, then: This would give: Similarly, we can filter according to rows by setting the axis=0 and setting the row indices. DataFrame.sort_values() is the operation used to sort Pandas DataFrame. Its syntax is: The parameters are: For example, To sort the above student_records dataset such that the names are in ascending order, you need to have the following code: When you run the code, you can see that the data is sorted in ascending order of ‘Name’ as: To sort the values in descending order, you just need to set the parameter “ascending=False”. Suppose you want to sort the DataFrame by ‘Marks’ in descending order (useful to determine ranks), then: Thus, we get: You can also sort the DataFrame with respect to multiple columns. For example, you want to sort by both ‘Age’ and ‘Name’, then: Then we get the sorted data as: The data above is sorted by both ‘Age’ and ‘Name’. The ‘Age’ column takes the priority while sorting, as it was placed in the df.sort_values before the ‘Name’ column. In this article, we covered various methods for selecting, filtering, and sorting a DataFrame. In the next article, we will see how to iterate over rows and columns in Pandas DataFrame."
65,Python,Iterating over rows and columns in Pandas DataFrame ,"Iteration is the process in which we traverse the DataFrame, going over the items, and doing the necessary tasks. In this article, we will look at different ways of iterating over rows and columns in Pandas. Iterating over Pandas DataFrame can be visualized in a way similar to a Python dictionary. Just like there are key-value pairs in a dictionary, in a similar way, we iterate over keys and get the value pairs accordingly. Here we will consider the following student_record DataFrame to demonstrate our iterations over rows and columns: Thus, the DataFrame is: Now let’s get started with various kinds of iterations in Pandas. There are several ways to iterate over rows in Pandas DataFrame. Let’s have a look at them. This operation returns each index value along with a series containing the record of each row’s set of values. Let’s take a look at its working: Following is the output obtained: This function iterates over rows and returns a tuple for each row in the DataFrame. Each returned tuple consists of the index value along with all other values of the record. Take a look at the code: The returned output is: The various ways of iterating over columns in Pandas DataFrame are: This operation iterates over each column, taking the column label as key and the column data as the value of the key-value pair. Let’s use this method on the above student_record data: We get: We can also iterate over the columns by creating a list of column labels and then iterating over that list, as: Output obtained after terating over all columns in the list is: You can also choose only certain labels in your list and iterate over them, i.e., create a list with only those column names which you want to iterate over. Summary In this article, we looked at various ways for iterating over rows and columns in Pandas DataFrame. In the upcoming articles, we will look over various other operations such as apply(), map(), reduce(), etc."
66,Python,apply() in Pandas ,"apply() in Pandas is used to apply a function(e.g. lambda function) to a DataFrame or Series. This is highly useful in various Machine Learning and Data Analysis projects where we need to separate data based on certain conditions or apply lambda functions to a DataFrame or Series. Using Pandas apply(), we can apply a function along an axis of a DataFrame. The function is applied along each row if axis=0, and it is applied along each column if axis=1. The syntax is: The parameters are: Let’s look at various examples of the result of applying Pandas apply() on DataFrame. Consider the following DataFrame: Our DataFrame looks like: Let us apply a custom function to the DataFrame values. The function is: Now apply this function to the DataFrame df: As a result, the function gets applied to each and every value of the DataFrame: Now let’s apply some built-in NumPy functions: Output: We can also apply a function to each row or each column as: The result of summing entries of each row is: We can also apply list-like values using a lambda function. This results in: We can also apply a function on the values of a Series using the apply() function. The syntax for apply() on series is: The parameters are: Consider the following Series: This gives the following Series: Applying the function to this Series: This gives: Another example: Output: Similarly, we can apply various types of built-in and user-defined functions to the Series. In this article, we looked at the apply() function of Pandas. The next article will focus on map() and reduce() operations."
67,Python,Pandas map() and reduce() Operations ,"In this article, we will focus on the map() and reduce() operations in Pandas and how they are used for Data Manipulation. Pandas map() operation is used to map the values of a Series according to the given input value which can either be another Series, a dictionary, or a function. map() operation does not work on a DataFrame. Syntax: The parameters are: Let us look at few examples of map() operation on the following Series: This gives the following Series: Now applying map() operations on this Series, by using a dictionary as an argument: Output: You can also map it to a function, for example: Output: If we don’t use na_action=‘ignore’ here, then it would change the line at index 2 as – “He is from nan”. reduce() operation is used on a Series to apply the function passed in its argument to all elements on the Series. reduce() is defined in the functools module of Python. The way the algorithm of this function works is that initially, the function is called with the first two elements from the Series and the result is returned. The function is now applied to this result and the next element in the Series. The process keeps repeating itself until there are items in the sequence. The final result is ultimately returned by the function. For example, consider the following series: The series is: Now, let’s apply a function on this Series that uses reduce to find the product of all elements in the list: Output: Look at another example which uses reduce() to find minimum element of the Series: Output: In this article, we looked at map() and reduce() functions. In the next one, we will look at ways to handle missing values in Pandas."
68,Python,Handling Missing Values in Pandas ,"In a real-life scenario, we often come across datasets with missing values. However, we need to handle these missing values in order to perform Data Analysis or Machine Learning operations. Properly cleaned data makes it easier and more accurate to perform various functionalities. Missing data is generally represented by null, None, or NaN. In this article, we will look at various ways to detect, remove, or replaces data in Pandas. For this purpose let’s work on the following student_record dataset: The DataFrame is: We can see there are several NaN values in the DataFrame. isnull() function is used to check the DataFrame for missing values. It returns a DataFrame with Boolean values which are ‘True’ if the cell has NaN value. For example, for the student_record DataFrame: This gives the following output: We can also use this to display the rows which have null values: Output: Similar to the isnull() function, notnull() function returns ‘True’ for the rows which do not have null values. Eg: Output: dropna() will remove or drop the rows which contain null values. Eg: Output: We can also remove columns having null values using this function: Output: This function is used to replace the missing value with some value. Eg: This gives: We can also fill the values using previous values or string values, eg: Output: replace() is used to replace the missing values using required values. For example, This will replace the NaN values with 12.0 In this article, we looked at several ways to handle missing values in Pandas. In the upcoming article, our focus would be on Grouping data in Pandas."
69,Python,Grouping Data in Pandas ,"Putting related records in groups makes management and handling of data easier. Grouping data in Pandas is done by .groupby() function. It is used to group data into Series or DataFrames based on the criteria provided. The syntax is: The parameters are: We will work on the following student_record dataset: The data is: We can view groups by using the groupby() method. For example, to group students by country and view the groups, we can do the following: This gives a dictionary output with group label as key and indices as values: We can also view these groups as: Output: get_group() operation can be used to select a single group. For example: This displays record of a particular group, i.e., USA: Data can also be grouped using multiple columns. For example grouping using ‘Country’ and ‘Age’ labels: We get: We can also use Pandas groupby() to count entries in a group or calculate the average of a particular label of groups as: This gives: Various other operations such as filter(), apply(), etc. can be applied to groups using groupby(). One such example is: In this example, we have filtered those age-groups which contain more than 2 records, as: In this article, we looked at the groupby() method of Pandas and the various ways that can be used to group data."
70,Python,Merging and Joining in Pandas ,"Pandas provide various functionalities for combining separate datasets. In this article, we will look at methods for merging, joining, and concatenating datasets. For merging data, we use merge() operation. It combines data on common columns or indices. Both the initial datasets need to have a common key column on which the DataFrames would be joined. Let us look at the following examples of merging the DataFrames: The two DataFrames are: Now to merge these two DataFrames on the common key “S_Id”, we do: The following merged DataFrame is obtained: We can also merge DataFrames on multiple keys. For example, consider the following DataFrames: The DataFrames are: They have two columns common, i.e., S_Id and Age, but not all the combinations are common. Let us now merge these two DataFrames: We get: Notice that only the S_Id-Age records which are common for both DataFrames have been merged. The above is an example of Inner Join. Using Pandas, we can also merge datasets using other different types of Joins, i.e., Left Join, Right Join, and Outer Join along with the Inner Join. Let us visually understand the different forms of Joins: The first circle, i.e., (a+b) refers to the keys of the 1st DataFrame (df1). The second circle, i.e., (b+c) refers to those of 2nd DataFrame (df2). The various joins are: So, if we want the above example to be an Outer join, then modify the code as: We get: Notice that the key here is the S_Id-Age column combination (i.e., a union of the two) and those values are unique. The missing cells are filled with NaN, as apparent above. Similar to merge, we can also join one DataFrame to other DataFrame which can be differently indexed. The join() function is used for this purpose. By default, this function will attempt to left join the two DataFrames. Let’s look at an example to join the following two DataFrames: Following are the two DataFrames: Now, to join df1 with df2, we do: The following joined DataFrame is obtained: We can also change the type of join using the “how = <join type>” similar to that in merge() operation. This article focused on Merging and Joining data in Pandas. The next article will focus on concatenating dataframes. "
71,Python,Concatenating data in Pandas ,"Concatenation combines one or more different DataFrames into one. The concat() function of Pandas for combining DataFrames across rows or columns. Consider the following DataFrames: The three DataFrames are: Now, to concatenate them into one, we use: Using concat(), we can also concatenate along the columns. We just need to change the parameter “axis=1”. The type of join can also be specified. For example: The two DataFrames are: And the concatenated one is: This article focused on Concatenating data in Pandas. The next article will focus on crosstab, pivot tables and melt() function operations in pandas. "
72,Python,"Working with crosstab, pivot_tables, and melt functions in Pandas ","In this article, we will work with a few of the general functions of Pandas, namely crosstab, pivot_table, and melt. Pandas crosstab() function is used for computing a simple cross-tabulation of two or more factors. It computes a frequency table of the factors by default unless an array of values and an aggregation function are passed. The syntax is: The parameters are: Let us look at some examples of Pandas crosstab. Consider the following data: The DataFrame is: Now let’s create a crosstab table by ‘Country’ and ‘Gender’: The resulting crosstab table is: The table stores the count of the number of records by ‘Country’ and ‘Gender’. We can also create a crosstab of the number of students of Country age-wise and ‘Gender’. For this take ‘Country’ and ‘Age’ as indices and ‘Gender’ as columns: Output: Pandas pivot_table is used to create a spreadsheet-style pivot table as a DataFrame. The levels of the pivot table will be stored in multiIndex objects on the index and columns of the resulting DataFrame. The syntax is: The parameters are: Let’s look at some examples of pivot tables. Consider the following student record: The DataFrame is: Now to create a pivot table which stores the average age of male and female students of different countries, the code is: This results in the following Pivot table: Here, by default, the aggfunc is np.mean. We can also store max and min age of respective genders of the countries, as: This gives: Pandas melt() function is used for transforming or reshaping DataFrames. It unpivots a DataFrame from wide to long format. The syntax is: The parameters are: Let’s look at some examples of the melt() operation. Consider the following data: The DataFrame is: Applying melt() function on this DataFrame: This results in: In this article, we look at three functions of Pandas, namely, crosstab, pivot_tables, and melt. In the upcoming article, we will work with Pandas Date and Time."
73,Python,Working with Pandas Date and Time ,"Date and Time are commonly occurring and one of the important features in Data Science and Machine Learning problems. We often come across time-series data or problems regarding stock market predictions where we need to work with Date and Time functionalities. Pandas provide a wide variety of features to work with Date and Time data. In this article, we will learn to work with Date and Time manipulation with Pandas. Pandas can be used to parse a flexibly formatted string date from various sources and formats. Pandas to_datetime() function is used for such purposes. Output: Output: Output: We can create a sequence of Dates and Times ranges of certain fixed frequency using the Pandas date_range() function. For example, the following generates a sequence of days from 13th October 2020 to 20th October 2020. The result is: Here, freq=‘D’ implies the implies the intervals are a day ahead of the previous one. We can also set this as month(M), day(D), etc. We can also put the required number of DateTime values from the initial values using the ‘preiods’ parameter. For example: Output: The DateTime information can also be manipulated with time zone information using Pandas tz_localize() operation. For example: Output: We can also convert these date_ranges into DateTime features. Look at the code below: Output: Timestamp and Period are time span data structures. Timestamped data is the most basic type of data of Time Series, using the points in time. Date and Time arithmetic can be performed on Timestamped data using Pandas. Example: Output: Period is useful in cases where instead of representing exact DateTime, we need to represent a time span. Example: Output: We can use Timestamp to get the present time. Returns the present time: In this article, we worked with Pandas Date and Time. In the upcoming article, we will look at String columns in Pandas."
74,Python,Working with Strings in Pandas ,"In this article, we will work with Strings in Pandas DataFrames and Series. Pandas library provides some built-in string functions for manipulating data. Let’s create a Pandas Series with String values. Output: We can see that the dtype of this is ‘object’. We can convert the given Series or DataFrame to ‘string’ dtype. Or, also: The above two codes will return the same output: Note: The above two conversions work only on Python-2 and not on Python-3 Converts all uppercase strings to lowercase, and returns the series with lowercase. Output: Converts all lowercase strings to uppercase, and returns the series with lowercase. Output: Use to split each string in the Series or DataFrame with the given pattern, and then returns the list containing elements which were separated by that pattern. Output: Removes leading or trailing spaces in the strings. Output: Concatenates each string in the Index of the DataFrame or series with the specified separator. Returns the concatenated string. Output: Returns length of each string in the Series or the Index of the DataFrame. Output: Returns true if all alphabetical characters in each string in the Series or the Index of the DataFrame is lowercase. Output: Returns true if all alphabetical characters in each string in the Series or the Index of the DataFrame is uppercase. Output: Returns true if all characters in each string in the Series or the Index of the DataFrame is numeric. Output: Returns true if the string in the Series or DataFrame Index starts with the given pattern. Output: Returns true if the string in the Series or DataFrame Index ends with the given pattern. Output: This function returns One-Hot Encoded values in a DataFrame. The value is 1 for that element’s relative index else 0. Output: Replaces the first argument value with the second argument value. Output: Repeats each string by the given number of repetitions. Output: Returns count of the given pattern in each element in Series or Data-Frame. Output: Returns the position where the specified pattern first occurs. Output: Returns list of all occurrences of the specified pattern. Output: Converts uppercase to lowercase and vice-versa. Output: In this articl, we worked with Srings in Pandas. Next article will focus on Pandas Data Visualization."
75,Python,Data Visualization using Pandas ,"Data Visualization is the representation of data in a graphical format that facilitates comprehension and provides a deeper insight into understanding the data. Data can be represented using graphs, charts, pictures, etc. Pandas is one of the most commonly used Python libraries for Data Analysis. In this article, we will focus on Data Visualization using Pandas. Consider the following data: The DataFrame is: This sales_record dataset consists of information such as the sales and profit of a company over the years. Let’s plot some graphs to visualize this data more clearly. A line plot is useful to visualize the frequency of data along the number line. This is highly useful in the case of Time-series data. We can visualize the trend in Profit percentage of the company over the given 10 years using a line graph. The code for doing so in Pandas is: Output: Data can also be visualized using horizontal or vertical straight lines. For example, a bar graph can be used to visualize sales in various years as: Output: Multiple variables can also be represented on the same graph: Output: A histogram is useful for showing distribution frequency for continuous data. For example, for the sales_record data, to view the frequency distribution of profit percentage: Output: We can also visualize the sales data using a Pie chart. This is useful for a quick comparison between the quantities. For example to view the sales of various indices: Output: It is used to graphically represent quantitive areas in form of their areas. This is useful for comparisons. For example, the profit trend in the sales_record data is: Output: We can also view several quantities with their areas stacked on top of other, as: Output: To view data in form of Scatter plots, can be done in Pandas as: Output: Plots hexagons for intersecting data points of x and y-axis. Pandas uses the hexbin() method to achieve the same. For example, for the sales_record data: Output: This plots a smooth distribution curve for the density of the given values. For example, for the profit percentage in sales_record data: Output: In this article, we looked at Data Visualization using Pandas. In the next article, we will focus on Data Visualization using Matplotlib."
76,Python,Data Visualization using Matplotlib ,"Matplotlib is the most popular Python library for Data Visualization. It is a multi-platform, 2D plotting library and supports a wide variety of Operating Systems. In this article, we will focus on Data Visualization using matplotlib. We generally import matplotlib as: Let’s consider the following sales_records data for visualization using matplotlib: The DataFrame is: This sales_records dataset consists of the sales profile of three companies over the years. Let’s plot some graphs to visualize this data more clearly. A line plot is useful to visualize the frequency of data along the number line. This is highly useful in the case of Time-series data. We can visualize the trend in Sales of the Company1 over the given 10 years using a line graph. The code for doing so using Matplotlib is: Output: plt.xlabel() is used to label the x-axis. Similarly, plt.ylabel() labels the y-axis. plt.show() is used to display the plot. The color of the plot can also be modified. For example, to get the line plot in red color: Output: We can also view the Sales trend of three companies together in the same plot, as: Output: plt.title() is used to specify the title of the chart. plt.legend() displays associated legend. Functions can be plotted using line graphs of matplotlib, as: Output: Data can also be visualized using horizontal or vertical straight lines. To do so in matplotlib: Output: Multiple variables can also be represented on the same graph. This can be done with some modifications as: Output: A histogram is useful for showing distribution frequency for continuous data. For example, for the generated random data, we can view the distribution, as: Output: We can also visualize the sales data using a Pie chart. This is useful for a quick comparison between the quantities. Consider the example for the following data: Output:  We can also plot a stack plot where data of different categories are stacked together. This is an extension of the line chart and bar plot. For example, for sales_records data, the stack plot is: Output: To view data in form of Scatter plots, can be done using matplotlib as: Output: Box plot displays from 1st to 3rd quartile of a set of data containing the minimum, maximum, first quartile, third quartile, and median. Consider the example on the following random data: Output: In this article, we worked with Data Visualization using Matplotlib, its various features, and the different types of graphical representations that can be achieved through it. In the next tutorial, we will focus on visualization using Seaborn."
77,Python,Data Visualization using Seaborn ,"Seaborn is a Python library built on top of matplotlib. Seaborn is basically a Data Visualization library with a wide variety of wonderful styles and features for statistical plotting. In this article, we will focus on Data Visualization using Seaborn. Seaborn library has some mandatory dependencies which need to be installed first. They are: Seaborn can be installed using any of the two methods given below: To get started, we import the dependencies and the seaborn package, as: Consider the following data: The DataFrame is: This sales_record dataset consists of information such as the sales of three companies over the years. Let’s plot some graphs to visualize this data more clearly. A line plot is useful to visualize the frequency of data along the number line. We can visualize the sales trend of the companies over the given 10 years using a line graph. The code for doing so in Seaborn is: Output: Data can also be visualized using horizontal or vertical straight lines. In Seaborn, barplot() function works on a full dataset and plots the mean of the categories, as: Output: Seaborn can also be used to plot the distribution of data using Histograms, KDE plots, or joint distributions. The function for plotting the univariate distribution of data using Seaborn is distplot(). This function depicts both the histogram and the KDE function for the data. For example, using distplot() for visualizing the distribution of sales for Company3: Output: To view only the histogram, we can set “kde=False”, as: Output: The use of these plots can be visualized more properly in a distributed data like: Output: We can also view KDE plots separately using the kdeplot() function. Let’s visualize this for the following multivariate data: Output: We can also plot the joint distribution using seaborn. Consider the following examples: Output: Hexplot can also be plotted using the jointplot() function, by changing kind=‘hex’, as: Output: To view data in form of Scatter plots, can be done in using the relplot() function of Seaborn. Consider it on the sales_record data: Output: We can do the same using the scatterplot() function of Seaborn. Let’s view it on some random multivariate data: Output: The sales_records data is a good example to visualize categorical data, where we are viewing data of three different companies. With seaborn, this is done using the catplot() function. For example: Output: Using seaborn, we can view categorical data in different ways such as Jitter plot, Hue plot, Point plot, Box plot, etc. Jitter is the deviation from the true value. In the above plot, we can set jitter as False, which will give vales in a straight line rather than scattered: Output: Using a point plot, we can point out the estimate value and confidence interval. We can do so for the original sales_records data for three companies, as: Output: Box plot shows three quartile values of the distribution. For the sales_record data: Output: The data can also be represented using a violin plot, as: Output: If there we want to introduce another dimension such that its categories are visualized in different colors, then we set that feature in the hue parameter. Consider the following code: Here, Type is of A and B, which can be represented using hue as: In this article, we looked at Data Visualization using Seaborn. We have seen various plots such as line plot, bar plot, histogram, distribution plot, joint plot, scatter plot, cat plot and box plot."
78,Python,Data Visualization using Seaborn ,"Seaborn is a Python library built on top of matplotlib. Seaborn is basically a Data Visualization library with a wide variety of wonderful styles and features for statistical plotting. In this article, we will focus on Data Visualization using Seaborn. Seaborn library has some mandatory dependencies which need to be installed first. They are: Seaborn can be installed using any of the two methods given below: To get started, we import the dependencies and the seaborn package, as: Consider the following data: The DataFrame is: This sales_record dataset consists of information such as the sales of three companies over the years. Let’s plot some graphs to visualize this data more clearly. A line plot is useful to visualize the frequency of data along the number line. We can visualize the sales trend of the companies over the given 10 years using a line graph. The code for doing so in Seaborn is: Output: Data can also be visualized using horizontal or vertical straight lines. In Seaborn, barplot() function works on a full dataset and plots the mean of the categories, as: Output: Seaborn can also be used to plot the distribution of data using Histograms, KDE plots, or joint distributions. The function for plotting the univariate distribution of data using Seaborn is distplot(). This function depicts both the histogram and the KDE function for the data. For example, using distplot() for visualizing the distribution of sales for Company3: Output: To view only the histogram, we can set “kde=False”, as: Output: The use of these plots can be visualized more properly in a distributed data like: Output: We can also view KDE plots separately using the kdeplot() function. Let’s visualize this for the following multivariate data: Output: We can also plot the joint distribution using seaborn. Consider the following examples: Output: Hexplot can also be plotted using the jointplot() function, by changing kind=‘hex’, as: Output: To view data in form of Scatter plots, can be done in using the relplot() function of Seaborn. Consider it on the sales_record data: Output: We can do the same using the scatterplot() function of Seaborn. Let’s view it on some random multivariate data: Output: The sales_records data is a good example to visualize categorical data, where we are viewing data of three different companies. With seaborn, this is done using the catplot() function. For example: Output: Using seaborn, we can view categorical data in different ways such as Jitter plot, Hue plot, Point plot, Box plot, etc. Jitter is the deviation from the true value. In the above plot, we can set jitter as False, which will give vales in a straight line rather than scattered: Output: Using a point plot, we can point out the estimate value and confidence interval. We can do so for the original sales_records data for three companies, as: Output: Box plot shows three quartile values of the distribution. For the sales_record data: Output: The data can also be represented using a violin plot, as: Output: If there we want to introduce another dimension such that its categories are visualized in different colors, then we set that feature in the hue parameter. Consider the following code: Here, Type is of A and B, which can be represented using hue as: In this article, we looked at Data Visualization using Seaborn. We have seen various plots such as line plot, bar plot, histogram, distribution plot, joint plot, scatter plot, cat plot and box plot."
79,Python,Solving Linear Programming using Python PuLP ,"Learn how to use Python PuLP to solve linear programming problems. As Senior operation manager, your job is to optimize scarce resources, improve productivity, reduce cost and maximize profit. For example, you want to maximize the profit of the manufacturing unit with constraints like labor working hours, machine capacity, and available raw material. Another example, As a marketing manager wants to allocate the optimum budget among alternative advertising media channels such as radio, television, newspaper, and magazine. Such problems can be considered optimization problems.  Optimization problems can be represented as a mathematical function that captures the tradeoff between the decisions that need to be made. The feasible solutions of such problems depend upon constraints specified in mathematical form. Linear programming is the core of any optimization problem. It is used to solve a wide variety of planning and supply chain optimization. Linear programming was introduced by George Dantzig in 1947. It uses linear algebra for determining the optimal allocation of scarce resources. In this tutorial, we are going to cover the following topics: LP models are based on linear relationships. It assumes that the objective and the constraint function in all LPs must be linear. This mathematical technique is used to solve the constrained optimization problem.  There are 4 basic assumptions of LP models: These are the following methods used to solve the Linear Programming Problem: Graphical is limited to the two-variable problem while simplex and Karmakar’s method can be used for more than two variables. There are various excellent optimization python packages are available such as SciPy, PuLP, Gurobi, and CPLEX. In this article, we will focus on the PuLP python library. PuLP is a general-purpose and open-source Linear Programming modeling package in python.  Install pulp package: PuLP modeling process has the following steps for solving LP problems:  This problem is taken from Introduction to Management Science By Stevenson and Ozgur.  A furniture company produces a variety of products. One department specializes in wood tables, chairs, and bookcases. These are made using three resources labor, wood, and machine time. The department has 60 hours of labor available each day, 16 hours of machine time, and 400 board feet of wood. A consultant has developed a linear programming model for the department. x1= quantity of tables x2= quantity of chairs x3= quantity of bookcases Objective Function: Profit = 40*x1+30*x2+45*x3 Constraints:  In this step, we will import all the classes and functions of pulp module and create a Maxzimization LP problem using LpProblem class. In this step, we will define the decision variables. In our problem, we have three variables wood tables, chairs, and bookcases. Let’s create them using LpVariable class. LpVariable will take the following four values: In this step, we will define the maximum objective function by adding it to the LpProblem object.  In this step, we will add the 4 constraints defined in the problem by adding them to the LpProblem object.  In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  Congratulations, you have made it to the end of this tutorial! In this article, we have learned Linear Programming, its assumptions, components, and implementation in the python PuLp library. We have solved the Linear programming problem using PuLP. Of course, this is just the beginning, and there is a lot more that we can do using PuLP in Optimization and Supply Chain. In upcoming articles, we will write more on different optimization problems and its solution using Python. You can revise the basics of mathematical concepts in this article."
80,Python,Solving Staff Scheduling Problem using Linear Programming ,"Learn how to use Linear Programming to solve Staff Scheduling problems.  As Senior operation manager, your job is to optimize scarce resources, improve productivity, reduce cost and maximize profit. For example, scheduling workers’ shifts for the most effective utilization of manpower. We need to consider the various restrictions of the total working hours of each employee, the number of shifts, shift hours, and other constraints. Such a problem can be considered an optimization problem. Staff or workforce scheduling is used in numerous use-cases like nurse staff scheduling in a hospital, air flight scheduling, staff scheduling in the hotel, and scheduling of drivers. Such schedules can be created based on various time periods like hours, days, weeks, and months. Various organizations use spreadsheets and software. Poorly managed schedule causes overlapping of employee allocation, no breaks between shifts. Ultimately it will cause poor employee performance. For effective workforce scheduling, we need to consider the number of constraints and formulate them in the right manner. Workforce scheduling will help in effective human resource utilization, balanced timing, balanced workload, reduce employee fatigue and give importance to individual preferences (link).  Linear programming is a mathematical model for optimizing the linear function. We can achieve the best results using linear programming for a given specific set of constraints. Linear programming is widely used in management and economic science problems such as production planning, network routing, resource scheduling, and resource allocation. Linear programming can also be helpful in scheduling human resources. Such type of problem is known as Staff Scheduling or Workforce Scheduling problems.  In this tutorial, we are going to cover the following topics:  In this problem, a saloon owner wants to determine the schedule for staff members. The staff consists of the full-time shift of 9 hours and part-time shift of 3 hours.  The saloon’s opening hours are divided into 4 shifts of 3 hours each. In each shift, different levels of demands are there that need the different number of staff members in each shift. The required number of nurses for each shift is mentioned in the below table: There is at least 1 full-time employee we need in each shift. The full-time employee will get $150 for 9 hours shift and the part-time employee will get $45 per shift. PuLP is an open-source library in Python for solving linear programming problems. In order to solve linear programming problems using PuLP, we need to formulate the objective function. PuLP will optimize to maximize or minimize the objective function. PuLP modeling process has the following steps for solving LP problems:   Decision Variables xi = Number of full-time employees scheduled in shift i.  yi = number of part-time employees scheduled in shift i. Objective Function: minimize Z= 150( x0 + x1 + x2 + x3 ) + 45( y0 + y1 + y2 + y3 ) Constraints 1:  x0 + y0 ≥ 6   x0 + x1 + y1 ≥ 8  x1 + x2 + y2 ≥ 11   x2 + x3 + y3 ≥ 6  Constraints 2:   x0 ≥ 1  x1 ≥ 1   x2 ≥ 1   In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have three variables wood tables, chairs, and bookcases. Let’s create them using LpVariable.dicts() class. LpVariable.dicts() used with Python’s list comprehension. LpVariable.dicts() will take the following four values:  In this step, we will define the minimum objective function by adding it to the LpProblem object. lpSum(vector) is used here to define multiple linear expressions. It also used list comprehension to add multiple variables.  In this code, we have summed up the two variables(full-time and part-time) list values in an additive fashion. Here, we are adding two types of constraints: employee starting shift constraints and minimum full-time employees during any period. We have added the 4 constraints defined in the problem by adding them to the LpProblem object.   In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  In this article, we have learned about Staff Scheduling problems, Problem Formulation, and implementation in the python PuLp library. We have solved the staff scheduling problem using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems and its solution using Python. You can revise the basics of mathematical concepts in this article and learn about Linear Programming in this article. "
81,Python,Solving Cargo Loading Problem using Integer Programming in Python ,"Learn how to use Python PuLP to solve Cargo loading problems and Knapsack Problems using Integer Programming. Linear programming deals with non-integer solutions but in certain scenarios, we need integer solutions such as the number of products to manufacture, number of apartments to construct, number of trees to plan. In this approach, we optimize the linear function and set of linear constraints on integer variables. Integer programming is widely utilized in the area of job scheduling, inventory management, transportation, computer science, and production planning. In this tutorial, we are going to cover the following topics: There are three types of Integer programming problems: The knapsack problem is defined as how many units of each different kind of item or product to put in a knapsack with a given capacity in order to maximize profit. It is also known as the fly-away kit problem because a jet pilot decides the most valuable items to take abroad a jet. The knapsack problem is a special case of integer programming where the objective function is maximized with a single less than or equal to linear constraint. It has many applications in the real world. The cargo loading problem is a typical example of a knapsack problem. This type of problem solves the capacity planning problem of a shipment. We load the items into a shipment with limited capacity. The main objective is to load the most optimum items in the shipment. Suppose there are n items 1,2,3 … n and the number of units of each item is mi. The weight per unit of item i is wi, ri is the revenue per unit of item I, and W is the total capacity of cargo.  One Cargo shipment of Shakti Pumps has a capacity of 10 tons. Shakti Pumps wants to ship three types of pumps A, B, and C in this shipment. Objective Function:  Maximize Z = 12X1 + 25X2 + 38X3 Constraints:   12X1 + 25X2 + 38X3≤ 10 (Cargo storage capacity 10 ton)  In this step, we will import all the classes and functions of pulp module and create a Maximization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have three variables A, B, and C. Let’s create them using LpVariable class. LpVariable will take the following four values: In this step, we will define the maximum objective function by adding it to the LpProblem object. In this step, we will add only 1 constraint defined in the problem by adding them to the LpProblem object. In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop. In this article, we have learned about Integer Programming, Knapsack Problems, Cargo Loading Problems, Problem Formulation, and implementation in python using the PuLp library. We have solved the cargo loading problem using an integer programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems and its solution using Python. You can revise the basics of mathematical concepts in this article and learn about Linear Programming in this article. "
82,Python,Solving Transportation Problem using Linear Programming in Python ,"Learn how to use Python PuLP to solve transportation problems using Linear Programming. In this tutorial, we will broaden the horizon of linear programming problems. We will discuss the Transportation problem. It offers various applications involving the optimal transportation of goods. The transportation model is basically a minimization model. The transportation problem is a type of Linear Programming problem. In this type of problem, the main objective is to transport goods from source warehouses to various destination locations at minimum cost. In order to solve such problems, we should have demand quantities, supply quantities, and the cost of shipping from source and destination. There are m sources or origin and n destinations, each represented by a node. The edges represent the routes linking the sources and the destinations. In this tutorial, we are going to cover the following topics: The transportation models deal with a special type of linear programming problem in which the objective is to minimize the cost. Here, we have a homogeneous commodity that needs to be transferred from various origins or factories to different destinations or warehouses.  Types of Transportation problems Methods for Solving Transportation Problem: Let’s see one example below. A company contacted the three warehouses to provide the raw material for their 3 projects. This constitutes the information needed to solve the problem. The next step is to organize the information into a solvable transportation problem. Let’s first formulate the problem. first, we define the warehouse and its supplies, the project and its demands, and the cost matrix. In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have various Route variables. Let’s create them using LpVariable.dicts() class. LpVariable.dicts() used with Python’s list comprehension. LpVariable.dicts() will take the following four values: Let’s first create a list route for the route between warehouse and project site and create the decision variables using LpVariable.dicts() the method. In this step, we will define the minimum objective function by adding it to the LpProblem object. lpSum(vector)is used here to define multiple linear expressions. It also used list comprehension to add multiple variables. In this code, we have summed up the two variables(full-time and part-time) list values in an additive fashion. Here, we are adding two types of constraints: supply maximum constraints and demand minimum constraints. We have added the 4 constraints defined in the problem by adding them to the LpProblem object.  In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop. From the above results, we can infer that Warehouse-A supplies the 300 units to Project -2. Warehouse-B supplies 150, 150, and 300 to respective project sites. And finally, Warehouse-C supplies 600 units to Project-3.  In this article, we have learned about Transportation problems, Problem Formulation, and implementation using the python PuLp library. We have solved the transportation problem using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems such as transshipment problem, assignment problem, balanced diet problem. You can revise the basics of mathematical concepts in this article and learn about Linear Programming in this article."
83,Python,Solving Blending Problem in Python using Gurobi ,"Learn how to use Python Gurobi to solve the blending problems using Linear Programming.  A blending problem is very similar to a diet problem. It is a well-known optimization problem. The main objective in such type of problem is to find the optimum combination of mixing different intermediates items or ingredients in order to minimize blending cost, meeting the quality standards and demand requirements of the final product.  The blending problem has have been used in various different applications and industries coal blending, blending of chemical fertilizers, mineral blending, Food processing. In the food processing industry, it is used to blend the number of ingredients with minimum cost. For example, multi-grain bread requires the blending of grains. blending tea recipe, blending juice or energy drinks.  In this tutorial, we will solve the blending problem for a food processing company. In the blending problem, we find a mix of ingredients for developing any food item. For example, A food processing manager wants to develop a new formula for Juice with a given restriction of sugar, levels of sugar in fruits, and cost of fruits while still meeting their nutritional standards. In such a problem, the manager can formulate the LP problem. In this tutorial, we are going to cover the following topics: There are various excellent optimization python packages are available such as SciPy, PuLP, Gurobi, and CPLEX. In this article, we will focus on the gurobipy python library. Install gurobipy package: GurobiPy modeling process has similar solving steps just like the PuLP library. Here are the steps for solving LP problems using gurobipy: In this step, we will import all the classes and functions of gurobipy module and create a new model object using Model class.  In this step, we will define the decision variables. In our problem, we have two variables ingredient-1 and ingredient-2. Let’s create them using addVar()method.   The addVar() will take the following two values: Binary variables must be either 0 or 1. Integer variables can take any integer value between the specified lower and upper bounds. Semi-continuous variables can take any value between the specified lower and upper bounds, or a value of zero. Semi-integer variables can take any integer value between the specified lower and upper bounds, or a value of zero. Let’s first create decision variables using addvar() method: In this step, we will define the minimum objective function by adding it to the Model object using setObjective() method. Constraint captures the restriction on the values of the decision variables. The simplest example is a linear constraint, which states that a linear expression on a set of variables takes a value that is either less-than-or-equal, greater-than-or-equal, or equal to another linear expression.  Here, we are adding two types of constraints: Protein constraints and Carbohydrates constraints. We are also adding the non-negativity constraints.  In this step, we will solve the LP problem by calling optimize() method. We can print the final value by using the following for loop.  From the above results, we can infer that the optimal amounts of the two ingredients are 6.67 and 2.0.  In this article, we have learned about Blending Problem, Problem Formulation, and implementation using the python gurobipy library. We have solved the Blending Problem using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems such as transshipment problem, assignment problem, balanced diet problem. You can revise the basics of mathematical concepts in this article and learn about Linear Programming using PuLP in this article. "
84,Python,Solving Assignment Problem using Linear Programming in Python ,"Learn how to use Python PuLP to solve Assignment problems using Linear Programming.  In earlier articles, we have seen various applications of Linear programming such as transportation, transshipment problem, Cargo Loading problem, and shift-scheduling problem. Now In this tutorial, we will focus on another model that comes under the class of linear programming model known as the Assignment problem. Its objective function is similar to transportation problems. Here we minimize the objective function time or cost of manufacturing the products by allocating one job to one machine. If we want to solve the maximization problem assignment problem then we subtract all the elements of the matrix from the highest element in the matrix or multiply the entire matrix by –1 and continue with the procedure. For solving the assignment problem, we use the Assignment technique or Hungarian method, or Flood’s technique. The transportation problem is a special case of the linear programming model and the assignment problem is a special case of transportation problem, therefore it is also a special case of the linear programming problem. In this tutorial, we are going to cover the following topics: A problem that requires pairing two sets of items given a set of paired costs or profit in such a way that the total cost of the pairings is minimized or maximized. The assignment problem is a special case of linear programming. For example, an operation manager needs to assign four jobs to four machines. The project manager needs to assign four projects to four staff members. Similarly, the marketing manager needs to assign the 4 salespersons to 4 territories. The manager’s goal is to minimize the total time or cost. A manager has prepared a table that shows the cost of performing each of four jobs by each of four employees. The manager has stated his goal is to develop a set of job assignments that will minimize the total cost of getting all 4 jobs.   In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have two variable lists: workers and jobs. Let’s create them using  LpVariable.dicts() class. LpVariable.dicts() used with Python’s list comprehension. LpVariable.dicts() will take the following four values: Let’s first create a list route for the route between warehouse and project site and create the decision variables using LpVariable.dicts() the method. In this step, we will define the minimum objective function by adding it to the LpProblem object. lpSum(vector)is used here to define multiple linear expressions. It also used list comprehension to add multiple variables.  Here, we are adding two types of constraints: Each job can be assigned to only one employee constraint and Each employee can be assigned to only one job. We have added the 2 constraints defined in the problem by adding them to the LpProblem object.  In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  From the above results, we can infer that Worker-1 will be assigned to Job-1, Worker-2 will be assigned to job-3, Worker-3 will be assigned to Job-2, and Worker-4 will assign with job-4. In this article, we have learned about Assignment problems, Problem Formulation, and implementation using the python PuLp library. We have solved the Assignment problem using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. You can also run other case studies on Cargo Loading problems, Staff scheduling problems. In upcoming articles, we will write more on different optimization problems such as transshipment problem, balanced diet problem. You can revise the basics of mathematical concepts in this article and learn about Linear Programming in this article. "
85,Python,Transshipment Problem in Python Using PuLP ,"Learn how to use Python PuLP to solve transshipment problems using Linear Programming.  In case of transportation problems, we ship directly the material from source to destination. It means there are no intermediate nodes or points but in real life, we find problems where companies have their warehouses as intermediate nodes. Such problems are known as the Transshipment problems. We can easily turn those problems into transportation problems with some extra constraints and solve the problem. The transshipment problem is a special case of the transportation problem With intermediate nodes in the shipment paths. For instance, If shipping is happening between New Delhi to Bangalore shipping via Hyderabad may be less expensive than non-stop direct shipping to Bangalore.  In this tutorial, we are going to cover the following topics:  The manager of Harley Sand and Gravel has decided to utilize two intermediate nodes as transshipment points for the temporary storage of topsoil. The following figure shows the network flow diagram (Source): In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have two lists of variables: Routes from source to intermediate nodes and Routes from intermediate nodes to destination. Let’s create them using LpVariable.dicts() class. LpVariable.dicts() used with Python’s list comprehension. LpVariable.dicts() will take the following four values: Let’s first create a list route for the route between warehouse and project site and create the decision variables using LpVariable.dicts() the method. In this step, we will define the minimum objective function by adding it to the LpProblem object. lpSum(vector)is used here to define multiple linear expressions. It also used list comprehension to add multiple variables.  Here, we are adding three types of constraints: supply maximum constraints, demand minimum constraints, and transshipment constraints. We have added the 4 constraints defined in the problem by adding them to the LpProblem object.   In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  In this article, we have learned about Transshipment problems, Problem Formulation, and implementation using the python PuLp library. We have solved the transshipment problem using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. You can also run other case studies on Cargo Loading problems, Staff scheduling problems. In upcoming articles, we will write more on different optimization problems such as assignment problems, balanced diet problems. You can revise the basics of mathematical concepts in this article and learn about Linear Programming in this article. "
86,Python,Solving Balanced Diet Problem in Python using PuLP ,"Learn how to use Python PuLP to solve the Balanced Diet Problem using Linear Programming. A proper diet is essential for each and every human being. It keeps them healthy, fit, and more prone to chronic diseases. Junk foods may cause severe health issues in the modern world. Achieving a balanced diet is a multi-variate optimization problem. It comprises various variables that need to be optimized at less cost.  The linear programming method can be considered as one of the ideal methods for optimizing the diet problem. Linear programming is one of the operation research tools for the decision-making process. Linear programming methods may reduce the cost of business problems. The main goal of the Balanced Diet Problem is used to optimize the nutritional need of the people at minimum cost using Linear programming. In this tutorial, we are going to cover the following topics:  Let’s see the balanced diet problem example available in Book Introduction to Management Science.  In this problem, we need to design the composition of food items wheat, rice, and corn flakes in a 12-ounce cereal box.  Let’s determine the optimal quantities of wheat, rice, and corn per box. In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have three variables wood tables, chairs, and bookcases. Let’s create them using LpVariable class. LpVariable will take the following four values: In this step, we will define the minimum objective function by adding it to the LpProblem object. Constraint captures the restriction on the values of the decision variables. The simplest example is a linear constraint, which states that a linear expression on a set of variables takes a value that is either less-than-or-equal, greater-than-or-equal, or equal to another linear expression.   In this step, we will add the 4 constraints defined in the problem by adding them to the LpProblem object.   In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  From the above results, we can infer that the optimal amount of three food items is 10.5, 0.0, and 1.5 ounces of food item per box.  In this article, we have learned about Balanced Diet problems, Problem Formulation, and implementation using the python pulp library. We have solved the Balanced Diet Problem example using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems such as multiperiod production, network flow problems. You can revise the basics of mathematical concepts in this article and learn about Linear Programming using PuLP in this article. I have written more articles on different optimization problems such as transshipment problems, assignment problems, blending problems. "
87,Python,Solving Multi-Period Production Scheduling Problem in Python using PuLP ,"Learn how to use Python PuLP to solve the Multi-Period Production Scheduling Problem using Linear Programming.  Manufacturing companies face the problem of production-inventory planning for a number of future time periods under given constraints. This problem is known as the Multi-Period Production Scheduling problem. In manufacturing companies, production-inventory demand varies across multiple period horizons. The main goal is the level up the production needs for the individual products in this fluctuating demand. The objective function for such a problem is the minimization of total costs associated with the production and inventory. This planning works with the dynamic method. We can determine the production schedules for each period using Linear programming in a rolling way.  In this tutorial, we are going to cover the following topics:  Let’s see the Multi-Period Scheduling Production Problem example from Decision Models Lecture 4 Notes. Here is the link to Notes PDF.  Let’s determine the optimal production schedule by minimizing the total cost using Linear programming. In this step, we will import all the classes and functions of pulp module and create a Minimization LP problem using LpProblem class.  In this step, we will define the decision variables. In our problem, we have two categories of variables quarterly production and inventory. Let’s create them using LpVariable.dicts() class. LpVariable.dicts() will take the following four values: In this step, we will define the minimum objective function by adding it to the LpProblem object. lpSum(vector) is used here to define multiple linear expressions. It also used list comprehension to add multiple variables.  Constraint captures the restriction on the values of the decision variables. The simplest example is a linear constraint, which states that a linear expression on a set of variables takes a value that is either less-than-or-equal, greater-than-or-equal, or equal to another linear expression.   In this step, we will add the production capacity and inventory balance constraints defined in the problem by adding them to the LpProblem object using addConstraints() function.   In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  From the above results, we can infer the optimal number of units for each quarter for inventory and production. This is the final solution for the Multi-Period Production Scheduling problem.  In this article, we have learned about the Multi-Period Production Scheduling problem, Problem Formulation, and implementation using the python pulp library. We have solved the Multi-Period Production Scheduling problem example using a Linear programming problem in Python. Of course, this is just a simple case study, we can add more constraints to it and make it more complicated. In upcoming articles, we will write more on different optimization problems such as network flow problems. You can revise the basics of mathematical concepts in this article and learn about Linear Programming using PuLP in this article. I have written more articles on different optimization problems such as transshipment problems, assignment problems, blending problems.  "
88,Python,Sensitivity Analysis in Python ,"Learn Sensitivity Analysis using Python and why it is important for Decision Makers to interpret the model.  In today’s world, creating models is not enough we also need to explain the models on different-different aspects. Model Interpretability is a more crucial and curious topic for data researchers and analysts to understand the model and get the most out of that model. Sometimes, it delivers new and unique data patterns for decision-making. Sensitivity analysis is a method to explore the impact of feature changes on the LP model. In this method, we will change one feature and keep others to constant, and check the impact on model output. The main goal of Sensitivity analysis is to observe the effects of feature changes on the optimal solutions for the LP model. It can provide additional insights or information for the optimal solutions to an LP model. We can perform Sensitivity Analysis in 3 ways:  In this tutorial, we are going to cover the following topics: The shadow price is the change in the optimal value of the objective function per unit increase in the right-hand side (RHS) for a constraint and everything else remain unchanged. The slack variable is an amount of a resource that is unused. Slack variable indicates the range of feasibility for that constraint. If slack = 0 then constraints is a binding constraint. Changing the binding constraint changes the solution. Non-binding constraint means any change within this range will not have an effect on the optimal value of the objective function. There are various excellent optimization python packages are available such as SciPy, PuLP, Gurobi, and CPLEX. In this article, we will focus on the PuLP python library. PuLP is a general-purpose and open-source Linear Programming modeling package in python.  Install pulp package: PuLP modeling process has the following steps for solving LP problems:  This problem is taken from Introduction to Management Science By Stevenson and Ozgur.  A glass manufacturing company produces two types of glass products A and B.  A= Quantity of type A glass B= Quantity of type B glass  Objective Function: Profit = 60 * A + 50 * B Constraints:  In this step, we will import all the classes and functions of pulp module and create a Maxzimization LP problem using LpProblem class. In this step, we will define the decision variables. In our problem, we have three variables wood tables, chairs, and bookcases. Let’s create them using LpVariable class. LpVariable will take the following four values: In this step, we will define the maximum objective function by adding it to the LpProblem object.  In this step, we will add the 4 constraints defined in the problem by adding them to the LpProblem object.  In this step, we will solve the LP problem by calling solve() method. We can print the final value by using the following for loop.  You can see the shadow value for Constraints C2 and C3 is 10 and 13.33. it means if we do a unit change in RHS of constraints C2 and C3 will affect the objective function by 10 and 13.33. Let’s see this in the example of the next section.  Slack value for C1 constraint is 24 which indicates the range of feasibility for that constraint. It also indicates that the constraint is a binding constraint. Constraint C2 and C3 have slack = 0. It means constraint is a binding constraint. Nonbinding constraint means any change within this range will not have an effect on the optimal value of the objective function. We can check this by changing the value of constraint C1. Let’s see the following example:    As you have seen the shadow value for Constraints C2 and C3 are 10 and 13.33. it means if we do a unit change in RHS of constraints C2 and C3 will affect the objective function by 10 and 13.33. Let’s see this in the following example. Here we are changing the RHS value of constraint C2:   As you can see we have updated the RHS value of constraint C2 by 1 unit and the objective function got increased by 10 units. that’s what shadow value represents here. Congratulations, you have made it to the end of this tutorial! In this article, we have learned Sensitivity analysis in LP modeling, Model Interpretability, Shadow value, and slack variable with the examples in the python PuLp library. We have solved the Linear programming problem using PuLP and focused on sensitivity analysis with practical demonstration. Of course, this is just a very basic example of sensitivity analysis. In upcoming articles, we will write more on different optimization problems with sensitivity analysis. You can revise the basics of mathematical concepts in this article and learn about Linear Programming using PuLP in this article. I have written more articles on different optimization problems such as transshipment problems, assignment problems, blending problems.  "
89,Big Data,Apache Airflow: A Workflow Management Platform ,"Apache Airflow is a workflow management platform that schedules and monitors the data pipelines. We can also describe airflow as a batch-oriented framework for building data pipelines. Airflow helps us build, schedule, and monitor the data pipelines using the python-based framework. It captures data processes activities and coordinates the real-time updates to a distributed environment. Apache Airflow is not a data processing or ETL tool it orchestrates the various tasks in the data pipelines. The data pipeline is a set of several tasks that need to be executed in a given flow with dependencies to achieve the main objective. We can introduce the dependencies between tasks using graphs in the data pipeline. In graph-based solutions, tasks are represented as nodes and dependencies as directed edges between two tasks. This directed graph may lead to a deadlock situation so it is necessary to make it acyclic. Airflow uses Directed Acyclic Graphs (DAGs) to represent a data pipeline that efficiently executes the tasks as per graph flow. In this tutorial, we will explore the concepts of Data Pipelines and Apache Airflow. We will focus on DAGs and Airflow Architecture. Also, we will discuss when to use and when not to use the Airflow. In this tutorial, we are going to cover the following topics: The data pipeline consists series of tasks for data processing. It consists of three key components: source, processing tasks or steps, and sink(or destination). Data pipelines allow the flow of data between applications such as databases, data warehouses, data lakes, and cloud storage services. A data pipeline is used to automate the data transfer and data transformation between a source and sink repository. The data pipeline is the broader term for moving data between systems and ETL is the kind of data pipeline. Data Pipelines help us to deal with complex data processing operations. In this world of information, organizations are dealing with different-different workflows for collecting data from multiple sources, preprocessing data, uploading data, and reporting. Workflow management tools help us to automate all those operations in a scheduled manner. Apache Airflow is one of the workflow management platforms for scheduling and executing complex data pipelines. Airflow uses Directed Acyclic Graphs (DAGs) to represent a data pipeline that efficiently executes the tasks as per graph flow. Directed Acyclic Graph (DAG) comprises directed edges, nodes, and no loop or cycles. Acyclic means there are no circular dependencies in the graph between tasks. Circular dependency creates a problem in task execution. for example, if task-1 depends upon task-2 and task-2 depends upon task-1 then this situation will cause deadlock and leads to logical inconsistency.  In Airflow, we can write our DAGs in python and schedule them for execution at regular intervals such as every minute, every hour, every day, every week, and so on. Airflow is comprising four main components: Some of the other alternatives for airflow are Argos, Conductor, Make, Nifi, Metaflow, and Kubeflow. In Apache Airflow, data pipelines are represented as Directed Acyclic Graph that defines tasks at nodes and dependencies using directed edges. Airflow offers various features such as open-source, batch-oriented, and python-based workflow management. It has main three components scheduler, workers, and webserver. All these three components coordinate and execute data pipelines with a real-time monitoring feature.  In the upcoming tutorials, we will try to focus on Airflow Implementation, Operators, Templates, and Semantic Scheduling. You can also explore Big Data Technologies such as Hadoop and Spark on this portal."
90,Big Data,Apache Sqoop ,"In this tutorial, we will focus on the data ingestion tool Apache Sqoop for processing big data. Most of the web application portals store in Relation databases. These relational databases are the most common source for data storage. We need to transfer this data into the Hadoop system for analysis and processing purposes for various applications. Sqoop is a data ingestion tool that is designed to transfer data between RDBMS systems(such as Oracle, MySQL, SQL Server, Postgres, Teradata, etc) and Hadoop HDFS. Sqoop stands for — “SQL to Hadoop & Hadoop to SQL”. It is developed by Cloudera. Before Sqoop, the developer needs to write the MapReduce program to extract and load the data between RDBMS and Hadoop HDFS. This will cause the following problems: Sqoop makes it easier for the developer by providing CLI commands for data import and export. The developer needs to provide the basic required details such as source, destination, type of operations, and user authentication. It converts command from CLI to Map-Reduce Job.  Here, you will see a retail_db database. It means we already have a few default databases. It means there is no need to create the database, we can use the available database. In our example, we are using retail_db.  3. Show tables in retail_db using show tables statement. Select retail_db databases using the use statement.mysql> use retail_db;mysql> show tables; 4. Check the schema of the orders table using describe command.mysql> describe orders; 5. You can also see the records of orders table: mysql> select * from orders limit 5; In the sub-section, we have worked on the MySQL database and explored the orders table in retail_db databases.   [[email protected] ~]$ mysql -uroot -pcloudera [[email protected] ~]$ sqoop export — connect jdbc:mysql://localhost/retail_db — table orders_demo — username root — password cloudera — export-dir /user/cloudera/orders In this tutorial, we have discussed Apache Sqoop, its need, features, and architecture. Also, We have practiced the Sqoop Commands and performed various operations such as Import data from MySQL to HDFS, Export data from HDFS to MySQL. We have also compared the Sqoop with Flume."
91,Big Data,Apache Hive Hands-0n ,"In this tutorial, we will focus on Hadoop Hive for processing big data. Hive is a component in Hadoop Stack. It is an open-source data warehouse tool that runs on top of Hadoop. It was developed by Facebook and later it is donated to the Apache foundation. It reads, writes, and manages big data tables stored in HDFS or other data sources.  Hive doesn’t offer insert, delete and update operations but it is used to perform analytics, mining, and report generation on the large data warehouse. Hive uses Hive query language similar to SQL. Most of the syntax is similar to the MySQL database. It is used for OLAP(Online Analytical Processing) purposes. In the year 2006, Facebook was generating 10 GB of data per day and in 2007 its data increased by 1TB per day. After few days, it is generating 15 TB of data per day. Initially, Facebook is using the Scribe server, Oracle database, and Python scripts for processing large data sets. As Facebook started gathering data then they shifted to Hadoop as its key tool for data analysis and processing.  Facebook is using Hadoop for managing its big data and facing problems for ETL operations because for each small operation they need to write the Java programs. They need a lot of Java resources that are difficult to find and Java is not easy to learn. So Facebook developed Hive which uses SQL-like syntaxes that are easy to learn and write. Hive makes it easy for people who know SQL just like other RDBMS tools.  The following are the features of the Hive. [[email protected] ~]$ hive hive> Load the data from employee.txt file employee.txt101,Alice,New York,IT,Soft Engg,4000102,Ali,Atlanta,Data Science,Sr Soft Engg,4500103,Chang,New York,Data Science,Lead,6000104,Robin,Chicago,IT,Manager,7000   Output:  Load the data from project.txt fileproject.txt101,2001,Web Portal102,2002,NER Model103,2003,OCR Model104,2004,Web Portal Output: The table you have created in the above subsection is an internal table or by default internal table. In order to create an external table, you have to use an external keyword as shown below syntax: External table also non as non-managed table. You can understand the difference between internal and external table form the following comparison: It is used to join two or more relations bases on the common column. Let’s perofrom the JOIN operation on employee and project table: Output: It can used to group the data based on given field or column in a table. Let’s see an example of Group By in the following query:  Output: Subquery is an query with in query or nested query. Here, output of one query will become input for other query. Let’s see an example of sub query in th following query: Output:  The following are the limitations of the Hive. In this tutorial, we have discussed Apache Hive Features, Architecture, Components, and Limitations. We have also compared the Hive Vs SQL, Various operations( such as Order By, Sort By, Distributed By, and Cluster By ), and Partitions Vs Buckets. Also, We have executed the HQL in Hive and performed various operations such as loading data, Join, Group By, and Sub-queries.   "
92,Big Data,Apache Pig Hands-On ,"In this tutorial, we will focus on scripting language Apache PIG for processing big data.  Apache Pig is a scripting platform that runs top on Hadoop. It is a high-level and declarative language. It is designed for non-java programmers. Pig uses Latin scripts data flow language.  Hadoop is written in Java and initially, most of the developers write map-reduce jobs in Java. It means till then Java is the only language to interact with the Hadoop system. Yahoo came with one scripting language known as Pig in 2009. Here are few other reasons why Yahoo developed the Pig. Yahoo’s managers have faced problems in performing small tasks. For each small and big change, they need to call the programming team. Also, programmers need to write lengthy codes for small tasks. To overcome these problems, yahoo developed a scripting platform Pig. Pig help researchers to analyze data with simple and few lines of declarative syntax.  Apache Pig offers two core components: Pig works in the following steps: We can execute Pig in two modes: Local and MapReduce mode. file1.csv file2.csv  Lets perform operations in to the Pig environment: Output: (3,Amsterdam,3,5000)(5,The Hague,5,10000)(9,Rotterdam,9,2500)(10,Amsterdam,10,6000)(12,The Hague,12,4000) Output:(5,10000)(10,6000) Output:(Amsterdam,{(10,Amsterdam,10,6000),(3,Amsterdam,3,5000)})(Rotterdam,{(9,Rotterdam,9,2500)})(The Hague,{(12,The Hague,12,4000),(5,The Hague,5,10000)}) Output: (Amsterdam,2)(Rotterdam,1)(The Hague,2) Output: (5,The Hague)(3,Amsterdam)   We can count the occurrence of words in a given input file in PiG. Lets see the script below:  Output:(PM,1)(in,1)(of,1)(on,1)(to,1)(CMs,1)(and,1)(Modi,1)(India,2)(fresh,1)(Health,1)(issues,1)(speaks,1)(states,1)(Vaccine,1)(hotspot,1)(Lockdown,1)(Ministry,1)(Shortage,1)(Extension,1)(guidelines,1)(management,1)(Coronavirus,3)(containment,1) In this tutorial, we have discussed Hadoop PIG Features, Architecture, Components, and its working. Also, we have discussed the scripting in Pig and experimented with operators, and implemented a word count program in Pig. "
93,Big Data,Introduction to Apache Spark ,"In this tutorial, we will focus on Spark, Spark Framework, its Architecture, working, Resilient Distributed Datasets, RDD operations, Spark programming language, sand comparison of Spark with MapReduce. Spark is a fast cluster computing system that is compatible with Hadoop. It has the capability to work with any Hadoop supported storage system such as HDFS, S3. Spark uses in-memory computing to improve efficiency. In-memory computation does not save the intermediate output results to disk. Spark also uses caching to handle repetitive queries. Spark is up to 100x times compared to Hadoop. Spark is developed in Scala.  Spark is another Big Data framework. Spark supports In-Memory processing. Hadoop reads and writes data directly from disk thus wasting a significant amount of time in disk I/O. To tackle this scenario Spark stores intermediate results in memory thus reducing disk I/O and increasing speed of processing. Spark also uses the master-slave architecture. It has main two entities: Driver and Executors. The driver is a central coordinator (Driver) that communicates with multiple distributed executors. In spark, the application starts with the initialization of SparkContext instance. After this, the driver program gets started and asks for resources from the cluster manager and the cluster manager will launch the executors. The driver sends all the operations to executors. These operations can be actions and transformations over RDDs. Executors perform the task and save the final results.  In case of any executor crash/failure, tasks will be assigned to different executors. Spark has the capability to deal with failure and slow machines. Whenever any node crashes or gets slower then spark launches a speculative copy of the task on another executor or node in the cluster. We can stop the application by using the SparkContext.stop() method. This will terminate all the executors and release the cluster resources.  Spark uses one important data structure to distribute data over the executors in the cluster. This data structure is known as RDD (Resilient Distributed Datasets). RDD is an immutable data structure that can be distributed across the cluster for parallel computation. RDDs can be cached and persisted in memory.  In Map Reduce, data sharing among the nodes is slow because of data replication, serialization, and disk IO operations. Hadoop spends more than 90% of the time read-write operations on HDFS. To address this problem, researchers came with a new key idea of Resilient Distributed Datasets (RDD). RDD supports in-memory computation. In-memory means data is stored in the form of objects across the job and these all operations performed in the RAM. The in-memory concept helped in making 10 to 100 times faster data transfer operations. We can perform two types of basic operations on the RDD: “Transformations are lazy, they don’t compute right away. Transformation is only computed when any action is performed.”  You can execute the task in spark using Scala, Java, Python, and R language. Scala works faster with scala language compared to other languages because Spark is written in Scala. Most of the data scientists prefer Python for doing their tasks. But before using python we need to understand the difference between Python and Scala in Spark.  One of the major drawbacks of MapReduce is that it permanently stores the whole dataset on HDFS after executing each task. This operation is very expensive due to data replication. Spark doesn’t write data permanently on disk after each operation. It is an improvement over Mapreduce. Spark uses the in-memory concept for faster operations. This idea is given by Microsoft’s Dryad paper.  The main advantage of spark is that it launches any task faster compared to MapReduce. MapReduce launches JVM for each task while Spark keeps JVM running on each executor so that launching any task will not take much time.  Congratulations, you have made it to the end of this tutorial! In this tutorial, we will focus on Spark, Spark framework, its architecture, working, Resilient Distributed Datasets, RDD operations, Spark programming language, sand comparison of Spark with MapReduce. I look forward to hearing any feedback or questions. You can ask a question by leaving a comment, and I will try my best to answer it."
94,Big Data,MapReduce Algorithm ,"In this tutorial, we will focus on MapReduce Algorithm, its working, example, Word Count Problem, Implementation of wordcount problem in PySpark, MapReduce components, applications, and limitations. Map-Reduce is a programming model or framework for processing large distributed data. It processing data that resides on hundreds of machines.  It is a simple, elegant, and easy to understand programming model. It is based on the parallel computation of the job in a distributed environment. It processes a large amount of data in a reasonable time. Here, distribution means parallel computing of the same task on each CPU with a different dataset. MapReduce program can be written in JAVA, Python, and C++. MapReduce has two basic operations: The first operation is applied to each of the input records, and the second operation aggregates the output results. Map-Reduce must define two functions: MapReduce has basically two steps: map and reduce. map phase load, parse, transform, and filters the data. The map tasks generally load, parse, transform, and filter data. Each reduce task handles the results of the map task output. Map task takes a set of input files that distributed over HDFS. These files were divided into byte-oriented chunks. Finally, these chunks were consumed by the map task. The map task performs the following sub-operations: read, map, combine, and partition. The output of map tasks is a combination of a key-value pair that is also intermediate key-values.  Reduce phase takes the output of the map phase as input and converts it into final key-value pairs. The reduce task performs the following sub-operations: shuffle, sort, and reduce. In this problem, we have to count the frequency of each word in the given large text files. In the map phase, the file is broken into sentences, and each sentence will split into tokens or words. after tokenization, the map converts it into key values pair. and each key-value pair is known as intermediate key-value pairs.  In the reduce phase, the intermediate key-value pair shuffled and grouped based on the key in the key-value pair. After shuffle and group, pairs were aggregated and merged based on similar keys.  In this step, we have solved the word count problem using the MapReduce algorithm. We have executed this code in the Databricks community edition.  # read filerdd = sc.textFile(“/FileStore/tables/tripadvisor_review.txt”) #combined map-reduce algorithmword_count=rdd.flatMap(lambda x:x.split(“ “)).map(lambda x: (x,1)).reduceByKey(lambda x,y:x+y) # check initial 10 keywordsword_count.take(10) The reader reads the input records and parses it into the key-value pairs. These pairs pass to the mapper.  Mapper function takes the key-value pairs as input, processes each pair, and generates the output intermediate key-value pairs. Both input and output are of the mapper are different from each other. Partition takes intermediate key/value pairs as input and splits them into shards. It distributes the keys evenly in a way that similar keys grouped onto the same reducer.  The main objective of this step is to group similar key items so that reducer can easily aggregate them. This step performed on the reducer. It takes input files written by partitioners and arranges them in a group for aggregation purposes at reducer. The reducer takes the shuffled and sorted data for aggregation purposes. It performs an aggregate operation on grouped data.  We can use MapReduce to solve problems that are “huge but not hard”. it means your dataset can be large enough but your problem should be simple. If your problem can be converted into the key-value pairs, needs only aggregate and filter operations and operations ban be executed in isolation.  MapReduce has the following applications: Congratulations, you have made it to the end of this tutorial! In this tutorial, we will focus on MapReduce Algorithm, its working, example, Word Count Problem, Implementation of wordcount problem in PySpark, MapReduce components, applications, and limitations. I look forward to hearing any feedback or questions. You can ask a question by leaving a comment, and I will try my best to answer it."
95,Big Data,Hadoop Distributed File System ,"Hadoop is a Big Data computing platform for handling large datasets. Hadoop has a core two components: HDFS and MapReduce. In this tutorial, we will focus on the HDFS (Hadoop Distributed File System). HDFS is a storage component of Hadoop, It is a Distributed File System. It is optimized for high throughput and replicates blocks for failure handling.  HDFS stands for Hadoop Distributed File System. HDFS is a file system that is inspired by Google File Systems and designed for executing MapReduce jobs. HDFS reads input data in large chunks of input, processes it, and writes large chunks of output. HDFS is written in Java. It is scalable and reliable data storage, and it is designed to handle large clusters of commodity servers. HDFS cluster has a single NameNode that acts as the master node and Multiple DataNodes. NameNode manages the file system and regulates access to files. It controls and manages the services. DataNode provides block storage and retrieval services. Namenode maintains the file system and file Blockmap in memory. It keeps the metadata of any change in the block of data so that failure and system crash situations can be easily handled.  Job Tracker executes on a master node and Task Tracker executes on every slave node(master node also have a Task Tracker). Each slave ties with processing (TaskTracker) and the storage (DataNode). The Job Tracker maintains records of each resource in the Hadoop cluster. it schedules and assigns resources to the Task Tracker nodes. It regularly receives the progress status from Task Tracker. The task tracker regularly receives execution requests from Job Tracker. Task tracker tracks each task(mapper or reducer) and updates Job tracker about the workload. Task-Trackers’ main responsibility is to manage the processing resources on each slave node. In HDFS, each file is broken into equal size of blocks. Blocks are replicated to handle fault tolerance. NameNode continuously receives a Heartbeat or status update and a data block report from each DataNode in the cluster. DataNode failure means the failure of the slave node. DataNode periodically passes a signal or heartbeat to NameNode. It is helpful to tress the DataNode working status. When NameNode does not receive any heartbeat signals from DataNode then it assumes that DataNode is dead. After that, NameNode transfers the load of the current dead DataNode to other DataNode.  NameNode holds the metadata for the whole Hadoop cluster environment. It means it keeps track of each DataNode and maintains its record. NameNode failure is considered as a single point of failure(SPOF). If NameNode crashes due to system, hard drive failure whole cluster information will be lost.  Why is the NameNode a single point of failure? What is bad or difficult about having a complete duplicate of the NameNode running as well? We can recover from the situation of NameNode or SPOF by maintaining two NameNodes where one acts as a primary and the other NameNode acts as a secondary NameNode. It recovers by maintaining regular checkpoints on the Secondary NameNode. Here, Secondary NameNode is not a complete backup for the NameNode but it keeps the data of primary NameNode. It performs a checkpoint process periodically.  The main problem with the secondary NameNode is that it does not provide automatic failure recovery. It means in case of Namenode failure, the Hadoop administrator needs to take manually recover the data from Secondary Namenode. Hadoop 2.0 offers high availability compare to Hadddop 1.X. It uses the concept of Standby Namenode. The standby NameNode is used to handle the problem of a Single Point of Failure(SPOF). The standby NameNode provides automatic failover of NameNode failure. Here, Hadoop uses 2 NameNodes alongside one another so that if one of the Namenodes fails then the cluster will quickly use the other NameNode. In this standby NameNOde concept, DataNode sends all the signals to both the NameNodes and keeps a shared directory in Network File System.  Hadoop 2.x keeps 3 replicas by default for handling any kind of failure. This is a good strategy for data locality but keeping multiple copies extra overhead and slow down the overall throughput. Hadoop 3.x storage solved this 200% overhead by using erasure coding storage. It is cost-effective and saves IO operation time.  First, we split the data into several blocks in HDFS, and then we pass to the Erasure encoding. Erasure Encoding output several parity blocks. A combination of data and parity bock is known as an encoding group and in case of a failure, the erasure decoding group reconstructs the original data. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have covered a lot of details about HDFS. What is HDFS?, its features, how Hadoop works?, NameNode, DataNode, Job-Tracker, Task-Tracker, Data Replication, Handling DataNode, and NameNode Failure, Hadoop, 2.X, and Hadoop 3.X. Also, discussed the comparison of HDFS with NTFS and RDBMS. Hopefully, you can now utilize the HDFS knowledge in your big data and data science career. Thanks for reading this tutorial!"
96,Big Data,"Understanding BigData: Its Characteristics, Challenges, and Benefits ","In this tutorial, we will focus on what is big data, its characteristics, types, benefits, barriers, and job roles. In this world of information, people are generating and consuming more-more data from each aspect of life. Human activities such as shopping, traveling, utility bills, banking operations, citizen, and government services are on electronic platforms. Organizations are moving from paper to digital platforms for most of the activities. These all activities generating tons of data every day. Organizations and governments utilize this data and generate some values using analytics for smooth operations and to gain competitive advantages. So in this century of information where data is playing a crucial role there, Big Data Analytics can be the game-changer for business.  Data is a number, word, or letter without Context or you can say it is a collection of raw facts. Information is a collection of data with the context or you can say it is process data. When we process the information we will identify hidden patterns and understand its implications then it will become knowledge. This knowledge has the real power of making crucial decisions in any government or private organization. “Hiding within those mounds of data is the knowledge that could change the life of a patient, or change the world.” — Atul Butte, Stanford Big data is a large amount of data that cannot be handled by traditional IT systems. It has the capability to generate value for business and growth. Big data is high-volume, high-velocity, and/or high-variety information assets that demand cost-effective, innovative forms of information processing that enable enhanced insight, decision making, and process automation.  — Gartner IT Glossary In 2020, Facebook has 4.5 billion users in the world.  Here are the following characteristics: Apart from these three core characteristics, we need to focus on 2 more characteristics: Validity and Value. Only large data will not enough we also need valid data and we can generate some values out of it. If we can’t generate any value then there is no point to accumulate and analyze it. Another interesting view of the characteristics of Big Data can be seen in the following diagram: A fruit seller or any other small business that deals with one type of product, one market, and a single location. Such business does not need any Big data analytics but they do analyze their business by observing the customers. As per their own analysis and understanding of customers, he will offer the discount and according to quote the price.  If we talk about big stores such as Walmart, Costco, Aldi have multiple products, multiple markets, and multiple locations. They have millions of customers and millions of transactions each hour from various stores located around the world. This will generate a huge number of records per hour such data over the period of time will become big data due to volume, velocity, and variety. Such big organizations need big data solutions to understand customer needs, opinions, preferences, purchase habits, and sale trends. Not only these retail stores, but other companies also have big data problems. Telecom companies such as Airtel having billions of calls each hour they need to store detail about each call. Financial companies such as Visa having millions of transactions each hour. Delivery service companies such as FedEx delivering and tracking millions of items across the world. Organizations that are generating millions of transitions per minute/hour facing the problem of Big Data.  Big data analytics is the process of exploring large data sets that contains a variety of data such as Tabular, Text, Image, Videos, Audios, etc. Here we uncover the hidden patterns, find correlations, trends, Identify potential customers and their business values, customer opinion and preferences, hiring top talents, and other business-related information. Big data can be analyzed in two ways: Streaming and batch processing. In streaming, real-time data is processed and analyzed to provide quick and effective insights. In batch processing, data is stored and executed in batches for processing and analyzing to provide useful insights.  “Without big data analytics, companies are blind and deaf, wandering out onto the web like deer on a freeway.” — Geoffrey Moore, author and consultant  Big data is quite a new field. It’s mostly growth that happened in the last decade. Because of growth in data day by day, it puts lots of challenges and barriers for researchers and industry people. Let’s see those challenges and barriers:  The World’s most of the data is not structured i.e. it is unstructured type data. Most of the businesses and government organizations didn’t try to take advantage of it. Big data has the capability to generate more job opportunities and research challenges for industry and academia. The majority of the market is captured by Hadoop and Spark big data platforms for data processing and analysis. "
97,Big Data,Introduction to Hadoop ,"In this tutorial, we will focus on what is Hadoop, its features, components, job trends, architecture, ecosystem, applications, and disadvantage. The main objective of any big data platform is to extract the patterns and insights from the large and heterogeneous data for decision making. In this tutorial, we will focus on the basics of Hadoop, its features, components, job trends, architecture, ecosystem, applications, and disadvantages.  Hadoop was first developed by Doug Cutting and Mike Cafarella at Yahoo In 2005. They developed for handling web crawler project where they need to process billions of web pages for building a search engine. Hadoop is inspired by two research papers from Google: Google File Systems(2003) and MapReduce(2004). Hadoop got his name from the toy elephant. In 2006, yahoo donated Hadoop to apache. Hadoop is an open-source distributed computing framework for Big Data. Hadoop divides the big datasets into small chunks and executes them in a parallel fashion in a distributed environment. In this distributed system, data processed across nodes or clusters of computers using parallel programming models. It scales processing from single servers to a thousand of multiple machines. Hadoop has the capability of automatic failure detection and handling.   Hadoop has the following features: Hadoop has the following two core components: After 15 years old technology, Hadoop is quite popular in the Big Data industry and still offers job opportunities. In the last 2–3 years, the number of jobs reduced in Hadoop but the Global Hadoop market prediction said that the market will grow at a CAGR of 33% between 2019 and 2024. What I can suggest to you is that learn some support techniques of Hadoop and Big Data such as Spark, Kafka, etc. These things will help you in landing a promising career in Big Data. Let’s see a detailed architecture diagram of Hadoop: Hadoop is a standard tool for big data processing. It has the capability to compute large datasets and draw some insights. It is helpful in the aggregation of large datasets because of reducing operations. It is also useful for large scale ETL operations. In real-time Analytics, we need quick results. Hadoop is not suitable for real-time processing because it works batch processing( so data processing is time consuming). Hadoop is not going to replace the existing infrastructures such as MySQL and Oracle. Hadoop is costlier for small datasets. Hadoop offers the following applications: Finally, we can say Hadoop is an open-source distributed platform for processing big data efficiently and effectively. In this tutorial, our main focus is an introduction to Hadoop. you have understood what Hadoop is, its features, components, job trends, architecture, ecosystem, applications, and disadvantage. In the next tutorial, we will focus on Hadoop Distributed File Systems(HDFS). "
98,Case Studies,Predicting Employee Churn in Python ,"Analyze employee churn, Why employees are leaving the company, and How to predict, who will leave the company? In the past, most of the focus on the ‘rates’ such as attrition rate and retention rates. HR Managers compute the previous rates try to predict future rates using data warehousing tools. These rates present the aggregate impact of churn but this is the half picture. Another approach can be the focus on individual records in addition to aggregate. There are lots of case studies on customer churn are available. In customer churn, you can predict who and when a customer will stop buying. Employee churn is similar to customer churn. It mainly focuses on the employee rather than the customer. Here, you can predict who, and when an employee will terminate the service. Employee churn is expensive, and incremental improvements will give big results. It will help us in designing better retention plans and improving employee satisfaction. In this tutorial, you are going to cover the following topics: For more such tutorials, projects, and courses visit DataCamp Employee churn can be defined as a leak or departure of an intellectual asset from a company or organization. or in simple words, you can say, when employees leave the organization is known as churn. another definition can be when a member of a population leaves a population, is known as churn. In Research, it was found that employee churn will be affected by age, tenure, pay, job satisfaction, salary, working conditions, growth potential, and employee’s perceptions of fairness. Some other variables such as age, gender, ethnicity, education, and marital status, were important factors in the prediction of employee churn. In some cases such as the employee with niche, skills are harder to replace. It affects the ongoing work and productivity of existing employees. Acquiring new employees as a replacement has its own costs like hiring costs and training costs. Also, the new employee will take time to learn skills at a similar level of technical or business expertise knowledge of an older employee. Organizations tackle this problem by applying machine learning techniques to predict employee churn, which helps them in taking necessary actions. The following points help you to understand, employee and customer churn in a better way: Employee churn has unique dynamics compared to customer churn. It helps us in designing better employee retention plans and improving employee satisfaction. Data science algorithms can predict future churn. Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization. Let’s first load the required HR dataset using pandas’ read CSV function. You can download data from the following link: https://www.kaggle.com/liujiaqi/hr-comma-sepcsv Output: Output: After you have loaded the dataset, you might want to know a little bit more about it. You can check attriutes names and datatypes using info(). You can describe 10 attributes in detail as: In the given dataset, you have two types of employee one who stayed and another who left the company. So, you can divide data into two groups and compare their characteristics. Here, you can find the average of both the groups using groupby() and mean() function. Output: Here you can interpret, Employees who left the company had low satisfaction levels, low promotion rate, low salary, and worked more compare to those who stayed in the company. The describe() function in pandas is very handy in getting various summary statistics. This function returns the count, mean, standard deviation, minimum and maximum values, and the quantiles of the data. Output: Let’s check how many employees were left? Here, you can plot a bar graph using Matplotlib. the bar graph is suitable for showing discrete variable counts. Output: Here, you can see out of 15000 approx 3571 were left and 11428 stayed. The no of employees left is 23 % of the total employment. Similarly, you can also plot a bar graph to count the number of employees deployed on How many projects? Output: Similarly, you can also plot the bar graph to count the number of employees has on How much experience? Output: Most of the employee experience between 2–4 years. Also, there is a huge gap between 3 years and 4 years of experienced employees. This how you can analyze features one by one but it will time-consuming. The better option is here to use the Seaborn library and plot all the graphs in a single run using subplots. Output: You can observe the following points in the above visualization: Output: You can observe the following points in the above visualization: The following features are most influencing a person to leave the company: Let’s find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. so let’s bunch them in the group of people using cluster analysis. Output: Here, Employee who left the company can be grouped into 3 types of employees: Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column’s value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder. Here, you imported the preprocessing module and created the Label Encoder object. Using this LabelEncoder object you fit and transform the “salary” and “Departments “ column into the numeric column. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). you need to pass basically 3 parameters features, target, and test_set size. Additionally, you can use random_state in order to get the same kind of train and test set. Here, Dataset is broken into two parts in the ratio of 70:30. It means 70% of data will be used for model training and 30% for model testing. Let’s build an employee churn prediction model. Here, you are going to predict churn using Gradient Boosting Classifier. YOu can learn more about ensemble technique in this article. First, import the GradientBoostingClassifier module and create Gradient Boosting classifier object using GradientBoostingClassifier() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). Well, you got a classification rate of 97%, considered as good accuracy. Precision: Precision is about being precise i.e. How precise your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Gradient Boosting model predicted an employee is going to leave, that employee actually left 95% time. Recall: If there is an employee who actually left present in the test set and your Gradient Boosting model is able to identify it 92% of the time. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have learned What is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using matplotlib and seaborn, model building and evaluation using the python scikit-learn package. I look forward to hearing any feedback or questions. you can ask the question by leaving a comment and I will try my best to answer it. For more such tutorials, projects, and courses visit DataCamp Originally published at https://www.datacamp.com/community/tutorials/predicting-employee-churn-python Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
99,Case Studies,Spotify Song Recommender System in Python ,"Build a Song Recommender System using Content-Based Filtering in Python.  With the rapid growth in online and mobile platforms, lots of music platforms are coming into the picture. These platforms are offering songs lists from across the globe. Every individual has a unique taste for music. Most people are using Online music streaming platforms such as Spotify, Apple Music, Google Play, or Pandora.  Online Music listeners have lots of choices for the song. These customers sometimes get very difficult in selecting the songs or browsing the long list. The service providers need an efficient and accurate recommender system for suggesting relevant songs. As data scientists, we need to understand the patterns in music listening habits and predict the accurate and most relevant recommendations.  In this tutorial, we are going to cover the following topics: The content-based filtering method is based on the analysis of item features. It determines which features are most important for suggesting the songs. For example, if the user has liked a song in the past and the feature of that song is the theme and that theme is party songs then Recommender System will recommend the songs based on the same theme. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using the Spotify dataset to discover similar songs for recommendation using cosine similarity and sigmoid kernel.  In this tutorial, you will build a book recommender system. You can download this dataset from here.   Let’s load the data into pandas dataframe:  Output: Let’s understand the dataset. In this dataset, we have 15 columns: acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence, target, song_title, artist. Before building the model, first we normalize or scale the dataset. For scaling it we are using MinMaxScaler of Scikit-learn library. In this section, we are building a content-based recommender system using similarity measures such as Cosine and Sigmoid Kernel. Here, we will find the similarities among items or songs feature set and pick the top 10 most similar songs and recommend them.  Cosine similarity measures the cosine angle between two feature vectors. Its value implies that how two records are related to each other. Cosine similarity can be computed for the non-equal size of text documents. In the above code, we have computed the similarity using Cosine similarity and returned the Top-10 recommended songs. Let’s make a forecast using computed cosine similarity on the Spotify song dataset. In the above code, we have generated the Top-10 song list based on cosine similarity.  Let’s make a forecast using computed Sigmoid kernel on Spotify song dataset.  In the above code, we have generated the Top-10 song list based on Sigmoid Kernel.  Congratulations, you have made it to the end of this tutorial! In this tutorial, we have built the song recommender system using cosine similarity and Sigmoid kernel. This developed recommender system is a content-based recommender system. In another article, we have developed the recommender system using collaborative filtering. You can check that article here Book Recommender System using KNN. You can also check another article on the NLP-based recommender system."
100,Case Studies,Building Movie Recommender System using Text Similarity ," In this tutorial, we will focus on the movie recommender system using the NLP technique. With the dawn of the internet, utilizing information has become pervasive but the rapid growth of information causes the problem of information overload. In this large amount of information, how to find the right information which meets customer needs. In this context, Recommender System can help us to deal with such huge information. Also, with the increase in user options and rapid change in user preferences, we need some online systems that quickly adapt and recommend the relevant items.  A recommender system computes and suggests the relevant items based on user details, content details, and their interaction logs such as ratings. For example, Netflix is a streaming platform that recommends movies and series and keeps the consumer engaged on their platform. This engagement motivates customers to renew their subscriptions.  Content-based recommender system uses descriptive details products in order to make recommendations. For example, if the user has liked a web series in the past and the feature of that web series comedy genre then Recommender System will recommend the next series or movie based on the same genre. So the system adapts and learns the user behavior and suggests the items based on that behavior. In this article, we are using movie description or overview text to discover similar movies for recommendation using text similarity. In this tutorial, we are going to cover the following topics:   In this tutorial, we will build a movie recommender system using text similarity measures. You can download this dataset from here. Let’s load the data into pandas dataframe: Output: In the above code snippet, we have loaded The Movie Database (TMDb) data in Pandas DataFrame. In this section, we can explore the text overview of given movies. for doing exploratory analysis, the best way is to use Wordcloud and understand the most frequent words in the overview of the movie.  In the above code block, we have imported the wordcloud, stopwords, and matplotlib library. First, we created the combined text of all the movie overview descriptions and created the wordcloud on white background.  In the Text Similarity Problems, If we are applying cosine similarity then we have to convert texts into the respective vectors because we directly can’t use text for finding similarity. Let’s create vectors for given movie reviews using the TF-IDF approach.  TF-IDF(Term Frequency-Inverse Document Frequency) normalizes the document term matrix. It is the product of TF and IDF. TF-IDF normalizes the document weights. The higher value of TF-IDF for a word represents a higher occurrence in that document. Output:  In the above code block, Scikit-learn TfidfVectorizer is available for generating the TF-IDF Matrix. Cosine similarity measures the cosine angle between two text vectors. Its value implies that how two documents are related to each other. Cosine similarity can be computed for the non-equal size of text documents. In the above code, we have computed the cosine similarity using the cosine_similarity() method of sklearn.metrics module. Let’s make a forecast using computed cosine similarity on movie description data. In the above code, we have generated the Top-10 movies based on similar movie overview descriptions. Congratulations, you have made it to the end of this tutorial! In the last decade, the use of recommendation systems is increasing rapidly in lots of business ventures such as online retail business, learning, tourism, fashion, and library portals. The recommendation system assists in choosing the right thing from a large number of items by focusing on item features and user profiles. In this tutorial, we have built the movie recommender system using text similarity. In upcoming articles, we will write more articles on different recommender systems using Python. "
101,Case Studies,Book Recommender System using KNN ,"In this world of the internet, information is generating and growing rapidly. This huge amount of information will create information overload. People need to make decisions and make choices from available information or options. For example, People want to make a decision which Smartphone should they buy, which course they enroll in, which movie they watch, and which book they buy.  Recommender System is an automatic suggestion system that provides suggestions on things just like your friends, relatives, or neighbors give suggestions. A recommender system will help customers to find interesting things, reduce the number of options, and discover new things. At the same time, it also helps businesses to increase sales, increase user satisfaction and understand user needs. You have also seen youtube where they recommend videos based on your previously watched videos. Netflix recommends related movies based on previously watched movies. In this tutorial, we are going to cover the following topics: The main functions of the recommender system are: In this article, we will build a Book Recommenders System using KNN.  Collaborative filtering focuses on the ratings of the items given by users. It is based on “Wisdom of the crowd”. It predicts the suggested items based on the taste information from other users i.e. recommendations are from collaborative user ratings. collaborative filtering is used by large organizations such as Amazon and Netflix. It will suffer from cold start problems, sparsity problems, popularity bias, and first starter. Content-based filtering recommends items to users based on the description of the items and user profile. For example, recommending products based on the textual description, recommending movies based on their textual overview, and recommending books based on associated keywords. It will suffer where content is not well represented by keywords and the problem of indistinguishable items(same set feature items).  In this tutorial, you will build a book recommender system. You can download this dataset from here.   Let’s load the data into pandas dataframe:  Output:  Output: In the above two code snippet, we have loaded the Ratings and Books data in Pandas DataFrame.  In this section, we will merge the ratings and books’ dataframes based on the ISBN column using merge() function. Output: (1031136, 10) In this section, we will create the pivot table with book-title on the index, user id on the column, and fill values book rating. But before this first, we take a sample(1%) of the whole dataset because this dataset has 1 million records. If we don;t do this it will take a very long time or may cause of memory error on 8 GB Laptop.  Output: (10311, 10) Let’s create a Pivot table: Output: It’s time to create a NearestNeighbours model for recommendations using the Scikit-lean library.  Let’s make a forecast using a trained model of NearestNeighbours and generate a list of recommended books.  Output:  In the above output, we can see the list of recommended books.  Issues with NN-Based Collaborative Filtering Congratulations, you have made it to the end of this tutorial! In this tutorial, we have built the recommender system using the K-Nearest Neighbors algorithm.  In upcoming articles, we will write more articles on different recommender systems using Python.   "
102,Case Studies,Recommendation System for Streaming Platforms ,"In this Python tutorial, explore movie data of popular streaming platforms and build a recommendation system. Due to the new culture of Binge-watching TV Shows and Movies, users are consuming content at a fast pace with available services like Netflix, Prime Video, Hulu, and Disney+. Some of these new platforms, such as Hulu and YouTube TV, also offer live streaming of events like Sports, live concerts/tours, and news channels. Live streaming is still not adopted by some of the streaming platforms, such as Netflix. Streaming platforms provide more flexibility to users to watch their favorite TV shows and movies, at any time, on any device. These services can attract more young and modern consumers because of its wide variety of TV and movie content. It allows them to watch any missed program as their availability. In this tutorial, you will analyze movie data of streaming platforms Netflix, Prime Video, Hulu, and Disney+ and try to understand their viewers. Let’s see the highlights of the tutorial: This data consisted of only movies available on streaming platforms such as Netflix, Prime Video, Hulu, and Disney+. You can download it from Kaggle here. Let’s describe data attributes in detail: Let’s import the necessary modules and load the dataset: In this section, You will work with missing values using the isnull() function. Let’s see an example below: You can see that the variables Age and Rotten tomatoes have more than 50 % missing values, which is alarming. Now, we will handle the missing values in the following steps: You can check the distribution of the Year column using the distplot() the function of seaborn. Let’s plot the Movie Year distribution plot. The chart is showing the distribution of movies origin year. You can interpret that most of the movies were made between the year 2000 to 2020. Let’s plot the IMDB rating distribution plot. The above distribution plot is slightly skewed. You can interpret that the mean IMDB of most movies is 6.5. Let’s plot the Movie Runtime distribution plot. From the above chart, you can interpret that the movie’s average runtime lies between 80 to 120 mins. In this section, you will see Streaming Platform wise movie distribution. First, you need to create a m_cnt() function that counts movies for a given streaming platform. After that, you can plot using Pie charts and understand the shares of streaming platforms. From the above plot, you can say that Prime Videos is hosting the maximum number of titles with 71% share and Netflix hosting 20% of titles. Disney+ and Hulu are hosting the lowest titles, 5.4%, and 3.4%, respectively. In this section, you will see genre-wise movie distribution. First, you need to prepare your data. You need to handle multiple genres given in a single cell of dataframe. For that, you can use split(), apply(), and stack() functions. split() function splits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each genre and stack() function stack them into a single column. After these three operations, a new Genres column will join with the existing dataframe, and you are ready to plot. You can show the top 10 genres with their movie count using value_counts() function and use plot() function of the pandas library. From the above plot, you can say that most of the movies have a common genre as Drama and Comedy. In this section, you will see the country-wise movie distribution. First, you need to prepare your data. You need to handle multiple countries given in a single cell of dataframe. For that, you can use split(), apply(), and stack() functions. The split() function splits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each country and stack() function stack them into a single column. After these three operations, a new Country column will join with the existing dataframe and you are ready to plot. You can show the top 10 countries with their movie count using value_counts() function and use the pandas library’s plot() function. The above graph shows that the majority of the movies were made in the United States. In this section, you will see language-wise movie distribution. First, you need to prepare your data. You need to handle multiple languages given in a single cell of the dataframe. For that, you can use split(), apply(), and stack() functions. split() function spits the multiple values with a comma and creates a list. apply(pd.Series,1) to create multiple columns for each language and stack() function stack them into a single column. After these three operations, a new Language column will join with the existing dataframe, and you are ready to plot. You can show the top 10 languages with their movie count using value_counts() function and use the plot() the function of the pandas library. From the above plot, you can conclude that the majority of movies were in the English language. In this section, you will plot platform-wise IMDB rating distribution. For getting these results, you need to apply the melt() function and plot FacetGrid plot. melt() function converts a wide dataframe to a long dataframe. Let’s see the example below: The above plot shows the average IMDB rating distribution on each platform. In the “Working With Missing Values Section”, I have dropped the Age column. For getting results of Runtime Per Platform Along with Age Group. I need to load the data again and apply the melt() function. After loading the dataset again and performing melting, its time to generate a plot for runtime vs. streaming platform for different age groups. The above plot shows that the total runtime on Prime Videos by 18+ age group users is way higher than compared to any other platform. You can interpret that the Prime Videos most of the content is focused on the 18+ Age group. In the past few years, with the leap of YouTube, Walmart, Netflix, and many other such web-based services, recommender systems have created tremendous impact in the industry. From suggesting products/services to increasing companies value by online Ads-based monetization and matching the user’s relevance and preference to make them buy. Recommender systems are irreplaceable in our daily web quests. Generally, these are Math based frameworks focusing on suggesting products/services to end-users that are relevant to their needs and wants. For example, movies to watch, articles to read, products to buy, music to listen to, or anything depending on the domain. There are majorly three methods to build a Recommender Systems: Let’s first preprocess the dataset for the recommender system. First, you check the missing values: You will build two recommender system based on cosine similarity. Now, you will compute the similarity score using cosine similarity. Here, recommended movies are not up to the mark. The reason behind this poor result is that you are using only movie ratings, movie runtimes, and platform variables. You can improve this by using other information such as genre, directors, and country. Since our last recommender system worked well but the recommendations were not up to the mark, so you will try a new, better approach to improve our results. You will use textual columns into a single column then use tokenizer and TF-IDF Vectorizer to create a sparse matrix of all the words TF-IDF score. Then you will select and scale the numerical variables and add them into the sparse matrix. You need to perform the following steps for preprocessing: This time the recommender system works way better than the older system, which shows that by adding more relevant data like description text, a content-based recommender system can be improved significantly. Congratulations, you have made it to the end of this tutorial! In this tutorial, you performed an exploratory analysis of the streaming platform movie dataset. You have explored missing values, individual distribution plots, and distribution of movies on each streaming platform. You have also discovered insights on genre, country, language, IMDB ratings, and movie runtime. Finally, you also have seen how to build a recommender system in python. Originally published on: Do you want to learn data science, check out on DataCamp. For more such article, you can visit my blog Machine Learning Geek"
103,Case Studies,Predicting Customer Lifetime Value in Python ,"Learn how to calculate Customer Life Time Value in Python. Italian economist Vilfredo Pareto states that 80% of the effect comes from 20% of the causes, this is known as 80/20 rule or Pareto principle. Similarly, 80% of companies’ business comes from 20% of customers. Companies need to identify those top customers and maintain a relationship with them to ensure continuous revenue. In order to maintain a long-term relationship with customers, companies need to schedule loyalty schemes such as the discount, offers, coupons, bonus point, and gifts. Targetting a new customer is more costly than retaining existing customers because you don’t need to spend resources, time, and work hard to acquire new customers. You just have to keep the existing customers happy. Business analyst accurately calculates customer acquisition cost using CLTV(Customer Lifetime Value). CLTV indicates the total revenue from the customer during the entire relationship. CLTV helps companies to focus on those potential customers who can bring in more revenue in the future. For more such tutorials and courses visit DataCamp: In this tutorial, you are going to cover the following topics: Photo by Austin Distel on Unsplash Customer Lifetime Value is a monetary value that represents the amount of revenue or profit a customer will give the company over the period of the relationship. CLTV demonstrates the implications of acquiring long-term customers compare to short-term customers. Customer lifetime value (CLV) can help you to answers the most important questions about sales to every company: Originally published at https://www.datacamp.com/community/tutorials/customer-life-time-value There are lots of approaches available for calculating CLTV. Everyone has his/her own view on it. For computing CLTV we need historical data of customers but you will unable to calculate for new customers. To solve this problem Business Analyst develops machine learning models to predict the CLTV of newly customers. Let’s explore some approaches for CLTV Calculation: Average Order Value = Total Revenue / Total Number of Orders Purchase Frequency = Total Number of Orders / Total Number of Customers Customer Lifetime=1/Churn Rate Churn Rate= 1-Repeat Rate Let’s first load the required Online Retail dataset using the pandas read CSV function. You can download the data from this link: http://archive.ics.uci.edu/ml/datasets/online+retail Sometimes you get a messy dataset. You may have to deal with duplicates, which will skew your analysis. In python, pandas offer function drop_duplicates(), which drops the repeated or duplicate records. In the given dataset, you can observe most of the customers are from the “United Kingdom”. So, you can filter data for United Kingdom customers. The describe() function in pandas is convenient in getting various summary statistics. This function returns the count, mean, standard deviation, minimum and maximum values, and the quantiles of the data. Here, you can observe some of the customers have ordered in a negative quantity, which is not possible. So, you need to filter Quantity greater than zero. Here, you can filter the necessary columns for calculating CLTV. You only need her five columns CustomerID, InvoiceDate, InvoiceNo, Quantity, and UnitPrice. Here, you are going to perform the following operations: 1. Calculate the Average Order Value 2. Calculate Purchase Frequency 3. Calculate Repeat Rate and Churn Rate 4. Calculate Profit Margin Profit margin is the commonly used profitability ratio. It represents how much percentage of total sales has earned as the gain. Let’s assume our business has approx 5% profit on the total sale. 5. Calculate Customer Lifetime Value Let’s build the CLTV prediction model. Here, you are going to predict CLTV using Linear Regression Model. Let’s first use the data loaded and filtered above. Extract month and year from InvoiceDate. The pivot table takes the columns as input, and groups the entries into a two-dimensional table in such a way that provides a multidimensional summarization of the data. Let’s sum all the month’s sales. Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables). Select the latest 6 months as an independent variable. To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let’s split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state as a seed value to maintain reproducibility, which means whenever you split the data will not affect the results. Also, if random_state is None, then the random number generator uses np.random for selecting records randomly. It means If you don’t set seed, it is different each time. First, import the Linear Regression module and create a Linear Regression object. Then, fit your model on the train set using fit() function and perform prediction on the test set using predict() function. In order to evaluate the overall fit of the linear model, we use the R-squared value. R-squared is the proportion of variance explained by the model. Value of R-squared lies between 0 and 1. A higher value of R-squared is considered better because it indicates the larger variance explained by the model. This model has a higher R-squared (0.96). This model provides a better fit for the data. For regression problems following evaluation metrics were used (Ritchie Ng): RMSE is more popular than MSE and MAE because RMSE is interpretable with y because of the same units. CLTV helps you to design an effective business plan and also provides a chance to scale your business. CLTV draw meaningful customer segments these segment can help you to identify the needs of the different-different segment. Customer Lifetime Value is a tool, not a strategy. CLTV can figure out the most profitable customers, but how you are going to make a profit from them, it depends on your strategy. Generally, CLTV models are confused and misused. Obsession with CLTV may create blinders. Companies only focus on finding the best customer group and focusing on them and repeating the business, but it’s also important to give attention to other customers. Congratulations, you have made it to the end of this tutorial! In this tutorial, you have covered a lot of details about Customer Lifetime Value. You have learned what customer lifetime value is, approaches for calculating CLTV, implementation of CLTV from scratch in python, a prediction model for CLTV, and the Pros and Cons of CLTV. Also, you covered some basic concepts of pandas such as groupby and pivot table for summarizing selected columns and rows of data. Hopefully, you can now utilize the CLTV concept to analyze your own datasets. Thanks for reading this tutorial! For more such tutorials and courses visit DataCamp: Originally published at https://www.datacamp.com/community/tutorials/customer-life-time-value Reach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
104,Case Studies,Introduction to Customer Segmentation in Python ,"In this tutorial, you’re going to learn how to implement customer segmentation using RFM(Recency, Frequency, Monetary) analysis from scratch in Python. In the Retail sector, the various chain of hypermarkets generating an exceptionally large amount of data. This data is generated on a daily basis across the stores. This large database of customer transactions needs to analyze for designing profitable strategies. All customers have different-different kinds of needs. With the increase in customer base and transaction, it is not easy to understand the requirement of each customer. Identifying potential customers can improve the marketing campaign, which ultimately increases sales. Segmentation can play a better role in grouping those customers into various segments. For more such tutorials and courses visit DataCamp: In this tutorial, you will cover the following topics: Customer segmentation is a method of dividing customers into groups or clusters on the basis of common characteristics. The market researcher can segment customers into the B2C model using various customer’s demographic characteristics such as occupation, gender, age, location, and marital status. Psychographic characteristics such as social class, lifestyle and personality characteristics, and behavioral characteristics such as spending, consumption habits, product/service usage, and previously purchased products. In the B2B model using various company’s characteristics such as the size of the company, type of industry, and location. Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-python RFM (Recency, Frequency, Monetary) analysis is a behavior-based approach grouping customers in segments. It groups the customers on the basis of their previous purchase transactions. how recently how often and how much did the customer buy. RFM filters customers into various groups for the purpose of better service. It helps managers to identify potential customers to do a more profitable business. There is a segment of customer who is the big spender but what if they purchased only once or how recently they purchased? Do they often purchase our product? Also, It helps managers to run an effective promotional campaign for personalized service. Here, Each of the three variables(Recency, Frequency, and Monetary) consists of four equal groups, which creates 64 (4x4x4) different customer segments. Steps of RFM(Recency, Frequency, Monetary): Let’s first load the required HR dataset using pandas’ read CSV function. You can download the data from this link. Sometimes you get a messy dataset. You may have to deal with duplicates, which will skew your analysis. In python, pandas offer function drop_duplicates(), which drops the repeated or duplicate records. In the given dataset, you can observe most of the customers are from the “United Kingdom”. So, you can filter data for United Kingdom customers. Also, We are dropping the missing values. The describe() function in pandas is convenient in getting various summary statistics. This function returns the count, mean, standard deviation, minimum and maximum values, and the quantiles of the data. Here, you can observe some of the customers have ordered in a negative quantity, which is not possible. So, you need to filter Quantity greater than zero. Here, you can filter the necessary columns for RFM analysis. You only need her five columns CustomerID, InvoiceDate, InvoiceNo, Quantity, and UnitPrice. CustomerId will uniquely define your customers, InvoiceDate help you calculate recency of purchase, InvoiceNo helps you to count the number of time transaction performed(frequency). Quantity purchased in each transaction and UnitPrice of each unit purchased by the customer will help you to calculate the total purchased amount. Here, you are going to perform the following operations: Customers with the lowest recency, highest frequency, and monetary amounts are considered as top customers. qcut() is Quantile-based discretization function. qcut bins the data based on sample quantiles. For example, 1000 values for 4 quantiles would produce a categorical object indicating quantile membership for each customer. Combine all three quartiles(r_quartile,f_quartile,m_quartile) in a single column, this rank will help you to segment the customers well group. All the customers in group 111 are potential customers but still, we need to arrange them in monetary terms to sort within the potential group customers.  Congratulations, you have made it to the end of this tutorial! In this tutorial, you covered a lot of details about Customer Segmentation. you have learned what is the customer segmentation, Need of Customer Segmentation, Types of Segmentation, RFM analysis, Implementation of RFM from scratch in python. Also, you covered some basic concepts of pandas such as handling duplicates, groupby, and qcut() for bins based on sample quantiles. Hopefully, you can now utilize topic modeling to analyze your own datasets. Thanks for reading this tutorial! For more such tutorials and courses visit DataCamp: Originally published at https://www.datacamp.com/community/tutorials/random-forests-classifier-pythonReach out to me on Linkedin: https://www.linkedin.com/in/avinash-navlani/"
