{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.1.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Using cached pathy-0.6.0-py3-none-any.whl (42 kB)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp39-cp39-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Collecting thinc<8.1.0,>=8.0.9\n",
      "  Downloading thinc-8.0.10-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (4.61.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (1.21.2)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp39-cp39-win_amd64.whl (6.5 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Using cached spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.1-cp39-cp39-win_amd64.whl (451 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, preshed, blis, thinc, spacy-legacy, pathy, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-2.0.6 cymem-2.0.5 murmurhash-1.0.5 pathy-0.6.0 preshed-3.0.5 smart-open-5.2.1 spacy-3.1.3 spacy-legacy-3.0.8 srsly-2.4.1 thinc-8.0.10 typer-0.4.0 wasabi-0.8.2\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.26.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.61.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.21.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yoges\\anaconda3\\envs\\tensorflow2.0\\lib\\site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-20 09:16:41.300175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-10-20 09:16:41.300215: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "my_doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "When learning data science, you shouldn't get discouraged!\n",
       "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'learning', 'data', 'science', ',', 'you', 'should', \"n't\", 'get', 'discouraged', '!', '\\n', 'Challenges', 'and', 'setbacks', 'are', \"n't\", 'failures', ',', 'they', \"'re\", 'just', 'part', 'of', 'the', 'journey', '.', 'You', \"'ve\", 'got', 'this', '!']\n"
     ]
    }
   ],
   "source": [
    "# Create list of word tokens\n",
    "token_list = []\n",
    "\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"When learning data science, you shouldn't get discouraged!\", \"Challenges and setbacks aren't failures, they're just part of the journey.\", \"You've got this!\"]\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "# Add the 'sentencizer' component to the pipeline\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged! Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "# create list of sentence tokens\n",
    "sents_list = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    sents_list.append(sent.text)\n",
    "    \n",
    "print(sents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n"
     ]
    }
   ],
   "source": [
    "#importing stop words from English language.\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "#Printing the total number of stop words:\n",
    "print('Number of stop words: %d' % len(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'they', 'every', 'over', 'regarding', 'becomes', \"'ve\", 'with', 'meanwhile', 'yourselves', 'under', 'has', 'itself', 'hundred', 'than', 'also', 'nothing', 'seemed', 'neither', 'here', 'due', 'side', 'are', 'that', 'us', 'had', 'his', 'anyway', 'n‘t', 'without', 'by', 'moreover', 'indeed', 'hereby', 'be', 'third', 'and', 'two', 'whole', 're', 'is', 'would', '‘re', 'much', 'hereafter', 'whither', \"'s\", 'move', \"'d\", \"'m\", 'so', 'other', 'down', 'can', 'himself', 'its', 'thence', 'via', '‘d', 'n’t', 'latter', 'get', 'next', 'off', 'in', 'nevertheless', 'although', 'just', 'thereafter', 'whose', 'ten', 'full', 'since', 'ever', \"'ll\", 'upon', 'thereupon', 'themselves', 'using', 'among', 'thru', 'somehow', 'empty', 'go', 'whereas', 'from', 'more', 'someone', 'ours', 'both', 'always', 'already', 'into', 'were', 'anyhow', 'must', 'below', 'forty', 'along', 'am', 'others', 'hence', 'her', 'therefore', 'on', '’d', 'hers', 'either', 'several', 'there', 'per', 'any', 'whether', \"'re\", 'why', 'sixty', 'eleven', 'this', 'around', 'of', 'sometime', 'but', 'towards', 'each', 'often', '‘ll', 'their', 'hereupon', 'then', 'been', 'should', 'a', 'whereupon', 'to', 'amount', 'me', 'namely', 'though', 'those', 'most', 'latterly', 'whereby', 'something', 'could', 'whom', 'still', 'nine', '’ll', 'elsewhere', 'everything', 'not', 'four', 'six', 'up', 'least', 'was', '‘s', 'within', 'does', 'used', 'serious', 'take', 'who', 'nobody', 'through', 'everywhere', 'keep', 'whatever', 'beside', 'thereby', 'even', 'afterwards', 'part', 'while', 'again', 'cannot', 'perhaps', 'doing', 'or', 'none', 'never', 'how', 'anywhere', 'it', 'except', 'being', 'somewhere', 'once', 'no', 'until', 'yourself', 'if', 'herein', 'do', 'anything', 'noone', 'herself', 'only', 'he', 'thus', 'as', 'which', 'seem', 'many', 'well', 'twelve', 'enough', 'after', 'become', 'name', 'at', 'whenever', 'three', 'give', 'fifteen', 'nowhere', 'throughout', 'one', 'all', 'whoever', 'mine', 'quite', 'front', 'rather', 'she', 'top', 'might', 'seeming', 'really', 'last', 'before', 'whereafter', '’m', 'onto', 'own', 'out', 'please', 'wherever', 'between', 'an', 'beyond', 'beforehand', 'your', 'myself', 'wherein', 'fifty', 'various', 'my', 'besides', 'alone', 'mostly', 'unless', 'twenty', 'together', 'everyone', 'otherwise', 'make', 'about', 'else', 'now', 'have', '’ve', 'back', 'him', 'show', 'these', 'same', 'amongst', 'when', 'will', 'you', 'for', 'may', 'nor', 'some', 'i', 'first', 'five', 'see', 'ca', '’s', 'say', 'made', 'our', 'did', '‘m', '‘ve', 'we', 'very', 'behind', 'during', 'further', 'such', 'therein', 'becoming', 'anyone', 'because', 'above', 'almost', 'became', 'done', 'eight', 'seems', 'bottom', 'yours', 'yet', 'formerly', 'them', 'less', 'where', 'toward', 'against', 'however', 'few', 'what', 'ourselves', 'sometimes', 'former', 'the', 'whence', \"n't\", 'across', 'put', 'another', '’re', 'too', 'call'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: [learning, data, science, discouraged, Challenges, setbacks, failures, journey, got]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "# filtering stop words and punctuations\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        if word.is_punct==False:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ==> \n",
      "runs ==> \n",
      "running ==> \n",
      "runner ==> \n"
     ]
    }
   ],
   "source": [
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,\"==>\" ,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ==> run\n",
      "runs ==> run\n",
      "running ==> run\n",
      "runner ==> runner\n"
     ]
    }
   ],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Implementing lemmatization\n",
    "lem = nlp(\"run runs running runner\")\n",
    "\n",
    "# finding lemma for each word\n",
    "for word in lem:\n",
    "    print(word.text,\"==>\" ,word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: [learning, data, science, discouraged, Challenges, setbacks, failures, journey, got]\n",
      "Lemmatized Tokens: ['learn', 'data', 'science', 'discourage', 'challenge', 'setback', 'failure', 'journey', 'get']\n"
     ]
    }
   ],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "text = \"\"\"When learning data science, you shouldn't get discouraged! Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens=[]\n",
    "\n",
    "# filtering stop words and punctuations\n",
    "for word in doc:\n",
    "    if word.is_stop==False:\n",
    "        if word.is_punct==False:\n",
    "            filtered_tokens.append(word)\n",
    "\n",
    "print(\"Filtered Tokens:\",filtered_tokens)\n",
    "\n",
    "normalized_tokens=[]\n",
    "for token in filtered_tokens:\n",
    "    normalized_tokens.append(token.lemma_)\n",
    "    \n",
    "print(\"Lemmatized Tokens:\",normalized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DET\n",
      "is AUX\n",
      "well ADJ\n",
      "that DET\n",
      "ends VERB\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "\n",
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm   \n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()  \n",
    "\n",
    "# \"nlp\" Objectis used to create documents with linguistic annotations.\n",
    "docs = nlp(u\"All is well that ends well.\")\n",
    "\n",
    "for word in docs:\n",
    "    print(word.text,word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm   \n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load() \n",
    "\n",
    "nytimes= nlp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four zip codes there, Mayor Bill de Blasio (D) said Tuesday.The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City GPE\n",
      "Tuesday DATE\n",
      "At least 285 CARDINAL\n",
      "September DATE\n",
      "Brooklyn GPE\n",
      "four CARDINAL\n",
      "Bill de Blasio PERSON\n",
      "Tuesday DATE\n",
      "Orthodox NORP\n",
      "6 months old DATE\n",
      "up to $1,000 MONEY\n"
     ]
    }
   ],
   "source": [
    "for i in nytimes.ents:\n",
    "    print(i, i.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(New York City, 'GPE'), (Tuesday, 'DATE'), (At least 285, 'CARDINAL'), (September, 'DATE'), (Brooklyn, 'GPE'), (four, 'CARDINAL'), (Bill de Blasio, 'PERSON'), (Tuesday, 'DATE'), (Orthodox, 'NORP'), (6 months old, 'DATE'), (up to $1,000, 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "entities=[(i, i.label_) for i in nytimes.ents]\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York City\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    At least 285\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " people have contracted measles in the city since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    September\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", mostly in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Brooklyn\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "’s Williamsburg neighborhood. The order covers \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    four\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " zip codes there, Mayor \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bill de Blasio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " (D) said \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tuesday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".The mandate orders all unvaccinated people in the area, including a concentration of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Orthodox\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " Jews, to receive inoculations, including for children as young as \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6 months old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Anyone who resists could be fined \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    up to $1,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for visualization of Entity detection importing displacy from spacy:\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nytimes, style = \"ent\",jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pursuit pursuit pobj In\n",
      "a wall wall pobj of\n",
      "President Trump Trump nsubj ran\n"
     ]
    }
   ],
   "source": [
    "# importing the model en_core_web_sm of English for vocabluary, syntax & entities\n",
    "import en_core_web_sm   \n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load() \n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "docp = nlp (\"In pursuit of a wall, President Trump ran into one.\")\n",
    "\n",
    "for chunk in docp.noun_chunks:\n",
    "   print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b1712bf940854fbdb12479437a0cab4b-0\" class=\"displacy\" width=\"1800\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">In</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">pursuit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">wall,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">President</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Trump</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">ran</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">into</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">one.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-1\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M215.0,266.5 L223.0,254.5 207.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,266.5 L398.0,254.5 382.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-4\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-b1712bf940854fbdb12479437a0cab4b-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-b1712bf940854fbdb12479437a0cab4b-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,266.5 L1623.0,254.5 1607.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for visualization of Entity detection importing displacy from spacy:\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(docp, style=\"dep\", jupyter= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n"
     ]
    }
   ],
   "source": [
    "# import en_core_web_sm small spacy model\n",
    "import en_core_web_sm\n",
    "\n",
    "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "mango = nlp(u'mango')\n",
    "\n",
    "print(mango.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.462305  , -0.97013503, -0.3536405 ,  0.28740364, -0.01573728,\n",
       "       -0.24513936, -1.215326  , -0.8796606 , -0.33882028, -0.85366464,\n",
       "        1.1009694 , -0.40891293,  0.22952707,  0.32104927, -0.16520308,\n",
       "        0.19346984,  0.18104246, -0.25050682, -0.86570626, -0.5158702 ,\n",
       "        0.13842583, -1.1441295 , -1.2371405 , -0.31056306, -0.77198493,\n",
       "       -0.7328714 ,  0.821449  ,  0.46671125,  0.46151486, -0.3285221 ,\n",
       "        0.5737759 ,  0.5633069 ,  0.81746995, -0.1666174 , -0.31984073,\n",
       "        0.10492463, -1.0577446 ,  0.35842416,  0.47972912, -0.29047596,\n",
       "       -0.07571032,  1.112559  , -0.21457072,  1.0962675 , -0.1150732 ,\n",
       "        0.00683655,  0.3471359 ,  0.7762994 , -0.18421805, -0.4036425 ,\n",
       "        0.42345917,  0.25998825,  0.43403518, -0.3259907 ,  1.1417992 ,\n",
       "       -0.21782616,  0.6406765 ,  0.25259757, -0.17306823, -0.4783872 ,\n",
       "        0.89445263,  0.16820912, -0.01807833,  0.81608206, -0.42206132,\n",
       "       -0.67849445,  0.26927558, -0.5631349 ,  0.6785864 ,  1.0213488 ,\n",
       "        1.2156711 ,  0.1348517 , -0.53032076, -0.13513318,  0.05589192,\n",
       "       -0.2488123 ,  0.3784064 , -0.9752467 , -0.5351178 ,  0.12411818,\n",
       "        0.12952325,  0.01232275, -0.50991184,  0.6610786 , -0.86778444,\n",
       "        0.00903399,  0.5004437 , -0.63288045,  0.75803006, -0.28494287,\n",
       "       -1.0945169 ,  0.6785468 ,  0.3368305 ,  0.9685489 , -0.7742628 ,\n",
       "        0.30807123], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mango.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-0a8d80dd26ac>:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  doc1.similarity(doc2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.766287121319621"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## try medium or large spacy english models\n",
    "\n",
    "doc1 = nlp(\"I like apples.\")\n",
    "\n",
    "doc2 = nlp(\"I hate oranges.\")\n",
    "\n",
    "# cosine similarity\n",
    "doc1.similarity(doc2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
